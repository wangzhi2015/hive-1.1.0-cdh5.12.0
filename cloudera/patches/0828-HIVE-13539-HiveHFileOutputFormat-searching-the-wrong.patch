From cf58d4d7c52830b1dfc4e584c323631dc47171e3 Mon Sep 17 00:00:00 2001
From: Chaoyu Tang <ctang@cloudera.com>
Date: Fri, 18 Nov 2016 15:56:49 -0500
Subject: [PATCH 0828/1164] HIVE-13539: HiveHFileOutputFormat searching the
 wrong directory for HFiles (Tim Robertson, Chaoyu
 Tang, reviewed by Aihua Xu)

Change-Id: Ib04a0fb2668ea004775864ae93acbfc52859c1b3
---
 .../hadoop/hive/hbase/HiveHFileOutputFormat.java   |   13 ++++++++++---
 .../src/test/queries/positive/hbase_bulk.m         |    2 +-
 .../src/test/queries/positive/hbase_handler_bulk.q |    3 +++
 3 files changed, 14 insertions(+), 4 deletions(-)

diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
index da376d8..6f02ba8 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HiveHFileOutputFormat.java
@@ -50,6 +50,7 @@
 import org.apache.hadoop.mapred.JobConf;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.mapreduce.JobContext;
+import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;
 import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 import org.apache.hadoop.util.Progressable;
 
@@ -115,6 +116,7 @@ public RecordWriter getHiveRecordWriter(
           job.getConfiguration(), progressable);
 
     final Path outputdir = FileOutputFormat.getOutputPath(tac);
+    final Path taskAttemptOutputdir = FileOutputCommitter.getTaskAttemptPath(tac, outputdir);
     final org.apache.hadoop.mapreduce.RecordWriter<
       ImmutableBytesWritable, KeyValue> fileWriter = getFileWriter(tac);
 
@@ -148,7 +150,7 @@ public void close(boolean abort) throws IOException {
           // location specified by the user.
           FileSystem fs = outputdir.getFileSystem(jc);
           fs.mkdirs(columnFamilyPath);
-          Path srcDir = outputdir;
+          Path srcDir = taskAttemptOutputdir;
           for (;;) {
             FileStatus [] files = fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER);
             if ((files == null) || (files.length == 0)) {
@@ -161,6 +163,11 @@ public void close(boolean abort) throws IOException {
             if (srcDir.getName().equals(columnFamilyName)) {
               break;
             }
+            if (files[0].isFile()) {
+              throw new IOException("No family directories found in " + taskAttemptOutputdir + ". "
+                  + "The last component in hfile path should match column family name "
+                  + columnFamilyName);
+            }
           }
           for (FileStatus regionFile : fs.listStatus(srcDir, FileUtils.STAGING_DIR_PATH_FILTER)) {
             fs.rename(
@@ -171,8 +178,8 @@ public void close(boolean abort) throws IOException {
           }
           // Hive actually wants a file as task output (not a directory), so
           // replace the empty directory with an empty file to keep it happy.
-          fs.delete(outputdir, true);
-          fs.createNewFile(outputdir);
+          fs.delete(taskAttemptOutputdir, true);
+          fs.createNewFile(taskAttemptOutputdir);
         } catch (InterruptedException ex) {
           throw new IOException(ex);
         }
diff --git a/hbase-handler/src/test/queries/positive/hbase_bulk.m b/hbase-handler/src/test/queries/positive/hbase_bulk.m
index f8bb47d..475aafc 100644
--- a/hbase-handler/src/test/queries/positive/hbase_bulk.m
+++ b/hbase-handler/src/test/queries/positive/hbase_bulk.m
@@ -32,7 +32,7 @@ from src
 where value='val_100' or value='val_200';
 
 dfs -count /tmp/data/hbpartition;
-dfs -cp /tmp/data/hbpartition/* /tmp/hbpartition.lst;
+dfs -cp -f /tmp/data/hbpartition/* /tmp/hbpartition.lst;
 
 set mapred.reduce.tasks=3;
 set hive.mapred.partitioner=org.apache.hadoop.mapred.lib.TotalOrderPartitioner;
diff --git a/hbase-handler/src/test/queries/positive/hbase_handler_bulk.q b/hbase-handler/src/test/queries/positive/hbase_handler_bulk.q
index f03da63..85581ec 100644
--- a/hbase-handler/src/test/queries/positive/hbase_handler_bulk.q
+++ b/hbase-handler/src/test/queries/positive/hbase_handler_bulk.q
@@ -10,6 +10,9 @@ tblproperties ('hbase.table.name' = 'positive_hbase_handler_bulk');
 
 set hive.hbase.generatehfiles=true;
 set hfile.family.path=/tmp/hb_target/cf;
+set mapreduce.input.fileinputformat.split.maxsize=200;
+set mapreduce.input.fileinputformat.split.minsize=200;
+set mapred.reduce.tasks=2;
 
 -- this should produce three files in /tmp/hb_target/cf
 insert overwrite table hb_target select distinct key, value from src cluster by key;
-- 
1.7.9.5


From 83a671c4d0079ed2f08eec6c00c43f469735105f Mon Sep 17 00:00:00 2001
From: Jimmy Xiang <jxiang@cloudera.com>
Date: Tue, 24 Feb 2015 11:59:23 -0800
Subject: [PATCH 0053/1164] CDH-25471 Investigate unit tests failures on
 cdh5-1.1.0 - [parquet_external_time]

---
 .../hive/ql/io/parquet/convert/ETypeConverter.java |    6 +---
 .../io/parquet/read/DataWritableReadSupport.java   |    7 ++++-
 .../parquet/read/ParquetRecordReaderWrapper.java   |   31 ++++++++++----------
 3 files changed, 23 insertions(+), 21 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
index 377e362..0f31d11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/ETypeConverter.java
@@ -18,7 +18,6 @@
 import java.util.ArrayList;
 import java.util.Map;
 
-import com.google.common.base.Strings;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTime;
 import org.apache.hadoop.hive.ql.io.parquet.timestamp.NanoTimeUtils;
@@ -145,10 +144,7 @@ protected TimestampWritable convert(Binary binary) {
           Map<String, String> metadata = parent.getMetadata();
           //Current Hive parquet timestamp implementation stores it in UTC, but other components do not do that.
           //If this file written by current Hive implementation itself, we need to do the reverse conversion, else skip the conversion.
-          boolean skipConversion = false;
-          if (Boolean.valueOf(metadata.get(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname))) {
-            skipConversion = !Strings.nullToEmpty(metadata.get("createdBy")).startsWith("parquet-mr");
-          }
+          boolean skipConversion = Boolean.valueOf(metadata.get(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname));
           Timestamp ts = NanoTimeUtils.getTimestamp(nt, skipConversion);
           return new TimestampWritable(ts);
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
index 47cd682..57ae7a9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
@@ -19,6 +19,7 @@
 import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.IOConstants;
 import org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
@@ -30,7 +31,6 @@
 import parquet.hadoop.api.ReadSupport;
 import parquet.io.api.RecordMaterializer;
 import parquet.schema.MessageType;
-import parquet.schema.MessageTypeParser;
 import parquet.schema.PrimitiveType;
 import parquet.schema.PrimitiveType.PrimitiveTypeName;
 import parquet.schema.Type;
@@ -153,6 +153,11 @@
       throw new IllegalStateException("ReadContext not initialized properly. " +
         "Don't know the Hive Schema.");
     }
+    String key = HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname;
+    if (!metadata.containsKey(key)) {
+      metadata.put(key, String.valueOf(HiveConf.getBoolVar(
+        configuration, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)));
+    }
     return new DataWritableRecordConverter(readContext.getRequestedSchema(), metadata);
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
index 6dc85fa..f69d13c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
@@ -16,10 +16,10 @@
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
@@ -49,6 +49,8 @@
 import parquet.hadoop.util.ContextUtil;
 import parquet.schema.MessageTypeParser;
 
+import com.google.common.base.Strings;
+
 public class ParquetRecordReaderWrapper  implements RecordReader<Void, ArrayWritable> {
   public static final Log LOG = LogFactory.getLog(ParquetRecordReaderWrapper.class);
 
@@ -61,6 +63,7 @@
   private boolean firstRecord = false;
   private boolean eof = false;
   private int schemaSize;
+  private boolean skipTimestampConversion = false;
 
   private final ProjectionPusher projectionPusher;
 
@@ -93,7 +96,14 @@ public ParquetRecordReaderWrapper(
     setFilter(oldJobConf);
 
     // create a TaskInputOutputContext
-    final TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(oldJobConf, taskAttemptID);
+    Configuration conf = oldJobConf;
+    if (skipTimestampConversion ^ HiveConf.getBoolVar(
+        conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {
+      conf = new JobConf(oldJobConf);
+      HiveConf.setBoolVar(conf,
+        HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION, skipTimestampConversion);
+    }
+    final TaskAttemptContext taskContext = ContextUtil.newTaskAttemptContext(conf, taskAttemptID);
 
     if (split != null) {
       try {
@@ -218,6 +228,7 @@ public boolean next(final Void key, final ArrayWritable value) throws IOExceptio
    * @return a ParquetInputSplit corresponding to the oldSplit
    * @throws IOException if the config cannot be enhanced or if the footer cannot be read from the file
    */
+  @SuppressWarnings("deprecation")
   protected ParquetInputSplit getSplit(
       final InputSplit oldSplit,
       final JobConf conf
@@ -248,7 +259,9 @@ protected ParquetInputSplit getSplit(
         LOG.warn("Skipping split, could not find row group in: " + (FileSplit) oldSplit);
         split = null;
       } else {
-        populateReadMetadata(readContext.getReadSupportMetadata(), fileMetaData, conf);
+        if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)) {
+          skipTimestampConversion = !Strings.nullToEmpty(fileMetaData.getCreatedBy()).startsWith("parquet-mr");
+        }
         split = new ParquetInputSplit(finalPath,
                 splitStart,
                 splitLength,
@@ -264,16 +277,4 @@ protected ParquetInputSplit getSplit(
     }
     return split;
   }
-
-  /**
-   * Method populates the read metadata, using filemetadata and Hive configuration.
-   * @param metadata read metadata to populate
-   * @param fileMetaData parquet file metadata
-   * @param conf hive configuration
-   */
-  private void populateReadMetadata(Map<String, String> metadata, FileMetaData fileMetaData, JobConf conf) {
-    metadata.put("createdBy", fileMetaData.getCreatedBy());
-    metadata.put(HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION.varname,
-      String.valueOf(HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_PARQUET_TIMESTAMP_SKIP_CONVERSION)));
-  }
 }
-- 
1.7.9.5


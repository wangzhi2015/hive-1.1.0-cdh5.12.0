From e784891a40be71c67ed56bb588a9773552794ec0 Mon Sep 17 00:00:00 2001
From: Sergio Pena <sergio.pena@cloudera.com>
Date: Wed, 27 May 2015 14:27:58 -0500
Subject: [PATCH 0356/1164] CDH-33274: HIVE-9605: Remove parquet nested
 objects from wrapper writable objects (Sergio
 Pena, reviewed by Ferdinand Xu)

Change-Id: Iac16584629c3aa073010419605e36e8ac0751df8
---
 .../benchmark/storage/ColumnarStorageBench.java    |    4 +-
 .../parquet/convert/HiveCollectionConverter.java   |    5 +--
 .../ql/io/parquet/convert/HiveGroupConverter.java  |   10 -----
 .../hive/ql/io/parquet/convert/Repeated.java       |    8 ++--
 .../parquet/serde/AbstractParquetMapInspector.java |   13 +++---
 .../parquet/serde/DeepParquetHiveMapInspector.java |    6 +--
 .../parquet/serde/ParquetHiveArrayInspector.java   |   43 +++++--------------
 .../serde/StandardParquetHiveMapInspector.java     |    6 +--
 .../hive/ql/io/parquet/TestArrayCompatibility.java |   44 ++++++++++----------
 .../hive/ql/io/parquet/TestDataWritableWriter.java |   44 ++++++++------------
 .../hive/ql/io/parquet/TestMapStructures.java      |   36 ++++++++--------
 .../hive/ql/io/parquet/TestParquetSerDe.java       |    8 +---
 .../serde/TestAbstractParquetMapInspector.java     |    4 +-
 .../serde/TestDeepParquetHiveMapInspector.java     |    4 +-
 .../serde/TestParquetHiveArrayInspector.java       |    3 +-
 .../serde/TestStandardParquetHiveMapInspector.java |    4 +-
 16 files changed, 92 insertions(+), 150 deletions(-)

diff --git a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java b/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
index d335716..b1feeec 100644
--- a/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
+++ b/itests/hive-jmh/src/main/java/org/apache/hive/benchmark/storage/ColumnarStorageBench.java
@@ -193,13 +193,13 @@ private ArrayWritable createRecord(final List<TypeInfo> columnTypes) {
         case LIST: {
           List<TypeInfo> elementType = new ArrayList<TypeInfo>();
           elementType.add(((ListTypeInfo) type).getListElementTypeInfo());
-          fields[pos++] = record(createRecord(elementType));
+          fields[pos++] = createRecord(elementType);
         } break;
         case MAP: {
           List<TypeInfo> keyValueType = new ArrayList<TypeInfo>();
           keyValueType.add(((MapTypeInfo) type).getMapKeyTypeInfo());
           keyValueType.add(((MapTypeInfo) type).getMapValueTypeInfo());
-          fields[pos++] = record(record(createRecord(keyValueType)));
+          fields[pos++] = record(createRecord(keyValueType));
         } break;
         case STRUCT: {
           List<TypeInfo> elementType = ((StructTypeInfo) type).getAllStructFieldTypeInfos();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveCollectionConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveCollectionConverter.java
index 6621a87..f1c8b6f 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveCollectionConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveCollectionConverter.java
@@ -3,7 +3,6 @@
 import com.google.common.base.Preconditions;
 import java.util.ArrayList;
 import java.util.List;
-import java.util.Map;
 
 import org.apache.hadoop.io.ArrayWritable;
 import org.apache.hadoop.io.Writable;
@@ -65,8 +64,8 @@ public void start() {
 
   @Override
   public void end() {
-    parent.set(index, wrapList(new ArrayWritable(
-        Writable.class, list.toArray(new Writable[list.size()]))));
+    parent.set(index, new ArrayWritable(
+        Writable.class, list.toArray(new Writable[0])));
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
index 4809f9b..c6d03a1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/HiveGroupConverter.java
@@ -13,7 +13,6 @@
  */
 package org.apache.hadoop.hive.ql.io.parquet.convert;
 
-import org.apache.hadoop.io.ArrayWritable;
 import org.apache.hadoop.io.Writable;
 import parquet.io.api.Converter;
 import parquet.io.api.GroupConverter;
@@ -72,15 +71,6 @@ protected static Converter getConverterFromDescription(Type type, int index, Con
     return getConverterFromDescription(type.asGroupType(), index, parent);
   }
 
-  /**
-   * The original list and map conversion didn't remove the synthetic layer and
-   * the ObjectInspector had to remove it. This is a temporary fix that adds an
-   * extra layer for the ObjectInspector to remove.
-   */
-  static ArrayWritable wrapList(ArrayWritable list) {
-    return new ArrayWritable(Writable.class, new Writable[] {list});
-  }
-
   public abstract void set(int index, Writable value);
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/Repeated.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/Repeated.java
index fdea782..ee57b31 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/Repeated.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/Repeated.java
@@ -107,8 +107,8 @@ public void parentStart() {
 
     @Override
     public void parentEnd() {
-      parent.set(index, HiveGroupConverter.wrapList(new ArrayWritable(
-          Writable.class, list.toArray(new Writable[list.size()]))));
+      parent.set(index, new ArrayWritable(
+          Writable.class, list.toArray(new Writable[list.size()])));
     }
 
     @Override
@@ -167,8 +167,8 @@ public void parentStart() {
 
     @Override
     public void parentEnd() {
-      parent.set(index, wrapList(new ArrayWritable(
-          Writable.class, list.toArray(new Writable[list.size()]))));
+      parent.set(index, new ArrayWritable(
+          Writable.class, list.toArray(new Writable[list.size()])));
     }
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
index 62c61fc..49bf1c5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/AbstractParquetMapInspector.java
@@ -59,15 +59,12 @@ public ObjectInspector getMapValueObjectInspector() {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
-
-      if (mapContainer == null || mapContainer.length == 0) {
+      final Writable[] mapArray = ((ArrayWritable) data).get();
+      if (mapArray == null || mapArray.length == 0) {
         return null;
       }
 
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
       final Map<Writable, Writable> map = new LinkedHashMap<Writable, Writable>();
-
       for (final Writable obj : mapArray) {
         final ArrayWritable mapObj = (ArrayWritable) obj;
         final Writable[] arr = mapObj.get();
@@ -91,12 +88,12 @@ public int getMapSize(final Object data) {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
+      final Writable[] mapArray = ((ArrayWritable) data).get();
 
-      if (mapContainer == null || mapContainer.length == 0) {
+      if (mapArray == null || mapArray.length == 0) {
         return -1;
       } else {
-        return ((ArrayWritable) mapContainer[0]).get().length;
+        return mapArray.length;
       }
     }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
index d38c641..143d72e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/DeepParquetHiveMapInspector.java
@@ -40,14 +40,12 @@ public Object getMapValueElement(final Object data, final Object key) {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
+      final Writable[] mapArray = ((ArrayWritable) data).get();
 
-      if (mapContainer == null || mapContainer.length == 0) {
+      if (mapArray == null || mapArray.length == 0) {
         return null;
       }
 
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
-
       for (final Writable obj : mapArray) {
         final ArrayWritable mapObj = (ArrayWritable) obj;
         final Writable[] arr = mapObj.get();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
index 3d1d98c..05e92b5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/ParquetHiveArrayInspector.java
@@ -56,20 +56,13 @@ public Object getListElement(final Object data, final int index) {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
-        return null;
-      }
-
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
+      final Writable[] array = ((ArrayWritable) data).get();
+      if (array == null || array.length == 0) {
         return null;
       }
 
-      if (index >= 0 && index < ((ArrayWritable) subObj).get().length) {
-        return ((ArrayWritable) subObj).get()[index];
+      if (index >= 0 && index < array.length) {
+        return array[index];
       } else {
         return null;
       }
@@ -89,19 +82,12 @@ public int getListLength(final Object data) {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
+      final Writable[] array = ((ArrayWritable) data).get();
+      if (array == null || array.length == 0) {
         return -1;
       }
 
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return 0;
-      }
-
-      return ((ArrayWritable) subObj).get().length;
+      return array.length;
     }
 
     if (data instanceof List) {
@@ -118,21 +104,12 @@ public int getListLength(final Object data) {
     }
 
     if (data instanceof ArrayWritable) {
-      final Writable[] listContainer = ((ArrayWritable) data).get();
-
-      if (listContainer == null || listContainer.length == 0) {
+      final Writable[] array = ((ArrayWritable) data).get();
+      if (array == null || array.length == 0) {
         return null;
       }
 
-      final Writable subObj = listContainer[0];
-
-      if (subObj == null) {
-        return null;
-      }
-
-      final Writable[] array = ((ArrayWritable) subObj).get();
-      final List<Writable> list = new ArrayList<Writable>();
-
+      final List<Writable> list = new ArrayList<Writable>(array.length);
       for (final Writable obj : array) {
         list.add(obj);
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
index 5aa1448..22250b3 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/serde/StandardParquetHiveMapInspector.java
@@ -37,12 +37,12 @@ public Object getMapValueElement(final Object data, final Object key) {
       return null;
     }
     if (data instanceof ArrayWritable) {
-      final Writable[] mapContainer = ((ArrayWritable) data).get();
+      final Writable[] mapArray = ((ArrayWritable) data).get();
 
-      if (mapContainer == null || mapContainer.length == 0) {
+      if (mapArray == null || mapArray.length == 0) {
         return null;
       }
-      final Writable[] mapArray = ((ArrayWritable) mapContainer[0]).get();
+
       for (final Writable obj : mapArray) {
         final ArrayWritable mapObj = (ArrayWritable) obj;
         final Writable[] arr = mapObj.get();
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestArrayCompatibility.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestArrayCompatibility.java
index f7f3e57..d45d8ee 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestArrayCompatibility.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestArrayCompatibility.java
@@ -44,8 +44,8 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        new IntWritable(34), new IntWritable(35), new IntWritable(36)));
+    ArrayWritable expected = list(
+        new IntWritable(34), new IntWritable(35), new IntWritable(36));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -91,9 +91,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new FloatWritable(1.0f), new FloatWritable(1.0f)),
-        record(new FloatWritable(2.0f), new FloatWritable(2.0f))));
+        record(new FloatWritable(2.0f), new FloatWritable(2.0f)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -130,8 +130,8 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        new IntWritable(34), new IntWritable(35), new IntWritable(36)));
+    ArrayWritable expected = list(
+        new IntWritable(34), new IntWritable(35), new IntWritable(36));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -180,9 +180,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new LongWritable(1234L)),
-        record(new LongWritable(2345L))));
+        record(new LongWritable(2345L)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -219,8 +219,8 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        new IntWritable(34), new IntWritable(35), new IntWritable(36)));
+    ArrayWritable expected = list(
+        new IntWritable(34), new IntWritable(35), new IntWritable(36));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -269,9 +269,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new LongWritable(1234L)),
-        record(new LongWritable(2345L))));
+        record(new LongWritable(2345L)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -321,9 +321,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         new LongWritable(1234L),
-        new LongWritable(2345L)));
+        new LongWritable(2345L));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -379,9 +379,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new DoubleWritable(0.0), new DoubleWritable(0.0)),
-        record(new DoubleWritable(0.0), new DoubleWritable(180.0))));
+        record(new DoubleWritable(0.0), new DoubleWritable(180.0)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -455,10 +455,10 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new DoubleWritable(0.0), new DoubleWritable(0.0)),
         null,
-        record(new DoubleWritable(0.0), new DoubleWritable(180.0))));
+        record(new DoubleWritable(0.0), new DoubleWritable(180.0)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -528,9 +528,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new DoubleWritable(0.0), new DoubleWritable(180.0)),
-        record(new DoubleWritable(0.0), new DoubleWritable(0.0))));
+        record(new DoubleWritable(0.0), new DoubleWritable(0.0)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -601,9 +601,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new DoubleWritable(0.0), new DoubleWritable(180.0)),
-        record(new DoubleWritable(0.0), new DoubleWritable(0.0))));
+        record(new DoubleWritable(0.0), new DoubleWritable(0.0)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestDataWritableWriter.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestDataWritableWriter.java
index 8f03c5b..73425e3 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestDataWritableWriter.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestDataWritableWriter.java
@@ -311,13 +311,11 @@ public void testArrayType() throws Exception {
         + "}\n";
 
     ArrayWritable hiveRecord = createGroup(
-        createGroup(
-            createArray(
-                createInt(1),
-                createNull(),
-                createInt(2)
-            )
-        )
+      createArray(
+          createInt(1),
+          createNull(),
+          createInt(2)
+      )
     );
 
     // Write record to Parquet format
@@ -363,18 +361,16 @@ public void testMapType() throws Exception {
     ArrayWritable hiveRecord = createGroup(
         createGroup(
             createArray(
-                createArray(
-                    createString("key1"),
-                    createInt(1)
-                ),
-                createArray(
-                    createString("key2"),
-                    createInt(2)
-                ),
-                createArray(
-                    createString("key3"),
-                    createNull()
-                )
+                createString("key1"),
+                createInt(1)
+            ),
+            createArray(
+                createString("key2"),
+                createInt(2)
+            ),
+            createArray(
+                createString("key3"),
+                createNull()
             )
         )
     );
@@ -432,14 +428,10 @@ public void testArrayOfArrays() throws Exception {
         + "}\n";
 
     ArrayWritable hiveRecord = createGroup(
-        createGroup(
+        createArray(
             createArray(
-                createGroup(
-                    createArray(
-                        createInt(1),
-                        createInt(2)
-                    )
-                )
+                createInt(1),
+                createInt(2)
             )
         )
     );
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapStructures.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapStructures.java
index ca48050..3c7401f 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapStructures.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestMapStructures.java
@@ -64,9 +64,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new Text("lettuce"), new IntWritable(34)),
-        record(new Text("cabbage"), new IntWritable(18))));
+        record(new Text("cabbage"), new IntWritable(18)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -131,10 +131,10 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
+    ArrayWritable expected = list(
         record(new Text("lettuce"), new IntWritable(34)),
         record(new Text("kale"), null),
-        record(new Text("cabbage"), new IntWritable(18))));
+        record(new Text("cabbage"), new IntWritable(18)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -212,9 +212,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        record(new Text("green"), list(new Text("lettuce"), new Text("kale"), null)),
-        record(new Text("brown"), null)));
+    ArrayWritable expected = list(
+        record(new Text("green"), record(new Text("lettuce"), new Text("kale"), null)),
+        record(new Text("brown"), null));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -307,9 +307,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        record(new Text("low"), list(new IntWritable(34), new IntWritable(35), null)),
-        record(new Text("high"), list(new IntWritable(340), new IntWritable(360)))));
+    ArrayWritable expected = list(
+        record(new Text("low"), record(new IntWritable(34), new IntWritable(35), null)),
+        record(new Text("high"), record(new IntWritable(340), new IntWritable(360))));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -368,9 +368,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(record(
+    ArrayWritable expected = list(record(
         record(new IntWritable(7), new IntWritable(22)),
-        new DoubleWritable(3.14))));
+        new DoubleWritable(3.14)));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -429,9 +429,9 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(record(
+    ArrayWritable expected = list(record(
         new DoubleWritable(3.14),
-        record(new IntWritable(7), new IntWritable(22)))));
+        record(new IntWritable(7), new IntWritable(22))));
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
@@ -524,13 +524,13 @@ public void write(RecordConsumer rc) {
           }
         });
 
-    ArrayWritable expected = record(list(
-        record(new Text("a"), list(
+    ArrayWritable expected = list(
+        record(new Text("a"), record(
             record(new Text("b"), new IntWritable(1)))),
-        record(new Text("b"), list(
+        record(new Text("b"), record(
             record(new Text("a"), new IntWritable(-1)),
             record(new Text("b"), new IntWritable(-2))))
-    ));
+    );
 
     List<ArrayWritable> records = read(test);
     Assert.assertEquals("Should have only one record", 1, records.size());
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
index 21f889a..dbb2795 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetSerDe.java
@@ -55,7 +55,6 @@ public void testParquetHiveSerDe() throws Throwable {
       arr[4] = new DoubleWritable((double) 5.3);
       arr[5] = new BytesWritable("hive and hadoop and parquet. Big family.".getBytes("UTF-8"));
       arr[6] = new BytesWritable("parquetSerde binary".getBytes("UTF-8"));
-      final Writable[] mapContainer = new Writable[1];
       final Writable[] map = new Writable[3];
       for (int i = 0; i < 3; ++i) {
         final Writable[] pair = new Writable[2];
@@ -63,16 +62,13 @@ public void testParquetHiveSerDe() throws Throwable {
         pair[1] = new IntWritable(i);
         map[i] = new ArrayWritable(Writable.class, pair);
       }
-      mapContainer[0] = new ArrayWritable(Writable.class, map);
-      arr[7] = new ArrayWritable(Writable.class, mapContainer);
+      arr[7] = new ArrayWritable(Writable.class, map);
 
-      final Writable[] arrayContainer = new Writable[1];
       final Writable[] array = new Writable[5];
       for (int i = 0; i < 5; ++i) {
         array[i] = new BytesWritable(("elem_" + i).getBytes("UTF-8"));
       }
-      arrayContainer[0] = new ArrayWritable(Writable.class, array);
-      arr[8] = new ArrayWritable(Writable.class, arrayContainer);
+      arr[8] = new ArrayWritable(Writable.class, array);
 
       final ArrayWritable arrWritable = new ArrayWritable(Writable.class, arr);
       // Test
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
index ef05150..f5d9cb4 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestAbstractParquetMapInspector.java
@@ -71,11 +71,9 @@ public void testRegularMap() {
     final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
     final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
 
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{
       new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
 
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
     final Map<Writable, Writable> expected = new HashMap<Writable, Writable>();
     expected.put(new IntWritable(0), new IntWritable(1));
     expected.put(new IntWritable(2), new IntWritable(3));
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
index 8646ff4..1ca6861 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestDeepParquetHiveMapInspector.java
@@ -58,11 +58,9 @@ public void testRegularMap() {
     final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
     final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
 
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{
       new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
 
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
     assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
     assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
     assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new ShortWritable((short) 0)));
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
index f3a24af..0ce654d 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestParquetHiveArrayInspector.java
@@ -58,9 +58,8 @@ public void testEmptyContainer() {
 
   @Test
   public void testRegularList() {
-    final ArrayWritable internalList = new ArrayWritable(Writable.class,
+    final ArrayWritable list = new ArrayWritable(Writable.class,
             new Writable[]{new IntWritable(3), new IntWritable(5), new IntWritable(1)});
-    final ArrayWritable list = new ArrayWritable(ArrayWritable.class, new ArrayWritable[]{internalList});
 
     final List<Writable> expected = new ArrayList<Writable>();
     expected.add(new IntWritable(3));
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
index 278419f..950956a 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/serde/TestStandardParquetHiveMapInspector.java
@@ -57,11 +57,9 @@ public void testRegularMap() {
     final Writable[] entry1 = new Writable[]{new IntWritable(0), new IntWritable(1)};
     final Writable[] entry2 = new Writable[]{new IntWritable(2), new IntWritable(3)};
 
-    final ArrayWritable internalMap = new ArrayWritable(ArrayWritable.class, new Writable[]{
+    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{
       new ArrayWritable(Writable.class, entry1), new ArrayWritable(Writable.class, entry2)});
 
-    final ArrayWritable map = new ArrayWritable(ArrayWritable.class, new Writable[]{internalMap});
-
     assertEquals("Wrong result of inspection", new IntWritable(1), inspector.getMapValueElement(map, new IntWritable(0)));
     assertEquals("Wrong result of inspection", new IntWritable(3), inspector.getMapValueElement(map, new IntWritable(2)));
     assertNull("Wrong result of inspection", inspector.getMapValueElement(map, new ShortWritable((short) 0)));
-- 
1.7.9.5


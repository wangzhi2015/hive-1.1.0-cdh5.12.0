From 77a58fddf729d73929f534965c15753453b08806 Mon Sep 17 00:00:00 2001
From: Sergio Pena <sergio.pena@cloudera.com>
Date: Mon, 27 Feb 2017 10:12:28 -0600
Subject: [PATCH 0994/1164] CDH-50912: HIVE-15881: Use
 hive.exec.input.listing.max.threads variable name
 instead of mapred.dfsclient.parallelism.max
 (Sergio Pena, reviewed by Yongzhi Che, Thomas
 Poepping, Illya Yalovvy, Sahil Takiar)

Change-Id: Ied7f292a4064cdeecfe941119507f93732f75221
---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    4 +
 .../org/apache/hadoop/hive/ql/exec/Utilities.java  |   64 +++++--
 .../apache/hadoop/hive/ql/exec/TestUtilities.java  |  185 +++++++++++++++-----
 3 files changed, 196 insertions(+), 57 deletions(-)

diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index b5aad83..6ff9a50 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -2190,6 +2190,10 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {
             + "it will execute batch wise with the configured batch size. "
             + "The default value is zero. Zero means it will execute directly (Not batch wise)"),
 
+
+    HIVE_EXEC_INPUT_LISTING_MAX_THREADS("hive.exec.input.listing.max.threads", 0,
+        "Maximum number of threads that Hive uses to list file information from file systems (recommended > 1 for blobstore)."),
+
     /* BLOBSTORE section */
 
     HIVE_BLOBSTORE_SUPPORTED_SCHEMES("hive.blobstore.supported.schemes", "s3,s3a,s3n",
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 1251513..734b8da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -220,6 +220,9 @@
   public static final String MAPRED_REDUCER_CLASS = "mapred.reducer.class";
   public static final String HIVE_ADDED_JARS = "hive.added.jars";
 
+  @Deprecated
+  protected static String DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX = "mapred.dfsclient.parallelism.max";
+
   /**
    * ReduceField:
    * KEY: record key
@@ -2455,6 +2458,41 @@ public static void copyTablePropertiesToConf(TableDesc tbl, JobConf job) {
   private static final Object INPUT_SUMMARY_LOCK = new Object();
 
   /**
+   * Returns the maximum number of executors required to get file information from several input locations.
+   * It checks whether HIVE_EXEC_INPUT_LISTING_MAX_THREADS or DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX are > 1
+   *
+   * @param conf Configuration object to get the maximum number of threads.
+   * @param inputLocationListSize Number of input locations required to process.
+   * @return The maximum number of executors to use.
+   */
+  @VisibleForTesting
+  static int getMaxExecutorsForInputListing(final Configuration conf, int inputLocationListSize) {
+    if (inputLocationListSize < 1) return 0;
+
+    int maxExecutors = 1;
+
+    if (inputLocationListSize > 1) {
+      int listingMaxThreads = HiveConf.getIntVar(conf, ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS);
+
+      // DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX must be removed on next Hive version (probably on 3.0).
+      // If HIVE_EXEC_INPUT_LISTING_MAX_THREADS is not set, then we check of the deprecated configuration.
+      if (listingMaxThreads <= 0) {
+        listingMaxThreads = conf.getInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, 0);
+        if (listingMaxThreads > 0) {
+          LOG.warn("Deprecated configuration is used: " + DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX +
+              ". Please use " + ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname);
+        }
+      }
+
+      if (listingMaxThreads > 1) {
+        maxExecutors = Math.min(inputLocationListSize, listingMaxThreads);
+      }
+    }
+
+    return maxExecutors;
+  }
+
+  /**
    * Calculate the total size of input files.
    *
    * @param ctx
@@ -2503,9 +2541,9 @@ public static ContentSummary getInputSummary(final Context ctx, MapWork work, Pa
       final Map<String, ContentSummary> resultMap = new ConcurrentHashMap<String, ContentSummary>();
       ArrayList<Future<?>> results = new ArrayList<Future<?>>();
       final ExecutorService executor;
-      int maxThreads = ctx.getConf().getInt("mapred.dfsclient.parallelism.max", 0);
-      if (pathNeedProcess.size() > 1 && maxThreads > 1) {
-        int numExecutors = Math.min(pathNeedProcess.size(), maxThreads);
+
+      int numExecutors = getMaxExecutorsForInputListing(ctx.getConf(), pathNeedProcess.size());
+      if (numExecutors > 1) {
         LOG.info("Using " + numExecutors + " threads for getContentSummary");
         executor = Executors.newFixedThreadPool(numExecutors,
             new ThreadFactoryBuilder().setDaemon(true)
@@ -3358,19 +3396,6 @@ public static double getHighestSamplePercentage (MapWork work) {
   public static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScratchDir,
       Context ctx, boolean skipDummy) throws Exception {
 
-    int numThreads = job.getInt("mapred.dfsclient.parallelism.max", 0);
-    ExecutorService pool = null;
-    if (numThreads > 1) {
-      pool = Executors.newFixedThreadPool(numThreads,
-              new ThreadFactoryBuilder().setDaemon(true).setNameFormat("Get-Input-Paths-%d").build());
-    }
-    return getInputPaths(job, work, hiveScratchDir, ctx, skipDummy, pool);
-  }
-
-  @VisibleForTesting
-  static List<Path> getInputPaths(JobConf job, MapWork work, Path hiveScratchDir,
-      Context ctx, boolean skipDummy, ExecutorService pool) throws Exception {
-
     Set<Path> pathsProcessed = new HashSet<Path>();
     List<Path> pathsToAdd = new LinkedList<Path>();
     // AliasToWork contains all the aliases
@@ -3417,6 +3442,13 @@ public static double getHighestSamplePercentage (MapWork work) {
       }
     }
 
+    ExecutorService pool = null;
+    int numExecutors = getMaxExecutorsForInputListing(job, pathsToAdd.size());
+    if (numExecutors > 1) {
+      pool = Executors.newFixedThreadPool(numExecutors,
+          new ThreadFactoryBuilder().setDaemon(true).setNameFormat("Get-Input-Paths-%d").build());
+    }
+
     List<Path> finalPathsToAdd = new LinkedList<>();
     List<Future<Path>> futures = new LinkedList<>();
     for (final Path path : pathsToAdd) {
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
index f1807db..e322544 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
@@ -19,6 +19,7 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import static org.apache.hadoop.hive.ql.exec.Utilities.getFileExtension;
+import static org.apache.hadoop.hive.ql.exec.Utilities.DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX;
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.fail;
 import static org.mockito.Mockito.doReturn;
@@ -34,7 +35,6 @@
 import java.util.Properties;
 import java.util.UUID;
 import java.util.concurrent.ExecutorService;
-import java.util.concurrent.Executors;
 
 import com.google.common.collect.ImmutableMap;
 import com.google.common.collect.Lists;
@@ -272,60 +272,142 @@ private Path setupTempDirWithSingleOutputFile(Configuration hconf) throws IOExce
   }
 
   /**
-   * Test for {@link Utilities#getInputPaths(JobConf, MapWork, Path, Context, boolean, ExecutorService)} with a single
-   * threaded {@link ExecutorService}.
+   * Check that calling {@link Utilities#getMaxExecutorsForInputListing(Configuration, int)}
+   * returns the maximum number of executors to use based on the number of input locations.
    */
   @Test
-  public void testGetInputPathsWithPool() throws Exception {
-    ExecutorService pool = Executors.newSingleThreadExecutor();
+  public void testGetMaxExecutorsForInputListing() {
+    Configuration conf = new Configuration();
+
+    final int ZERO_EXECUTORS = 0;
+    final int ONE_EXECUTOR = 1;
+    final int TWO_EXECUTORS = 2;
+
+    final int ZERO_THREADS = 0;
+    final int ONE_THREAD = 1;
+    final int TWO_THREADS = 2;
+
+    final int ZERO_LOCATIONS = 0;
+    final int ONE_LOCATION = 1;
+    final int TWO_LOCATIONS = 2;
+    final int THREE_LOCATIONS = 3;
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, ONE_THREAD);
+
+    assertEquals(ZERO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, TWO_THREADS);
+
+    assertEquals(ZERO_EXECUTORS,  Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR,  Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+
+    /*
+     * The following tests will verify the deprecation variable is still usable.
+     */
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, ZERO_THREADS);
+    conf.setInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, ZERO_THREADS);
+
+    assertEquals(ZERO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, ZERO_THREADS);
+    conf.setInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, ONE_THREAD);
+
+    assertEquals(ZERO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, ZERO_THREADS);
+    conf.setInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, TWO_THREADS);
+
+    assertEquals(ZERO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+
+    // Check that HIVE_EXEC_INPUT_LISTING_MAX_THREADS has priority overr DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX
+
+    conf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, TWO_THREADS);
+    conf.setInt(DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, ONE_THREAD);
+
+    assertEquals(ZERO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, ZERO_LOCATIONS));
+    assertEquals(ONE_EXECUTOR, Utilities.getMaxExecutorsForInputListing(conf, ONE_LOCATION));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, TWO_LOCATIONS));
+    assertEquals(TWO_EXECUTORS, Utilities.getMaxExecutorsForInputListing(conf, THREE_LOCATIONS));
+  }
+
+  /**
+   * Test for {@link Utilities#getInputPaths(JobConf, MapWork, Path, Context, boolean)} with a single
+   * threaded.
+   */
+  @Test
+  public void testGetInputPathsWithASingleThread() throws Exception {
+    final int NUM_PARTITIONS = 5;
 
     JobConf jobConf = new JobConf();
+
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 1);
+    runTestGetInputPaths(jobConf, NUM_PARTITIONS);
+  }
+
+  /**
+   * Test for {@link Utilities#getInputPaths(JobConf, MapWork, Path, Context, boolean)} with multiple
+   * threads.
+   */
+  @Test
+  public void testGetInputPathsWithMultipleThreads() throws Exception {
+    final int NUM_PARTITIONS = 5;
+
+    JobConf jobConf = new JobConf();
+
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 2);
+    runTestGetInputPaths(jobConf, NUM_PARTITIONS);
+  }
+
+  private void runTestGetInputPaths(JobConf jobConf, int numOfPartitions) throws Exception {
     MapWork mapWork = new MapWork();
     Path scratchDir = new Path(HiveConf.getVar(jobConf, HiveConf.ConfVars.LOCALSCRATCHDIR));
 
+    LinkedHashMap<String, ArrayList<String>> pathToAliasTable = new LinkedHashMap<>();
+
     String testTableName = "testTable";
-    String testPartitionName = "testPartition";
 
     Path testTablePath = new Path(testTableName);
-    Path testPartitionPath = new Path(testTablePath, testPartitionName);
-    Path testFileTablePath = new Path(testTablePath, "test.txt");
-    Path testFilePartitionPath = new Path(testPartitionPath, "test.txt");
+    Path[] testPartitionsPaths = new Path[numOfPartitions];
+    for (int i=0; i<numOfPartitions; i++) {
+      String testPartitionName = "p=" + 1;
+      testPartitionsPaths[i] = new Path(testTablePath, "p=" + i);
 
-    TableDesc mockTableDesc = mock(TableDesc.class);
+      pathToAliasTable.put(testPartitionsPaths[i].toString(), Lists.newArrayList(testPartitionName));
 
-    when(mockTableDesc.isNonNative()).thenReturn(false);
-    when(mockTableDesc.getProperties()).thenReturn(new Properties());
+      mapWork.getAliasToWork().put(testPartitionName, (Operator<?>) mock(Operator.class));
+    }
 
-    LinkedHashMap<String, ArrayList<String>> pathToAliasTable = new LinkedHashMap<>();
-    pathToAliasTable.put(testTablePath.toString(), Lists.newArrayList(testTableName));
     mapWork.setPathToAliases(pathToAliasTable);
 
-    mapWork.getAliasToWork().put(testTableName, (Operator<?>) mock(Operator.class));
-
     FileSystem fs = FileSystem.getLocal(jobConf);
     try {
       fs.mkdirs(testTablePath);
-      fs.create(testFileTablePath).close();
-
-      // Run a test with an un-partitioned table with a single file as the input
-      List<Path> tableInputPaths = Utilities.getInputPaths(jobConf, mapWork, scratchDir, mock(Context.class), false,
-              pool);
-      assertEquals(tableInputPaths.size(), 1);
-      assertEquals(tableInputPaths.get(0), testTablePath);
-
-      LinkedHashMap<String, ArrayList<String>> pathToAliasPartition = new LinkedHashMap<>();
-      pathToAliasPartition.put(testPartitionPath.toString(), Lists.newArrayList(testTableName));
-      mapWork.setPathToAliases(pathToAliasPartition);
-
-      fs.delete(testFileTablePath, false);
-      fs.mkdirs(testPartitionPath);
-      fs.create(testFilePartitionPath).close();
-
-      // Run a test with a partitioned table with a single partition and a single file as the input
-      List<Path> tablePartitionInputPaths = Utilities.getInputPaths(jobConf, mapWork, scratchDir, mock(Context.class),
-              false, pool);
-      assertEquals(tablePartitionInputPaths.size(), 1);
-      assertEquals(tablePartitionInputPaths.get(0), testPartitionPath);
+
+      for (int i=0; i<numOfPartitions; i++) {
+        fs.mkdirs(testPartitionsPaths[i]);
+        fs.create(new Path(testPartitionsPaths[i], "test1.txt")).close();
+      }
+
+      List<Path> inputPaths = Utilities.getInputPaths(jobConf, mapWork, scratchDir, mock(Context.class), false);
+      assertEquals(inputPaths.size(), numOfPartitions);
+      for (int i=0; i<numOfPartitions; i++) {
+        assertEquals(inputPaths.get(i), testPartitionsPaths[i]);
+      }
     } finally {
       if (fs.exists(testTablePath)) {
         fs.delete(testTablePath, true);
@@ -341,7 +423,7 @@ public void testGetInputSummaryWithASingleThread() throws IOException {
     JobConf jobConf = new JobConf();
     Properties properties = new Properties();
 
-    jobConf.setInt("mapred.dfsclient.parallelism.max", 0);
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 0);
     ContentSummary summary = runTestGetInputSummary(jobConf, properties, NUM_PARTITIONS, BYTES_PER_FILE, HiveInputFormat.class);
     assertEquals(NUM_PARTITIONS * BYTES_PER_FILE, summary.getLength());
     assertEquals(NUM_PARTITIONS, summary.getFileCount());
@@ -356,11 +438,19 @@ public void testGetInputSummaryWithMultipleThreads() throws IOException {
     JobConf jobConf = new JobConf();
     Properties properties = new Properties();
 
-    jobConf.setInt("mapred.dfsclient.parallelism.max", 2);
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 2);
     ContentSummary summary = runTestGetInputSummary(jobConf, properties, NUM_PARTITIONS, BYTES_PER_FILE, HiveInputFormat.class);
     assertEquals(NUM_PARTITIONS * BYTES_PER_FILE, summary.getLength());
     assertEquals(NUM_PARTITIONS, summary.getFileCount());
     assertEquals(NUM_PARTITIONS, summary.getDirectoryCount());
+
+    // Test deprecated mapred.dfsclient.parallelism.max
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 0);
+    jobConf.setInt(Utilities.DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, 2);
+    summary = runTestGetInputSummary(jobConf, properties, NUM_PARTITIONS, BYTES_PER_FILE, HiveInputFormat.class);
+    assertEquals(NUM_PARTITIONS * BYTES_PER_FILE, summary.getLength());
+    assertEquals(NUM_PARTITIONS, summary.getFileCount());
+    assertEquals(NUM_PARTITIONS, summary.getDirectoryCount());
   }
 
   @Test
@@ -372,7 +462,7 @@ public void testGetInputSummaryWithInputEstimator() throws IOException, HiveExce
     JobConf jobConf = new JobConf();
     Properties properties = new Properties();
 
-    jobConf.setInt("mapred.dfsclient.parallelism.max", 2);
+    jobConf.setInt(Utilities.DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, 2);
 
     properties.setProperty(hive_metastoreConstants.META_TABLE_STORAGE, InputEstimatorTestClass.class.getName());
     InputEstimatorTestClass.setEstimation(new InputEstimator.Estimation(NUM_OF_ROWS, BYTES_PER_FILE));
@@ -382,6 +472,19 @@ public void testGetInputSummaryWithInputEstimator() throws IOException, HiveExce
     assertEquals(NUM_PARTITIONS * BYTES_PER_FILE, summary.getLength());
     assertEquals(NUM_PARTITIONS * -1, summary.getFileCount());        // Current getInputSummary() returns -1 for each file found
     assertEquals(NUM_PARTITIONS * -1, summary.getDirectoryCount());   // Current getInputSummary() returns -1 for each file found
+
+    // Test deprecated mapred.dfsclient.parallelism.max
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 0);
+    jobConf.setInt(HiveConf.ConfVars.HIVE_EXEC_INPUT_LISTING_MAX_THREADS.varname, 2);
+
+    properties.setProperty(hive_metastoreConstants.META_TABLE_STORAGE, InputEstimatorTestClass.class.getName());
+    InputEstimatorTestClass.setEstimation(new InputEstimator.Estimation(NUM_OF_ROWS, BYTES_PER_FILE));
+
+    /* Let's write more bytes to the files to test that Estimator is actually working returning the file size not from the filesystem */
+    summary = runTestGetInputSummary(jobConf, properties, NUM_PARTITIONS, BYTES_PER_FILE * 2, HiveInputFormat.class);
+    assertEquals(NUM_PARTITIONS * BYTES_PER_FILE, summary.getLength());
+    assertEquals(NUM_PARTITIONS * -1, summary.getFileCount());        // Current getInputSummary() returns -1 for each file found
+    assertEquals(NUM_PARTITIONS * -1, summary.getDirectoryCount());   // Current getInputSummary() returns -1 for each file found
   }
 
   static class ContentSummaryInputFormatTestClass extends FileInputFormat implements ContentSummaryInputFormat {
@@ -410,7 +513,7 @@ public void testGetInputSummaryWithContentSummaryInputFormat() throws IOExceptio
     JobConf jobConf = new JobConf();
     Properties properties = new Properties();
 
-    jobConf.setInt("mapred.dfsclient.parallelism.max", 2);
+    jobConf.setInt(Utilities.DEPRECATED_MAPRED_DFSCLIENT_PARALLELISM_MAX, 2);
 
     ContentSummaryInputFormatTestClass.setContentSummary(new ContentSummary(BYTES_PER_FILE, 2, 1));
 
-- 
1.7.9.5


From b220eb1f7e3c64322464115b55440300ff01fd61 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Tue, 2 Feb 2016 18:03:44 -0800
Subject: [PATCH 0819/1164] HIVE-12988 : Improve dynamic partition loading IV
 (Ashutosh Chauhan via Prasanth J)

(cherry picked from commit a14ef8abe1df1516b8b9f486030bc3d584f940a9)

Conflicts:
	common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
	ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java

Change-Id: I28e7e5b9e52d7ac3bbf50df6ee04fd97ff1191e5
---
 .../org/apache/hadoop/hive/ql/metadata/Hive.java   |  287 +++++++++++---------
 .../java/org/apache/hadoop/fs/ProxyFileSystem.java |    5 +-
 2 files changed, 165 insertions(+), 127 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
index b456306..e44a49e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/Hive.java
@@ -30,7 +30,7 @@
 import java.io.FileNotFoundException;
 import java.io.IOException;
 import java.util.ArrayList;
-import java.util.Arrays;
+import java.util.Collections;
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.Iterator;
@@ -41,6 +41,11 @@
 import java.util.Map;
 import java.util.Map.Entry;
 import java.util.Set;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.atomic.AtomicInteger;
 
 import com.google.common.collect.ImmutableMap;
 import org.apache.commons.logging.Log;
@@ -125,7 +130,7 @@
 import org.apache.thrift.TException;
 
 import com.google.common.collect.Sets;
-import java.util.concurrent.atomic.AtomicInteger;
+import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 /**
  * This class has functions that implement meta data/DDL operations using calls
@@ -1454,7 +1459,7 @@ public Partition loadPartition(Path loadPath, Table tbl,
         Hive.replaceFiles(tbl.getPath(), loadPath, newPartPath, oldPartPath, getConf(),
             isSrcLocal);
       } else {
-        newFiles = new ArrayList<Path>();
+        newFiles = Collections.synchronizedList(new ArrayList<Path>());
         FileSystem fs = tbl.getDataLocation().getFileSystem(conf);
         Hive.copyFiles(conf, loadPath, newPartPath, fs, isSrcLocal, isAcid, newFiles);
       }
@@ -1678,9 +1683,13 @@ private void constructOneLBLocationMap(FileStatus fSta,
   public void loadTable(Path loadPath, String tableName, boolean replace,
       boolean holdDDLTime, boolean isSrcLocal, boolean isSkewedStoreAsSubdir, boolean isAcid)
       throws HiveException {
-    List<Path> newFiles = new ArrayList<Path>();
+
+    List<Path> newFiles = null;
     Table tbl = getTable(tableName);
     HiveConf sessionConf = SessionState.getSessionConf();
+    if (conf.getBoolVar(ConfVars.FIRE_EVENTS_FOR_DML) && !tbl.isTemporary()) {
+      newFiles = Collections.synchronizedList(new ArrayList<Path>());
+    }
     if (replace) {
       Path tableDest = tbl.getPath();
       replaceFiles(tableDest, loadPath, tableDest, tableDest, sessionConf, isSrcLocal);
@@ -2470,82 +2479,92 @@ public PrincipalPrivilegeSet get_privilege_set(HiveObjectType objectType,
     }
   }
 
-  // for each file or directory in 'srcs', make mapping for every file in src to safe name in dest
-  private static List<List<Path[]>> checkPaths(HiveConf conf, FileSystem fs,
-      FileStatus[] srcs, FileSystem srcFs, Path destf, boolean replace)
-      throws HiveException {
+  private static void copyFiles(final HiveConf conf, final FileSystem destFs,
+      FileStatus[] srcs, final FileSystem srcFs, final Path destf, final boolean isSrcLocal, final List<Path> newFiles)
+          throws HiveException {
 
-    List<List<Path[]>> result = new ArrayList<List<Path[]>>();
+    final HadoopShims.HdfsFileStatus fullDestStatus;
     try {
-      FileStatus destStatus = !replace && fs.exists(destf) ? fs.getFileStatus(destf) : null;
-      if (destStatus != null && !destStatus.isDir()) {
-        throw new HiveException("checkPaths: destination " + destf
-            + " should be a directory");
-      }
-      for (FileStatus src : srcs) {
-        FileStatus[] items;
-        if (src.isDir()) {
-          items = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);
-          Arrays.sort(items);
+      fullDestStatus = ShimLoader.getHadoopShims().getFullFileStatus(conf, destFs, destf);
+    } catch (IOException e1) {
+      throw new HiveException(e1);
+    }
+
+    if (!fullDestStatus.getFileStatus().isDirectory()) {
+      throw new HiveException(destf + " is not a directory.");
+    }
+    final boolean inheritPerms = HiveConf.getBoolVar(conf,
+        HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);
+    final List<Future<ObjectPair<Path, Path>>> futures = new LinkedList<>();
+    final ExecutorService pool = Executors.newFixedThreadPool(
+        conf.getIntVar(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT),
+        new ThreadFactoryBuilder().setDaemon(true).setNameFormat("MoveDir-Thread-%d").build());
+
+    for (FileStatus src : srcs) {
+      FileStatus[] files;
+      if (src.isDirectory()) {
+        try {
+          files = srcFs.listStatus(src.getPath(), FileUtils.HIDDEN_FILES_PATH_FILTER);
+        } catch (IOException e) {
+          pool.shutdownNow();
+          throw new HiveException(e);
+        }
+      } else {
+        files = new FileStatus[] {src};
+      }
+
+      for (FileStatus srcFile : files) {
+
+        final Path srcP = srcFile.getPath();
+        final boolean needToCopy = needToCopy(srcP, destf, srcFs, destFs);
+        // Strip off the file type, if any so we don't make:
+        // 000000_0.gz -> 000000_0.gz_copy_1
+        final String name;
+        final String filetype;
+        String itemName = srcP.getName();
+        int index = itemName.lastIndexOf('.');
+        if (index >= 0) {
+          filetype = itemName.substring(index);
+          name = itemName.substring(0, index);
         } else {
-          items = new FileStatus[] {src};
+          name = itemName;
+          filetype = "";
         }
+        futures.add(pool.submit(new Callable<ObjectPair<Path, Path>>() {
+          @Override
+          public ObjectPair<Path, Path> call() throws Exception {
+            Path destPath = new Path(destf, srcP.getName());
+            if (!needToCopy && !isSrcLocal) {
+              for (int counter = 1; !destFs.rename(srcP,destPath); counter++) {
+                destPath = new Path(destf, name + ("_copy_" + counter) + filetype);
+              }
+            } else {
+              destPath = mvFile(conf, srcP, destPath, isSrcLocal, srcFs, destFs, name, filetype);
+            }
 
-        List<Path[]> srcToDest = new ArrayList<Path[]>();
-        for (FileStatus item : items) {
-
-          Path itemSource = item.getPath();
-
-          if (Utilities.isTempPath(item)) {
-            // This check is redundant because temp files are removed by
-            // execution layer before
-            // calling loadTable/Partition. But leaving it in just in case.
-            srcFs.delete(itemSource, true);
-            continue;
-          }
-
-          if (!conf.getBoolVar(HiveConf.ConfVars.HIVE_HADOOP_SUPPORTS_SUBDIRECTORIES) &&
-            !HiveConf.getVar(conf, HiveConf.ConfVars.STAGINGDIR).equals(itemSource.getName()) &&
-            item.isDir()) {
-            throw new HiveException("checkPaths: " + src.getPath()
-                + " has nested directory " + itemSource);
-          }
-          // Strip off the file type, if any so we don't make:
-          // 000000_0.gz -> 000000_0.gz_copy_1
-          String name = itemSource.getName();
-          String filetype;
-          int index = name.lastIndexOf('.');
-          if (index >= 0) {
-            filetype = name.substring(index);
-            name = name.substring(0, index);
-          } else {
-            filetype = "";
-          }
-
-          Path itemDest = new Path(destf, itemSource.getName());
-
-          if (!replace) {
-            // It's possible that the file we're copying may have the same
-            // relative name as an existing file in the "destf" directory.
-            // So let's make a quick check to see if we can rename any
-            // potential offenders so as to allow them to move into the
-            // "destf" directory. The scheme is dead simple: simply tack
-            // on "_copy_N" where N starts at 1 and works its way up until
-            // we find a free space.
-
-            // removed source file staging.. it's more confusing when failed.
-            for (int counter = 1; fs.exists(itemDest) || destExists(result, itemDest); counter++) {
-              itemDest = new Path(destf, name + ("_copy_" + counter) + filetype);
+            if (inheritPerms) {
+              ShimLoader.getHadoopShims().setFullFileStatus(conf, fullDestStatus, destFs, destf);
+            }
+            if (null != newFiles) {
+              newFiles.add(destPath);
             }
+            return ObjectPair.create(srcP, destPath);
           }
-          srcToDest.add(new Path[]{itemSource, itemDest});
-        }
-        result.add(srcToDest);
+        }));
+      }
+    }
+    pool.shutdown();
+    for (Future<ObjectPair<Path, Path>> future : futures) {
+      try {
+        ObjectPair<Path, Path> pair = future.get();
+        LOG.debug(String.format("Moved src: {}", pair.getFirst().toString(), ", to dest: {}",
+                pair.getSecond().toString()));
+      } catch (Exception e) {
+        LOG.error("Failed to move: {}", e);
+        pool.shutdownNow();
+        throw new HiveException(e.getCause());
       }
-    } catch (IOException e) {
-      throw new HiveException("checkPaths: filesystem error in check phase. " + e.getMessage(), e);
     }
-    return result;
   }
 
   private static boolean destExists(List<List<Path[]>> result, Path proposed) {
@@ -2604,14 +2623,14 @@ private static String getQualifiedPathWithoutSchemeAndAuthority(Path srcf, FileS
 
   // Clears the dest dir when src is sub-dir of dest.
   public static void clearDestForSubDirSrc(final HiveConf conf, Path dest,
-      Path src, boolean isSrcLocal) throws IOException {
+                                           Path src, boolean isSrcLocal) throws IOException {
     FileSystem destFS = dest.getFileSystem(conf);
     FileSystem srcFS = src.getFileSystem(conf);
     if (isSubDir(src, dest, srcFS, destFS, isSrcLocal)) {
       final Path fullSrcPath = new Path(
-          getQualifiedPathWithoutSchemeAndAuthority(src, srcFS));
+              getQualifiedPathWithoutSchemeAndAuthority(src, srcFS));
       final Path fullDestPath = new Path(
-          getQualifiedPathWithoutSchemeAndAuthority(dest, destFS));
+              getQualifiedPathWithoutSchemeAndAuthority(dest, destFS));
       if (fullSrcPath.equals(fullDestPath)) {
         return;
       }
@@ -2620,7 +2639,7 @@ public static void clearDestForSubDirSrc(final HiveConf conf, Path dest,
         parent = parent.getParent();
       }
       FileStatus[] existingFiles = destFS.listStatus(
-          dest, FileUtils.HIDDEN_FILES_PATH_FILTER);
+              dest, FileUtils.HIDDEN_FILES_PATH_FILTER);
       for (FileStatus fileStatus : existingFiles) {
         if (!fileStatus.getPath().getName().equals(parent.getName())) {
           destFS.delete(fileStatus.getPath(), true);
@@ -2629,14 +2648,34 @@ public static void clearDestForSubDirSrc(final HiveConf conf, Path dest,
     }
   }
 
+  private static Path mvFile(HiveConf conf, Path srcf, Path destf, boolean isSrcLocal,
+      FileSystem srcFs, FileSystem destFs, String srcName, String filetype) throws IOException {
+
+    for (int counter = 1; destFs.exists(destf); counter++) {
+      destf = new Path(destf.getParent(), srcName + ("_copy_" + counter) + filetype);
+    }
+    if (isSrcLocal) {
+      // For local src file, copy to hdfs
+      destFs.copyFromLocalFile(srcf, destf);
+    } else {
+      //copy if across file system or encryption zones.
+      LOG.info("Copying source " + srcf + " to " + destf + " because HDFS encryption zones are different.");
+      FileUtils.copy(srcFs, srcf, destFs, destf,
+          true,    // delete source
+          false, // overwrite destination
+          conf);
+    }
+    return destf;
+  }
+
   //it is assumed that parent directory of the destf should already exist when this
   //method is called. when the replace value is true, this method works a little different
   //from mv command if the destf is a directory, it replaces the destf instead of moving under
   //the destf. in this case, the replaced destf still preserves the original destf's permission
-  public static boolean moveFile(HiveConf conf, Path srcf, Path destf,
+  public static boolean moveFile(HiveConf conf, Path srcf, final Path destf,
       boolean replace, boolean isSrcLocal) throws HiveException {
     boolean success = false;
-    FileSystem srcFs, destFs;
+    final FileSystem srcFs, destFs;
     try {
       destFs = destf.getFileSystem(conf);
     } catch (IOException e) {
@@ -2700,32 +2739,38 @@ public static boolean moveFile(HiveConf conf, Path srcf, Path destf,
             FileStatus[] srcs = destFs.listStatus(srcf, FileUtils.HIDDEN_FILES_PATH_FILTER);
             if (srcs.length == 0) {
               success = true; // Nothing to move.
-            }
-
-            /* Move files one by one because source is a subdirectory of destination */
-            for (FileStatus status : srcs) {
-              Path destFile;
-
-              /* Append the source filename to the destination directory */
-              if (destFs.isDirectory(destf)) {
-                destFile = new Path(destf, status.getPath().getName());
-              } else {
-                destFile = destf;
+            } else {
+              List<Future<Boolean>> futures = new LinkedList<>();
+              final ExecutorService pool = Executors.newFixedThreadPool(
+                  conf.getIntVar(ConfVars.HIVE_MOVE_FILES_THREAD_COUNT),
+                  new ThreadFactoryBuilder().setDaemon(true).setNameFormat("MoveDir-Thread-%d").build());
+              /* Move files one by one because source is a subdirectory of destination */
+              for (final FileStatus status : srcs) {
+                futures.add(pool.submit(new Callable<Boolean>() {
+                  @Override
+                  public Boolean call() throws Exception {
+                    return destFs.rename(status.getPath(), destf);
+                  }
+                }));
               }
-
-              // Destination should be replaced, so we delete it first
-              if (destFs.exists(destFile)) {
-                if (!destFs.delete(destFile, true)) {
-                  throw new HiveException(String.format("File to replace could not be deleted: %s", destFile));
+              pool.shutdown();
+              boolean allFutures = true;
+              for (Future<Boolean> future : futures) {
+                try {
+                  Boolean result = future.get();
+                  allFutures &= result;
+                  if (!result) {
+                    LOG.debug("Failed to rename.");
+                    pool.shutdownNow();
+                  }
+                } catch (Exception e) {
+                  LOG.debug("Failed to rename.", e);
+                  pool.shutdownNow();
+                  throw new HiveException(e.getCause());
                 }
               }
-
-              if (!(destFs.rename(status.getPath(), destFile))) {
-                throw new HiveException("Unable to move source " + status.getPath() + " to destination " + destf);
-              }
+              success = allFutures;
             }
-
-            success = true;
           } else {
             success = destFs.rename(srcf, destf);
           }
@@ -2751,8 +2796,9 @@ public static boolean moveFile(HiveConf conf, Path srcf, Path destf,
   /**
    * If moving across different FileSystems or differnent encryption zone, need to do a File copy instead of rename.
    * TODO- consider if need to do this for different file authority.
+   * @throws HiveException
    */
-  static protected boolean needToCopy(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs) throws HiveException, IOException {
+  static protected boolean needToCopy(Path srcf, Path destf, FileSystem srcFs, FileSystem destFs) throws HiveException {
     //Check if different FileSystems
     if (!srcFs.getClass().equals(destFs.getClass())) {
       return true;
@@ -2760,8 +2806,12 @@ static protected boolean needToCopy(Path srcf, Path destf, FileSystem srcFs, Fil
 
     //Check if different encryption zones
     HadoopShims.HdfsEncryptionShim hdfsEncryptionShim = SessionState.get().getHdfsEncryptionShim();
-    return hdfsEncryptionShim != null && (hdfsEncryptionShim.isPathEncrypted(srcf) || hdfsEncryptionShim.isPathEncrypted(destf))
-      && !hdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf);
+    try {
+      return hdfsEncryptionShim != null && (hdfsEncryptionShim.isPathEncrypted(srcf) || hdfsEncryptionShim.isPathEncrypted(destf))
+        && !hdfsEncryptionShim.arePathsOnSameEncryptionZone(srcf, destf);
+    } catch (IOException e) {
+      throw new HiveException(e);
+    }
   }
 
   /**
@@ -2812,22 +2862,7 @@ static protected void copyFiles(HiveConf conf, Path srcf, Path destf,
     if (isAcid) {
       moveAcidFiles(srcFs, srcs, destf, newFiles);
     } else {
-    // check that source and target paths exist
-      List<List<Path[]>> result = checkPaths(conf, fs, srcs, srcFs, destf, false);
-      // move it, move it
-      try {
-        for (List<Path[]> sdpairs : result) {
-          for (Path[] sdpair : sdpairs) {
-            if (!moveFile(conf, sdpair[0], sdpair[1], false, isSrcLocal)) {
-              throw new IOException("Cannot move " + sdpair[0] + " to "
-                  + sdpair[1]);
-            }
-            if (newFiles != null) newFiles.add(sdpair[1]);
-          }
-        }
-      } catch (IOException e) {
-        throw new HiveException("copyFiles: error while moving files!!! " + e.getMessage(), e);
-      }
+      copyFiles(conf, fs, srcs, srcFs, destf, isSrcLocal, newFiles);
     }
   }
 
@@ -2931,9 +2966,6 @@ protected static void replaceFiles(Path tablePath, Path srcf, Path destf, Path o
     try {
 
       FileSystem destFs = destf.getFileSystem(conf);
-      boolean inheritPerms = HiveConf.getBoolVar(conf,
-          HiveConf.ConfVars.HIVE_WAREHOUSE_SUBDIR_INHERIT_PERMS);
-
       // check if srcf contains nested sub-directories
       FileStatus[] srcs;
       FileSystem srcFs;
@@ -2947,7 +2979,6 @@ protected static void replaceFiles(Path tablePath, Path srcf, Path destf, Path o
         LOG.info("No sources specified to move: " + srcf);
         return;
       }
-      List<List<Path[]>> result = checkPaths(conf, destFs, srcs, srcFs, destf, true);
 
       if (oldPath != null) {
         boolean oldPathDeleted = false;
@@ -2996,10 +3027,14 @@ protected static void replaceFiles(Path tablePath, Path srcf, Path destf, Path o
       // directory if it is the root of an HDFS encryption zone.
       // 2. srcs must be a list of files -- ensured by LoadSemanticAnalyzer
       // in both cases, we move the file under destf
-      for (List<Path[]> sdpairs : result) {
-        for (Path[] sdpair : sdpairs) {
-          if (!moveFile(conf, sdpair[0], sdpair[1], true, isSrcLocal)) {
-            throw new IOException("Error moving: " + sdpair[0] + " into: " + sdpair[1]);
+      if (srcs.length == 1 && srcs[0].isDirectory()) {
+        if (!moveFile(conf, srcs[0].getPath(), destf, true, isSrcLocal)) {
+          throw new IOException("Error moving: " + srcf + " into: " + destf);
+        }
+      } else { // its either a file or glob
+        for (FileStatus src : srcs) {
+          if (!moveFile(conf, src.getPath(), new Path(destf, src.getPath().getName()), true, isSrcLocal)) {
+            throw new IOException("Error moving: " + srcf + " into: " + destf);
           }
         }
       }
diff --git a/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java b/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
index cb1e2b7..2c37a51 100644
--- a/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
+++ b/shims/common/src/main/java/org/apache/hadoop/fs/ProxyFileSystem.java
@@ -82,6 +82,7 @@ public ProxyFileSystem(FileSystem fs) {
    * @return
    * @throws IOException
    */
+  @Override
   public Path resolvePath(final Path p) throws IOException {
     // Return the fully-qualified path of path f resolving the path
     // through any symlinks or mount point
@@ -174,7 +175,9 @@ public boolean setReplication(Path src, short replication) throws IOException {
 
   @Override
   public boolean rename(Path src, Path dst) throws IOException {
-    return super.rename(swizzleParamPath(src), swizzleParamPath(dst));
+    Path dest = swizzleParamPath(dst);
+    // Make sure for existing destination we return false as per FileSystem api contract
+    return super.isFile(dest) ? false : super.rename(swizzleParamPath(src), dest);
   }
 
   @Override
-- 
1.7.9.5


From c3f2b2cfb4316b3aae9c28586895bb57618d7c47 Mon Sep 17 00:00:00 2001
From: Yongzhi Chen <yongzhi_chen@hotmail.com>
Date: Tue, 16 Feb 2016 10:22:53 -0600
Subject: [PATCH 0503/1164] CDH-37322: HIVE-13039: BETWEEN predicate is not
 functioning correctly with predicate pushdown on
 Parquet table (Yongzhi Chen, reviewed by Sergio
 Pena)

Conflicts:
	ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestConvertAstToSearchArg.java

Change-Id: Ibf055163ab1aa6f1616e68b692169f90e636c26c
---
 .../ql/io/parquet/FilterPredicateLeafBuilder.java  |    4 +-
 .../io/parquet/TestParquetRecordReaderWrapper.java |    2 +-
 .../parquet/read/TestParquetFilterPredicate.java   |    3 +-
 .../hive/ql/io/sarg/TestSearchArgumentImpl.java    |    8 +--
 ql/src/test/queries/clientpositive/parquet_ppd.q   |   20 +++++++
 .../test/results/clientpositive/parquet_ppd.q.out  |   61 ++++++++++++++++++++
 6 files changed, 90 insertions(+), 8 deletions(-)
 create mode 100644 ql/src/test/queries/clientpositive/parquet_ppd.q
 create mode 100644 ql/src/test/results/clientpositive/parquet_ppd.q.out

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/FilterPredicateLeafBuilder.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/FilterPredicateLeafBuilder.java
index 2797654..cd6b891 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/FilterPredicateLeafBuilder.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/FilterPredicateLeafBuilder.java
@@ -57,9 +57,9 @@ public FilterPredicate buildPredicate(PredicateLeaf.Operator op, List<Object> li
         }
         Object min = literals.get(0);
         Object max = literals.get(1);
-        FilterPredicate lt = not(buildPredict(PredicateLeaf.Operator.LESS_THAN_EQUALS,
+        FilterPredicate lt = not(buildPredict(PredicateLeaf.Operator.LESS_THAN,
             min, columnName));
-        FilterPredicate gt = buildPredict(PredicateLeaf.Operator.LESS_THAN, max, columnName);
+        FilterPredicate gt = buildPredict(PredicateLeaf.Operator.LESS_THAN_EQUALS, max, columnName);
         result = FilterApi.and(gt, lt);
         return result;
       default:
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetRecordReaderWrapper.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetRecordReaderWrapper.java
index a1e9471..b7ee1e5 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetRecordReaderWrapper.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/TestParquetRecordReaderWrapper.java
@@ -64,7 +64,7 @@ public void testBuilder() throws Exception {
     FilterPredicate p =
         ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);
     String expected =
-      "and(and(and(not(eq(x, null)), not(and(lt(y, 20), not(lteq(y, 10))))), not(or(or(eq(z, 1), " +
+      "and(and(and(not(eq(x, null)), not(and(lteq(y, 20), not(lt(y, 10))))), not(or(or(eq(z, 1), " +
         "eq(z, 2)), eq(z, 3)))), not(eq(a, Binary{\"stinger\"})))";
     assertEquals(expected, p.toString());
   }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/read/TestParquetFilterPredicate.java b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/read/TestParquetFilterPredicate.java
index 12eca38..edc727c 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/read/TestParquetFilterPredicate.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/parquet/read/TestParquetFilterPredicate.java
@@ -65,7 +65,8 @@ public void testFilterFloatColumns() {
     FilterPredicate p = ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);
 
     String expected =
-        "and(and(not(eq(a, null)), not(and(lt(a, 20.3), not(lteq(a, 10.2))))), not(or(or(eq(b, 1), eq(b, 2)), eq(b, 3))))";
+        "and(and(not(eq(a, null)), not(and(lteq(a, 20.3), not(lt(a, 10.2))))), not(or(or(eq(b, 1), eq(b, 2)), eq(b, 3))))";
     assertEquals(expected, p.toString());
   }
+
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java b/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java
index 2463da5..fedf077 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/io/sarg/TestSearchArgumentImpl.java
@@ -1518,8 +1518,8 @@ public void testExpression3() throws Exception {
     assertEquals(3, leaves.size());
 
     String[] conditions = new String[]{
-      "lt(id, 45)",                         /* id between 23 and 45 */
-      "not(lteq(id, 23))",                   /* id between 23 and 45 */
+      "lteq(id, 45)",                         /* id between 23 and 45 */
+      "not(lt(id, 23))",                   /* id between 23 and 45 */
       "eq(first_name, Binary{\"alan\"})",   /* first_name = 'alan'  */
       "eq(last_name, Binary{\"smith\"})"    /* 'smith' = last_name  */
     };
@@ -2022,7 +2022,7 @@ public void testExpression5() throws Exception {
         " required binary first_name; required int32 id; }");
     FilterPredicate p = ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);
     String expected =
-      "and(lt(first_name, Binary{\"greg\"}), not(lteq(first_name, Binary{\"david\"})))";
+      "and(lteq(first_name, Binary{\"greg\"}), not(lt(first_name, Binary{\"david\"})))";
     assertEquals(p.toString(), expected);
 
     assertEquals(PredicateLeaf.Type.STRING, leaves.get(0).getType());
@@ -2991,7 +2991,7 @@ public void testBuilder() throws Exception {
 
     FilterPredicate p = ParquetFilterPredicateConverter.toFilterPredicate(sarg, schema);
     String expected =
-      "and(and(and(not(eq(x, null)), not(and(lt(y, 20), not(lteq(y, 10))))), not(or(or(eq(z, 1), " +
+      "and(and(and(not(eq(x, null)), not(and(lteq(y, 20), not(lt(y, 10))))), not(or(or(eq(z, 1), " +
         "eq(z, 2)), eq(z, 3)))), not(eq(a, Binary{\"stinger\"})))";
     assertEquals(expected, p.toString());
   }
diff --git a/ql/src/test/queries/clientpositive/parquet_ppd.q b/ql/src/test/queries/clientpositive/parquet_ppd.q
new file mode 100644
index 0000000..56ca96e
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/parquet_ppd.q
@@ -0,0 +1,20 @@
+CREATE TABLE parquet_tbl(
+  key int,
+  ldate string)
+ PARTITIONED BY (
+ lyear string )
+ ROW FORMAT SERDE
+ 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
+ STORED AS INPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
+ OUTPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat';
+
+insert overwrite table parquet_tbl partition (lyear='2016') select
+  1,
+  '2016-02-03' from src limit 1;
+
+set hive.optimize.ppd.storage = true;
+set hive.optimize.ppd = true;
+select * from parquet_tbl where ldate between '2016-02-03' and '2016-02-03';
+drop table parquet_tbl;
diff --git a/ql/src/test/results/clientpositive/parquet_ppd.q.out b/ql/src/test/results/clientpositive/parquet_ppd.q.out
new file mode 100644
index 0000000..5f7628e
--- /dev/null
+++ b/ql/src/test/results/clientpositive/parquet_ppd.q.out
@@ -0,0 +1,61 @@
+PREHOOK: query: CREATE TABLE parquet_tbl(
+  key int,
+  ldate string)
+ PARTITIONED BY (
+ lyear string )
+ ROW FORMAT SERDE
+ 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
+ STORED AS INPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
+ OUTPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@parquet_tbl
+POSTHOOK: query: CREATE TABLE parquet_tbl(
+  key int,
+  ldate string)
+ PARTITIONED BY (
+ lyear string )
+ ROW FORMAT SERDE
+ 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'
+ STORED AS INPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat'
+ OUTPUTFORMAT
+ 'org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@parquet_tbl
+PREHOOK: query: insert overwrite table parquet_tbl partition (lyear='2016') select
+  1,
+  '2016-02-03' from src limit 1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@parquet_tbl@lyear=2016
+POSTHOOK: query: insert overwrite table parquet_tbl partition (lyear='2016') select
+  1,
+  '2016-02-03' from src limit 1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@parquet_tbl@lyear=2016
+POSTHOOK: Lineage: parquet_tbl PARTITION(lyear=2016).key SIMPLE []
+POSTHOOK: Lineage: parquet_tbl PARTITION(lyear=2016).ldate SIMPLE []
+PREHOOK: query: select * from parquet_tbl where ldate between '2016-02-03' and '2016-02-03'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@parquet_tbl
+PREHOOK: Input: default@parquet_tbl@lyear=2016
+#### A masked pattern was here ####
+POSTHOOK: query: select * from parquet_tbl where ldate between '2016-02-03' and '2016-02-03'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@parquet_tbl
+POSTHOOK: Input: default@parquet_tbl@lyear=2016
+#### A masked pattern was here ####
+1	2016-02-03	2016
+PREHOOK: query: drop table parquet_tbl
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@parquet_tbl
+PREHOOK: Output: default@parquet_tbl
+POSTHOOK: query: drop table parquet_tbl
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@parquet_tbl
+POSTHOOK: Output: default@parquet_tbl
-- 
1.7.9.5


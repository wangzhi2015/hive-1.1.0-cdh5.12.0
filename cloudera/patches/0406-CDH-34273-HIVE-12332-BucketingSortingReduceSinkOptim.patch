From e4ed4212f0ca5fd3e544a7d11fda7f95919e2234 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Thu, 5 Nov 2015 18:25:24 -0800
Subject: [PATCH 0406/1164] CDH-34273: HIVE-12332 :
 BucketingSortingReduceSinkOptimizer throws IOB
 exception for duplicate columns

Change-Id: I3270daefbcb8931d4e46c6151d5491bf55e8bdd0
---
 .../BucketingSortingReduceSinkOptimizer.java       |    6 +
 .../clientpositive/insertoverwrite_bucket.q        |   37 ++++
 .../clientpositive/insertoverwrite_bucket.q.out    |  181 ++++++++++++++++++++
 3 files changed, 224 insertions(+)
 create mode 100644 ql/src/test/queries/clientpositive/insertoverwrite_bucket.q
 create mode 100644 ql/src/test/results/clientpositive/insertoverwrite_bucket.q.out

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
index 00c9146..dbc708b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/BucketingSortingReduceSinkOptimizer.java
@@ -585,6 +585,12 @@ else if (op instanceof SelectOperator) {
 
             // Only columns can be selected for both sorted and bucketed positions
             for (int pos : bucketPositions) {
+              if (pos >= selectDesc.getColList().size()) {
+                // e.g., INSERT OVERWRITE TABLE temp1 SELECT  c0,  c0 FROM temp2;
+                // In such a case Select Op will only have one instance of c0 and RS would have two.
+                // So, locating bucketCol in such cases will generate error. So, bail out.
+                return null;
+              }
               ExprNodeDesc selectColList = selectDesc.getColList().get(pos);
               if (!(selectColList instanceof ExprNodeColumnDesc)) {
                 return null;
diff --git a/ql/src/test/queries/clientpositive/insertoverwrite_bucket.q b/ql/src/test/queries/clientpositive/insertoverwrite_bucket.q
new file mode 100644
index 0000000..5a10f94
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/insertoverwrite_bucket.q
@@ -0,0 +1,37 @@
+CREATE TABLE IF NOT EXISTS bucketinput( 
+data string 
+) 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
+CREATE TABLE IF NOT EXISTS bucketoutput1( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
+CREATE TABLE IF NOT EXISTS bucketoutput2( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';
+insert into table bucketinput values ("firstinsert1");
+insert into table bucketinput values ("firstinsert2");
+insert into table bucketinput values ("firstinsert3");
+set hive.enforce.bucketing = true; 
+set hive.enforce.sorting=true;
+insert overwrite table bucketoutput1 select * from bucketinput where data like 'first%'; 
+CREATE TABLE temp1
+(
+    change string,
+    num string
+)
+CLUSTERED BY (num) SORTED BY (num) INTO 4 BUCKETS;
+explain insert overwrite table temp1 select data, data from bucketinput;
+
+set hive.auto.convert.sortmerge.join=true; 
+set hive.optimize.bucketmapjoin = true; 
+set hive.optimize.bucketmapjoin.sortedmerge = true; 
+select * from bucketoutput1 a join bucketoutput2 b on (a.data=b.data);
+drop table temp1;
+drop table buckettestinput;
+drop table buckettestoutput1;
+drop table buckettestoutput2;
+
diff --git a/ql/src/test/results/clientpositive/insertoverwrite_bucket.q.out b/ql/src/test/results/clientpositive/insertoverwrite_bucket.q.out
new file mode 100644
index 0000000..263748f
--- /dev/null
+++ b/ql/src/test/results/clientpositive/insertoverwrite_bucket.q.out
@@ -0,0 +1,181 @@
+PREHOOK: query: CREATE TABLE IF NOT EXISTS bucketinput( 
+data string 
+) 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucketinput
+POSTHOOK: query: CREATE TABLE IF NOT EXISTS bucketinput( 
+data string 
+) 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucketinput
+PREHOOK: query: CREATE TABLE IF NOT EXISTS bucketoutput1( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucketoutput1
+POSTHOOK: query: CREATE TABLE IF NOT EXISTS bucketoutput1( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucketoutput1
+PREHOOK: query: CREATE TABLE IF NOT EXISTS bucketoutput2( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@bucketoutput2
+POSTHOOK: query: CREATE TABLE IF NOT EXISTS bucketoutput2( 
+data string 
+)CLUSTERED BY(data) 
+INTO 2 BUCKETS 
+ROW FORMAT DELIMITED FIELDS TERMINATED BY ','
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@bucketoutput2
+PREHOOK: query: insert into table bucketinput values ("firstinsert1")
+PREHOOK: type: QUERY
+PREHOOK: Input: default@values__tmp__table__1
+PREHOOK: Output: default@bucketinput
+POSTHOOK: query: insert into table bucketinput values ("firstinsert1")
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@values__tmp__table__1
+POSTHOOK: Output: default@bucketinput
+POSTHOOK: Lineage: bucketinput.data SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+PREHOOK: query: insert into table bucketinput values ("firstinsert2")
+PREHOOK: type: QUERY
+PREHOOK: Input: default@values__tmp__table__2
+PREHOOK: Output: default@bucketinput
+POSTHOOK: query: insert into table bucketinput values ("firstinsert2")
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@values__tmp__table__2
+POSTHOOK: Output: default@bucketinput
+POSTHOOK: Lineage: bucketinput.data SIMPLE [(values__tmp__table__2)values__tmp__table__2.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+PREHOOK: query: insert into table bucketinput values ("firstinsert3")
+PREHOOK: type: QUERY
+PREHOOK: Input: default@values__tmp__table__3
+PREHOOK: Output: default@bucketinput
+POSTHOOK: query: insert into table bucketinput values ("firstinsert3")
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@values__tmp__table__3
+POSTHOOK: Output: default@bucketinput
+POSTHOOK: Lineage: bucketinput.data SIMPLE [(values__tmp__table__3)values__tmp__table__3.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+PREHOOK: query: insert overwrite table bucketoutput1 select * from bucketinput where data like 'first%'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucketinput
+PREHOOK: Output: default@bucketoutput1
+POSTHOOK: query: insert overwrite table bucketoutput1 select * from bucketinput where data like 'first%'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucketinput
+POSTHOOK: Output: default@bucketoutput1
+POSTHOOK: Lineage: bucketoutput1.data SIMPLE [(bucketinput)bucketinput.FieldSchema(name:data, type:string, comment:null), ]
+PREHOOK: query: CREATE TABLE temp1
+(
+    change string,
+    num string
+)
+CLUSTERED BY (num) SORTED BY (num) INTO 4 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@temp1
+POSTHOOK: query: CREATE TABLE temp1
+(
+    change string,
+    num string
+)
+CLUSTERED BY (num) SORTED BY (num) INTO 4 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@temp1
+PREHOOK: query: explain insert overwrite table temp1 select data, data from bucketinput
+PREHOOK: type: QUERY
+POSTHOOK: query: explain insert overwrite table temp1 select data, data from bucketinput
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: bucketinput
+            Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: data (type: string), data (type: string)
+              outputColumnNames: _col0, _col1
+              Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+              Reduce Output Operator
+                key expressions: _col1 (type: string)
+                sort order: +
+                Map-reduce partition columns: _col1 (type: string)
+                Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+                value expressions: _col0 (type: string), _col1 (type: string)
+      Reduce Operator Tree:
+        Extract
+          Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            Statistics: Num rows: 3 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                name: default.temp1
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.temp1
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+
+PREHOOK: query: select * from bucketoutput1 a join bucketoutput2 b on (a.data=b.data)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@bucketoutput1
+PREHOOK: Input: default@bucketoutput2
+#### A masked pattern was here ####
+POSTHOOK: query: select * from bucketoutput1 a join bucketoutput2 b on (a.data=b.data)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@bucketoutput1
+POSTHOOK: Input: default@bucketoutput2
+#### A masked pattern was here ####
+PREHOOK: query: drop table temp1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@temp1
+PREHOOK: Output: default@temp1
+POSTHOOK: query: drop table temp1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@temp1
+POSTHOOK: Output: default@temp1
+PREHOOK: query: drop table buckettestinput
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table buckettestinput
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table buckettestoutput1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table buckettestoutput1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: drop table buckettestoutput2
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table buckettestoutput2
+POSTHOOK: type: DROPTABLE
-- 
1.7.9.5


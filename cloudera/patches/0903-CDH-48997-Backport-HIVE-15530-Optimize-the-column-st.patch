From e126dff4c938e12a4f471e9b1dba099627df9429 Mon Sep 17 00:00:00 2001
From: Chaoyu Tang <ctang@cloudera.com>
Date: Wed, 11 Jan 2017 09:33:52 -0500
Subject: [PATCH 0903/1164] CDH-48997: Backport HIVE-15530: Optimize the
 column stats update logic in table alteration
 (Yibing Shi via Chaoyu Tang)

Change-Id: I3d7e91bac5b7138d10e8a8dc003159a5bad03ae6
---
 .../queries/positive/hive_hfile_output_format.q    |   31 ++++++
 .../positive/hive_hfile_output_format.q.out        |  113 ++++++++++++++++++++
 .../hadoop/hive/metastore/HiveAlterHandler.java    |   12 +--
 .../hadoop/hive/metastore/MetaStoreUtils.java      |   15 +++
 .../apache/hadoop/hive/metastore/ObjectStore.java  |    1 -
 .../hive/metastore/TestHiveAlterHandler.java       |  108 +++++++++++++++++++
 .../hadoop/hive/metastore/TestMetaStoreUtils.java  |   41 +++++++
 7 files changed, 314 insertions(+), 7 deletions(-)
 create mode 100644 hbase-handler/src/test/queries/positive/hive_hfile_output_format.q
 create mode 100644 hbase-handler/src/test/results/positive/hive_hfile_output_format.q.out
 create mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveAlterHandler.java
 create mode 100644 metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreUtils.java

diff --git a/hbase-handler/src/test/queries/positive/hive_hfile_output_format.q b/hbase-handler/src/test/queries/positive/hive_hfile_output_format.q
new file mode 100644
index 0000000..87afa04
--- /dev/null
+++ b/hbase-handler/src/test/queries/positive/hive_hfile_output_format.q
@@ -0,0 +1,31 @@
+set hive.explain.user=false;
+set hive.fetch.task.conversion=none;
+
+-- dfs -rmr /tmp/coords_hfiles/o;
+
+CREATE TABLE source(id INT, longitude DOUBLE, latitude DOUBLE);
+
+INSERT INTO TABLE source VALUES (1, 23.54, -54.99);
+
+CREATE TABLE coords_hbase(id INT, x DOUBLE, y DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+  'hbase.columns.mapping' = ':key,o:x,o:y',
+  'hbase.table.default.storage.type' = 'binary');
+
+SET hfile.family.path=/tmp/coords_hfiles/o; 
+SET hive.hbase.generatehfiles=true;
+
+INSERT OVERWRITE TABLE coords_hbase 
+SELECT id, longitude, latitude
+FROM source
+CLUSTER BY id;
+
+EXPLAIN
+SELECT * FROM coords_hbase;
+
+SELECT * FROM coords_hbase;
+
+drop table source;
+drop table coords_hbase;
+dfs -rmr /tmp/coords_hfiles/o;
diff --git a/hbase-handler/src/test/results/positive/hive_hfile_output_format.q.out b/hbase-handler/src/test/results/positive/hive_hfile_output_format.q.out
new file mode 100644
index 0000000..3b7c091
--- /dev/null
+++ b/hbase-handler/src/test/results/positive/hive_hfile_output_format.q.out
@@ -0,0 +1,113 @@
+#### A masked pattern was here ####
+
+CREATE TABLE source(id INT, longitude DOUBLE, latitude DOUBLE)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@source
+#### A masked pattern was here ####
+
+CREATE TABLE source(id INT, longitude DOUBLE, latitude DOUBLE)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@source
+PREHOOK: query: INSERT INTO TABLE source VALUES (1, 23.54, -54.99)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@values__tmp__table__1
+PREHOOK: Output: default@source
+POSTHOOK: query: INSERT INTO TABLE source VALUES (1, 23.54, -54.99)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@values__tmp__table__1
+POSTHOOK: Output: default@source
+POSTHOOK: Lineage: source.id EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+POSTHOOK: Lineage: source.latitude EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]
+POSTHOOK: Lineage: source.longitude EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]
+PREHOOK: query: CREATE TABLE coords_hbase(id INT, x DOUBLE, y DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+  'hbase.columns.mapping' = ':key,o:x,o:y',
+  'hbase.table.default.storage.type' = 'binary')
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@coords_hbase
+POSTHOOK: query: CREATE TABLE coords_hbase(id INT, x DOUBLE, y DOUBLE)
+STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
+WITH SERDEPROPERTIES (
+  'hbase.columns.mapping' = ':key,o:x,o:y',
+  'hbase.table.default.storage.type' = 'binary')
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@coords_hbase
+PREHOOK: query: INSERT OVERWRITE TABLE coords_hbase 
+SELECT id, longitude, latitude
+FROM source
+CLUSTER BY id
+PREHOOK: type: QUERY
+PREHOOK: Input: default@source
+PREHOOK: Output: default@coords_hbase
+POSTHOOK: query: INSERT OVERWRITE TABLE coords_hbase 
+SELECT id, longitude, latitude
+FROM source
+CLUSTER BY id
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@source
+POSTHOOK: Output: default@coords_hbase
+PREHOOK: query: EXPLAIN
+SELECT * FROM coords_hbase
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT * FROM coords_hbase
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: coords_hbase
+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
+            Select Operator
+              expressions: id (type: int), x (type: double), y (type: double)
+              outputColumnNames: _col0, _col1, _col2
+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
+              File Output Operator
+                compressed: false
+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: NONE
+                table:
+                    input format: org.apache.hadoop.mapred.TextInputFormat
+                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                    serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT * FROM coords_hbase
+PREHOOK: type: QUERY
+PREHOOK: Input: default@coords_hbase
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM coords_hbase
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@coords_hbase
+#### A masked pattern was here ####
+PREHOOK: query: drop table source
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@source
+PREHOOK: Output: default@source
+POSTHOOK: query: drop table source
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@source
+POSTHOOK: Output: default@source
+PREHOOK: query: drop table coords_hbase
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@coords_hbase
+PREHOOK: Output: default@coords_hbase
+POSTHOOK: query: drop table coords_hbase
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@coords_hbase
+POSTHOOK: Output: default@coords_hbase
+#### A masked pattern was here ####
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
index 56c9ed6..245ab56 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/HiveAlterHandler.java
@@ -22,7 +22,8 @@
 import java.util.ArrayList;
 import java.util.Iterator;
 import java.util.List;
-
+import com.google.common.annotations.VisibleForTesting;
+import com.google.common.collect.Lists;
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -52,8 +53,6 @@
 import org.apache.hadoop.ipc.RemoteException;
 import org.apache.hive.common.util.HiveStringUtils;
 
-import com.google.common.collect.Lists;
-
 /**
  * Hive specific implementation of alter
  */
@@ -742,7 +741,7 @@ private void updatePartColumnStats(RawStore msdb, String dbName, String tableNam
         }
         if (oldPartition.getSd() != null && newPart.getSd() != null) {
         List<FieldSchema> oldCols = oldPartition.getSd().getCols();
-          if (!MetaStoreUtils.areSameColumns(oldCols, newPart.getSd().getCols())) {
+          if (!MetaStoreUtils.columnsIncluded(oldCols, newPart.getSd().getCols())) {
             updatePartColumnStatsForAlterColumns(msdb, oldPartition, oldPartName, partVals, oldCols, newPart);
           }
         }
@@ -755,7 +754,8 @@ private void updatePartColumnStats(RawStore msdb, String dbName, String tableNam
     }
   }
 
-  private void alterTableUpdateTableColumnStats(RawStore msdb,
+  @VisibleForTesting
+  void alterTableUpdateTableColumnStats(RawStore msdb,
       Table oldTable, Table newTable)
       throws MetaException, InvalidObjectException {
     String dbName = oldTable.getDbName().toLowerCase();
@@ -773,7 +773,7 @@ private void alterTableUpdateTableColumnStats(RawStore msdb,
       // Nothing to update if everything is the same
         if (newDbName.equals(dbName) &&
             newTableName.equals(tableName) &&
-            MetaStoreUtils.areSameColumns(oldCols, newCols)) {
+            MetaStoreUtils.columnsIncluded(oldCols, newCols)) {
           updateColumnStats = false;
         }
 
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
index 12b0ca9..f58e6b1 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/MetaStoreUtils.java
@@ -596,6 +596,21 @@ static boolean areSameColumns(List<FieldSchema> oldCols, List<FieldSchema> newCo
     return true;
   }
 
+  static boolean columnsIncluded(List<FieldSchema> oldCols, List<FieldSchema> newCols) {
+    if (oldCols.size() > newCols.size()) {
+      return false;
+    }
+
+    Set<FieldSchema> newColsSet = new HashSet<FieldSchema>(newCols);
+    for (final FieldSchema oldCol : oldCols) {
+      if (!newColsSet.contains(oldCol)) {
+        return false;
+      }
+    }
+
+    return true;
+  }
+
   /**
    * @return true if oldType and newType are compatible.
    * Two types are compatible if we have internal functions to cast one to another.
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
index ce917a8..5fc621d 100644
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/ObjectStore.java
@@ -6564,7 +6564,6 @@ public boolean updatePartitionColumnStatistics(ColumnStatistics colStats, List<S
     } finally {
       if (!committed) {
         rollbackTransaction();
-        return Lists.newArrayList();
       }
     }
   }
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveAlterHandler.java b/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveAlterHandler.java
new file mode 100644
index 0000000..03ea7fc
--- /dev/null
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/TestHiveAlterHandler.java
@@ -0,0 +1,108 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.metastore.api.*;
+import org.junit.Test;
+import org.mockito.Mockito;
+
+import java.util.Arrays;
+
+public class TestHiveAlterHandler {
+
+  @Test
+  public void testAlterTableAddColNotUpdateStats() throws MetaException, InvalidObjectException, NoSuchObjectException {
+    FieldSchema col1 = new FieldSchema("col1", "string", "col1 comment");
+    FieldSchema col2 = new FieldSchema("col2", "string", "col2 comment");
+    FieldSchema col3 = new FieldSchema("col3", "string", "col3 comment");
+    FieldSchema col4 = new FieldSchema("col4", "string", "col4 comment");
+
+    StorageDescriptor oldSd = new StorageDescriptor();
+    oldSd.setCols(Arrays.asList(col1, col2, col3));
+    Table oldTable = new Table();
+    oldTable.setDbName("default");
+    oldTable.setTableName("test_table");
+    oldTable.setSd(oldSd);
+
+    StorageDescriptor newSd = new StorageDescriptor(oldSd);
+    newSd.setCols(Arrays.asList(col1, col2, col3, col4));
+    Table newTable = new Table(oldTable);
+    newTable.setSd(newSd);
+
+    RawStore msdb = Mockito.mock(RawStore.class);
+    Mockito.doThrow(new RuntimeException("shouldn't be called")).when(msdb).getTableColumnStatistics(
+        oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3"));
+    HiveAlterHandler handler = new HiveAlterHandler();
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable);
+  }
+
+  @Test
+  public void testAlterTableDelColUpdateStats() throws MetaException, InvalidObjectException, NoSuchObjectException {
+    FieldSchema col1 = new FieldSchema("col1", "string", "col1 comment");
+    FieldSchema col2 = new FieldSchema("col2", "string", "col2 comment");
+    FieldSchema col3 = new FieldSchema("col3", "string", "col3 comment");
+    FieldSchema col4 = new FieldSchema("col4", "string", "col4 comment");
+
+    StorageDescriptor oldSd = new StorageDescriptor();
+    oldSd.setCols(Arrays.asList(col1, col2, col3, col4));
+    Table oldTable = new Table();
+    oldTable.setDbName("default");
+    oldTable.setTableName("test_table");
+    oldTable.setSd(oldSd);
+
+    StorageDescriptor newSd = new StorageDescriptor(oldSd);
+    newSd.setCols(Arrays.asList(col1, col2, col3));
+    Table newTable = new Table(oldTable);
+    newTable.setSd(newSd);
+
+    RawStore msdb = Mockito.mock(RawStore.class);
+    HiveAlterHandler handler = new HiveAlterHandler();
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable);
+    Mockito.verify(msdb, Mockito.times(1)).getTableColumnStatistics(
+        oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3", "col4")
+    );
+  }
+
+  @Test
+  public void testAlterTableChangePosNotUpdateStats() throws MetaException, InvalidObjectException, NoSuchObjectException {
+    FieldSchema col1 = new FieldSchema("col1", "string", "col1 comment");
+    FieldSchema col2 = new FieldSchema("col2", "string", "col2 comment");
+    FieldSchema col3 = new FieldSchema("col3", "string", "col3 comment");
+    FieldSchema col4 = new FieldSchema("col4", "string", "col4 comment");
+
+    StorageDescriptor oldSd = new StorageDescriptor();
+    oldSd.setCols(Arrays.asList(col1, col2, col3, col4));
+    Table oldTable = new Table();
+    oldTable.setDbName("default");
+    oldTable.setTableName("test_table");
+    oldTable.setSd(oldSd);
+
+    StorageDescriptor newSd = new StorageDescriptor(oldSd);
+    newSd.setCols(Arrays.asList(col1, col4, col2, col3));
+    Table newTable = new Table(oldTable);
+    newTable.setSd(newSd);
+
+    RawStore msdb = Mockito.mock(RawStore.class);
+    Mockito.doThrow(new RuntimeException("shouldn't be called")).when(msdb).getTableColumnStatistics(
+        oldTable.getDbName(), oldTable.getTableName(), Arrays.asList("col1", "col2", "col3", "col4"));
+    HiveAlterHandler handler = new HiveAlterHandler();
+    handler.alterTableUpdateTableColumnStats(msdb, oldTable, newTable);
+  }
+
+}
diff --git a/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreUtils.java b/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreUtils.java
new file mode 100644
index 0000000..21f9054
--- /dev/null
+++ b/metastore/src/test/org/apache/hadoop/hive/metastore/TestMetaStoreUtils.java
@@ -0,0 +1,41 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.metastore;
+
+import org.apache.hadoop.hive.metastore.api.FieldSchema;
+import org.junit.Assert;
+import org.junit.Test;
+
+import java.util.Arrays;
+
+public class TestMetaStoreUtils {
+
+  @Test
+  public void testColumnsIncluded() {
+    FieldSchema col1 = new FieldSchema("col1", "string", "col1 comment");
+    FieldSchema col2 = new FieldSchema("col2", "string", "col2 comment");
+    FieldSchema col3 = new FieldSchema("col3", "string", "col3 comment");
+    Assert.assertTrue(MetaStoreUtils.columnsIncluded(Arrays.asList(col1), Arrays.asList(col1)));
+    Assert.assertTrue(MetaStoreUtils.columnsIncluded(Arrays.asList(col1, col2), Arrays.asList(col1, col2)));
+    Assert.assertTrue(MetaStoreUtils.columnsIncluded(Arrays.asList(col1, col2), Arrays.asList(col2, col1)));
+    Assert.assertTrue(MetaStoreUtils.columnsIncluded(Arrays.asList(col1, col2), Arrays.asList(col1, col2, col3)));
+    Assert.assertTrue(MetaStoreUtils.columnsIncluded(Arrays.asList(col1, col2), Arrays.asList(col3, col2, col1)));
+    Assert.assertFalse(MetaStoreUtils.columnsIncluded(Arrays.asList(col1, col2), Arrays.asList(col1)));
+  }
+}
-- 
1.7.9.5


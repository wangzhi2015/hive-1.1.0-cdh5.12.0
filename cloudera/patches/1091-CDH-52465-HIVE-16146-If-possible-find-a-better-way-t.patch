From e749ea2daed62f5f9354dbc3d7016f27fe255cd8 Mon Sep 17 00:00:00 2001
From: Peter Vary <pvary@cloudera.com>
Date: Tue, 18 Apr 2017 07:32:11 +0200
Subject: [PATCH 1091/1164] CDH-52465: HIVE-16146: If possible find a better
 way to filter the TestBeeLineDriver output(Peter
 Vary via Zoltan Haindrich, reviewed by Vihang
 Karajgaonkar)

Signed-off-by: Zoltan Haindrich <kirk@rxd.hu>
(cherry picked from commit 2509e2fa735b0b88a992acfef703c36936b09b23)

Change-Id: Ifcd821cd10d77edb99d4382b5314067a3bec2296
---
 .../src/java/org/apache/hive/beeline/BeeLine.java  |   19 +
 .../src/java/org/apache/hive/beeline/Commands.java |   16 +-
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    7 +
 .../test/resources/testconfiguration.properties    |    4 +-
 .../main/java/org/apache/hive/beeline/QFile.java   |  333 ++
 .../apache/hive/beeline/QFileBeeLineClient.java    |  157 +
 .../java/org/apache/hive/beeline/package-info.java |   22 +
 .../java/org/apache/hive/beeline/qfile/QFile.java  |  336 --
 .../hive/beeline/qfile/QFileBeeLineClient.java     |  145 -
 .../apache/hive/beeline/qfile/package-info.java    |   22 -
 .../hadoop/hive/ql/hooks/PostExecutePrinter.java   |   15 +-
 .../hadoop/hive/ql/hooks/PreExecutePrinter.java    |   14 +-
 .../hadoop/hive/ql/session/OperationLog.java       |   75 +-
 .../beeline/drop_with_concurrency.q.out            |   89 +-
 .../clientpositive/beeline/escape_comments.q.out   |  637 +--
 .../beeline/select_dummy_source.q.out              |  251 +
 .../clientpositive/beeline/smb_mapjoin_1.q.out     | 1423 ++----
 .../clientpositive/beeline/smb_mapjoin_10.q.out    |  334 +-
 .../clientpositive/beeline/smb_mapjoin_11.q.out    | 4873 +++++++++-----------
 .../clientpositive/beeline/smb_mapjoin_12.q.out    | 1466 +++---
 .../clientpositive/beeline/smb_mapjoin_13.q.out    | 1203 ++---
 .../clientpositive/beeline/smb_mapjoin_16.q.out    |  341 +-
 .../clientpositive/beeline/smb_mapjoin_2.q.out     | 1440 ++----
 .../clientpositive/beeline/smb_mapjoin_3.q.out     | 1429 ++----
 .../clientpositive/beeline/smb_mapjoin_7.q.out     | 3072 +++++-------
 .../cli/operation/LogDivertAppenderForTest.java    |  115 +
 .../service/cli/operation/OperationManager.java    |    7 +
 .../hive/service/cli/session/HiveSessionImpl.java  |    3 +-
 28 files changed, 7346 insertions(+), 10502 deletions(-)
 create mode 100644 itests/util/src/main/java/org/apache/hive/beeline/QFile.java
 create mode 100644 itests/util/src/main/java/org/apache/hive/beeline/QFileBeeLineClient.java
 create mode 100644 itests/util/src/main/java/org/apache/hive/beeline/package-info.java
 delete mode 100644 itests/util/src/main/java/org/apache/hive/beeline/qfile/QFile.java
 delete mode 100644 itests/util/src/main/java/org/apache/hive/beeline/qfile/QFileBeeLineClient.java
 delete mode 100644 itests/util/src/main/java/org/apache/hive/beeline/qfile/package-info.java
 create mode 100644 ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out
 create mode 100644 service/src/java/org/apache/hive/service/cli/operation/LogDivertAppenderForTest.java

diff --git a/beeline/src/java/org/apache/hive/beeline/BeeLine.java b/beeline/src/java/org/apache/hive/beeline/BeeLine.java
index 8a21448..e8fb463 100644
--- a/beeline/src/java/org/apache/hive/beeline/BeeLine.java
+++ b/beeline/src/java/org/apache/hive/beeline/BeeLine.java
@@ -144,6 +144,10 @@
   private History history;
   private boolean isBeeLine = true;
 
+  // Indicates that we are in test mode.
+  // Print only the errors, the operation log and the query results.
+  private boolean isTestMode = false;
+
   private static final Options options = new Options();
 
   public static final String BEELINE_DEFAULT_JDBC_DRIVER = "org.apache.hive.jdbc.HiveDriver";
@@ -2287,4 +2291,19 @@ public String getCurrentDatabase() {
   public void setCurrentDatabase(String currentDatabase) {
     this.currentDatabase = currentDatabase;
   }
+
+  /**
+   * Setting the BeeLine into test mode.
+   * Print only the errors, the operation log and the query results.
+   * Should be used only by tests.
+   *
+   * @param isTestMode
+   */
+  void setIsTestMode(boolean isTestMode) {
+    this.isTestMode = isTestMode;
+  }
+
+  boolean isTestMode() {
+    return isTestMode;
+  }
 }
diff --git a/beeline/src/java/org/apache/hive/beeline/Commands.java b/beeline/src/java/org/apache/hive/beeline/Commands.java
index de23f75..0852297 100644
--- a/beeline/src/java/org/apache/hive/beeline/Commands.java
+++ b/beeline/src/java/org/apache/hive/beeline/Commands.java
@@ -981,7 +981,8 @@ private boolean executeInternal(String sql, boolean call) {
           hasResults = ((CallableStatement) stmnt).execute();
         } else {
           stmnt = beeLine.createStatement();
-          if (beeLine.getOpts().isSilent()) {
+          // In test mode we want the operation logs regardless of the settings
+          if (!beeLine.isTestMode() && beeLine.getOpts().isSilent()) {
             hasResults = stmnt.execute(sql);
           } else {
             logThread = new Thread(createLogRunnable(stmnt));
@@ -1208,6 +1209,12 @@ public void run() {
             try {
               // fetch the log periodically and output to beeline console
               for (String log : hiveStatement.getQueryLog()) {
+                if (!beeLine.isTestMode()) {
+                  beeLine.info(log);
+                } else {
+                  // In test mode print the logs to the output
+                  beeLine.output(log);
+                }
                 beeLine.info(log);
               }
               Thread.sleep(DEFAULT_QUERY_PROGRESS_INTERVAL);
@@ -1246,7 +1253,12 @@ private void showRemainingLogsIfAny(Statement statement) {
           return;
         }
         for (String log : logs) {
-          beeLine.info(log);
+          if (!beeLine.isTestMode()) {
+            beeLine.info(log);
+          } else {
+            // In test mode print the logs to the output
+            beeLine.output(log);
+          }
         }
       } while (logs.size() > 0);
     } else {
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 225d3eb..d125e4c 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -328,6 +328,13 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {
         "If not set, defaults to the codec extension for text files (e.g. \".gz\"), or no extension otherwise."),
 
     HIVE_IN_TEST("hive.in.test", false, "internal usage only, true in test mode", true),
+    HIVE_IN_TEST_SHORT_LOGS("hive.in.test.short.logs", false,
+        "internal usage only, used only in test mode. If set true, when requesting the " +
+        "operation logs the short version (generated by LogDivertAppenderForTest) will be " +
+        "returned"),
+    HIVE_IN_TEST_REMOVE_LOGS("hive.in.test.remove.logs", true,
+        "internal usage only, used only in test mode. If set false, the operation logs, and the " +
+        "operation log directory will not be removed, so they can be found after the test runs."),
 
     HIVE_IN_TEZ_TEST("hive.in.tez.test", false, "internal use only, true when in testing tez",
         true),
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index 579ea8a..152d427 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -341,8 +341,8 @@ beeline.positive.include=drop_with_concurrency.q,\
   smb_mapjoin_16.q,\
   smb_mapjoin_2.q,\
   smb_mapjoin_3.q,\
-  smb_mapjoin_7.q
-
+  smb_mapjoin_7.q,\
+  select_dummy_source.q
 
 minimr.query.negative.files=cluster_tasklog_retrieval.q,\
   file_with_header_footer_negative.q,\
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/QFile.java b/itests/util/src/main/java/org/apache/hive/beeline/QFile.java
new file mode 100644
index 0000000..a96f7f2
--- /dev/null
+++ b/itests/util/src/main/java/org/apache/hive/beeline/QFile.java
@@ -0,0 +1,333 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.beeline;
+
+import org.apache.commons.io.FileUtils;
+import org.apache.hadoop.hive.ql.QTestProcessExecResult;
+import org.apache.hadoop.hive.ql.QTestUtil;
+import org.apache.hadoop.util.Shell;
+import org.apache.hive.common.util.StreamPrinter;
+
+import java.io.ByteArrayOutputStream;
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.nio.charset.StandardCharsets;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.Set;
+import java.util.regex.Pattern;
+
+/**
+ * Class for representing a Query and the connected files. It provides accessors for the specific
+ * input and output files, and provides methods for filtering the output of the runs.
+ */
+public final class QFile {
+  private static final Set<String> srcTables = QTestUtil.getSrcTables();
+  private static final String DEBUG_HINT =
+      "The following files can help you identifying the problem:%n"
+      + " - Query file: %1s%n"
+      + " - Raw output file: %2s%n"
+      + " - Filtered output file: %3s%n"
+      + " - Expected output file: %4s%n"
+      + " - Client log file: %5s%n"
+      + " - Client log files before the test: %6s%n"
+      + " - Client log files after the test: %7s%n"
+      + " - Hiveserver2 log file: %8s%n";
+  private static final String USE_COMMAND_WARNING =
+      "The query file %1s contains \"%2s\" command.%n"
+      + "The source table name rewrite is turned on, so this might cause problems when the used "
+      + "database contains tables named any of the following: " + srcTables + "%n"
+      + "To turn off the table name rewrite use -Dtest.rewrite.source.tables=false%n";
+
+  private static final Pattern USE_PATTERN =
+      Pattern.compile("^\\s*use\\s.*", Pattern.CASE_INSENSITIVE);
+
+  private static final String MASK_PATTERN = "#### A masked pattern was here ####\n";
+
+  private String name;
+  private File inputFile;
+  private File rawOutputFile;
+  private File outputFile;
+  private File expectedOutputFile;
+  private File logFile;
+  private File beforeExecuteLogFile;
+  private File afterExecuteLogFile;
+  private static RegexFilterSet staticFilterSet = getStaticFilterSet();
+  private RegexFilterSet specificFilterSet;
+  private boolean rewriteSourceTables;
+
+  private QFile() {}
+
+  public String getName() {
+    return name;
+  }
+
+  public File getInputFile() {
+    return inputFile;
+  }
+
+  public File getRawOutputFile() {
+    return rawOutputFile;
+  }
+
+  public File getOutputFile() {
+    return outputFile;
+  }
+
+  public File getExpectedOutputFile() {
+    return expectedOutputFile;
+  }
+
+  public File getLogFile() {
+    return logFile;
+  }
+
+  public File getBeforeExecuteLogFile() {
+    return beforeExecuteLogFile;
+  }
+
+  public File getAfterExecuteLogFile() {
+    return afterExecuteLogFile;
+  }
+
+  public String getDebugHint() {
+    return String.format(DEBUG_HINT, inputFile, rawOutputFile, outputFile, expectedOutputFile,
+        logFile, beforeExecuteLogFile, afterExecuteLogFile,
+        "./itests/qtest/target/tmp/log/hive.log");
+  }
+
+  /**
+   * Filters the sql commands if necessary.
+   * @param commands The array of the sql commands before filtering
+   * @return The filtered array of the sql command strings
+   * @throws IOException File read error
+   */
+  public String[] filterCommands(String[] commands) throws IOException {
+    if (rewriteSourceTables) {
+      for (int i=0; i<commands.length; i++) {
+        if (USE_PATTERN.matcher(commands[i]).matches()) {
+          System.err.println(String.format(USE_COMMAND_WARNING, inputFile, commands[i]));
+        }
+        commands[i] = replaceTableNames(commands[i]);
+      }
+    }
+    return commands;
+  }
+
+  /**
+   * Replace the default src database TABLE_NAMEs in the queries with default.TABLE_NAME, like
+   * src->default.src, srcpart->default.srcpart, so the queries could be run even if the used
+   * database is query specific. This change is only a best effort, since we do not want to parse
+   * the queries, we could not be sure that we do not replace other strings which are not
+   * tablenames. Like 'select src from othertable;'. The q files containing these commands should
+   * be excluded. Only replace the tablenames, if rewriteSourceTables is set.
+   * @param source The original query string
+   * @return The query string where the tablenames are replaced
+   */
+  private String replaceTableNames(String source) {
+    for (String table : srcTables) {
+      source = source.replaceAll("(?is)(\\s+)" + table + "([\\s;\\n\\)])", "$1default." + table
+          + "$2");
+    }
+    return source;
+  }
+
+  /**
+   * The result contains the original queries. To revert them to the original form remove the
+   * 'default' from every default.TABLE_NAME, like default.src->src, default.srcpart->srcpart.
+   * @param source The original query output
+   * @return The query output where the tablenames are replaced
+   */
+  private String revertReplaceTableNames(String source) {
+    for (String table : srcTables) {
+      source = source.replaceAll("(?is)(\\s+)default\\." + table + "([\\s;\\n\\)])", "$1" + table
+          + "$2");
+    }
+    return source;
+  }
+
+  public void filterOutput() throws IOException {
+    String rawOutput = FileUtils.readFileToString(rawOutputFile, "UTF-8");
+    if (rewriteSourceTables) {
+      rawOutput = revertReplaceTableNames(rawOutput);
+    }
+    String filteredOutput = staticFilterSet.filter(specificFilterSet.filter(rawOutput));
+    FileUtils.writeStringToFile(outputFile, filteredOutput);
+  }
+
+  public QTestProcessExecResult compareResults() throws IOException, InterruptedException {
+    if (!expectedOutputFile.exists()) {
+      throw new IOException("Expected results file does not exist: " + expectedOutputFile);
+    }
+    return executeDiff();
+  }
+
+  public void overwriteResults() throws IOException {
+    FileUtils.copyFile(outputFile, expectedOutputFile);
+  }
+
+  private QTestProcessExecResult executeDiff() throws IOException, InterruptedException {
+    List<String> diffCommandArgs = new ArrayList<String>();
+    diffCommandArgs.add("diff");
+
+    // Text file comparison
+    diffCommandArgs.add("-a");
+
+    if (Shell.WINDOWS) {
+      // Ignore changes in the amount of white space
+      diffCommandArgs.add("-b");
+
+      // Files created on Windows machines have different line endings
+      // than files created on Unix/Linux. Windows uses carriage return and line feed
+      // ("\r\n") as a line ending, whereas Unix uses just line feed ("\n").
+      // Also StringBuilder.toString(), Stream to String conversions adds extra
+      // spaces at the end of the line.
+      diffCommandArgs.add("--strip-trailing-cr"); // Strip trailing carriage return on input
+      diffCommandArgs.add("-B"); // Ignore changes whose lines are all blank
+    }
+
+    // Add files to compare to the arguments list
+    diffCommandArgs.add(getQuotedString(expectedOutputFile));
+    diffCommandArgs.add(getQuotedString(outputFile));
+
+    System.out.println("Running: " + org.apache.commons.lang.StringUtils.join(diffCommandArgs,
+        ' '));
+    Process executor = Runtime.getRuntime().exec(diffCommandArgs.toArray(
+        new String[diffCommandArgs.size()]));
+
+    ByteArrayOutputStream bos = new ByteArrayOutputStream();
+    PrintStream out = new PrintStream(bos, true, "UTF-8");
+
+    StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, System.err);
+    StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out, out);
+
+    outPrinter.start();
+    errPrinter.start();
+
+    int result = executor.waitFor();
+
+    outPrinter.join();
+    errPrinter.join();
+
+    executor.waitFor();
+
+    return QTestProcessExecResult.create(result, new String(bos.toByteArray(),
+        StandardCharsets.UTF_8));
+  }
+
+  private static String getQuotedString(File file) {
+    return Shell.WINDOWS ? String.format("\"%s\"", file.getAbsolutePath()) : file.getAbsolutePath();
+  }
+
+  private static class Filter {
+    private final Pattern pattern;
+    private final String replacement;
+
+    public Filter(Pattern pattern, String replacement) {
+      this.pattern = pattern;
+      this.replacement = replacement;
+    }
+  }
+
+  private static class RegexFilterSet {
+    private final List<Filter> regexFilters = new ArrayList<Filter>();
+
+    public RegexFilterSet addFilter(String regex, String replacement) {
+      regexFilters.add(new Filter(Pattern.compile(regex), replacement));
+      return this;
+    }
+
+    public String filter(String input) {
+      for (Filter filter : regexFilters) {
+        input = filter.pattern.matcher(input).replaceAll(filter.replacement);
+      }
+      return input;
+    }
+  }
+
+  // These are the filters which are common for every QTest.
+  // Check specificFilterSet for QTest specific ones.
+  private static RegexFilterSet getStaticFilterSet() {
+    // Pattern to remove the timestamp and other infrastructural info from the out file
+    return new RegexFilterSet()
+        .addFilter("Reading log file: .*\n", "")
+        .addFilter("ERROR : ", "")
+        .addFilter(".*/tmp/.*\n", MASK_PATTERN)
+        .addFilter(".*file:.*\n", MASK_PATTERN)
+        .addFilter(".*file\\..*\n", MASK_PATTERN)
+        .addFilter(".*CreateTime.*\n", MASK_PATTERN)
+        .addFilter(".*transient_lastDdlTime.*\n", MASK_PATTERN)
+        .addFilter("(?s)(" + MASK_PATTERN + ")+", MASK_PATTERN);
+  }
+
+  /**
+   * Builder to generate QFile objects. After initializing the builder it is possible the
+   * generate the next QFile object using it's name only.
+   */
+  public static class QFileBuilder {
+    private File queryDirectory;
+    private File logDirectory;
+    private File resultsDirectory;
+    private boolean rewriteSourceTables;
+
+    public QFileBuilder() {
+    }
+
+    public QFileBuilder setQueryDirectory(File queryDirectory) {
+      this.queryDirectory = queryDirectory;
+      return this;
+    }
+
+    public QFileBuilder setLogDirectory(File logDirectory) {
+      this.logDirectory = logDirectory;
+      return this;
+    }
+
+    public QFileBuilder setResultsDirectory(File resultsDirectory) {
+      this.resultsDirectory = resultsDirectory;
+      return this;
+    }
+
+    public QFileBuilder setRewriteSourceTables(boolean rewriteSourceTables) {
+      this.rewriteSourceTables = rewriteSourceTables;
+      return this;
+    }
+
+    public QFile getQFile(String name) throws IOException {
+      QFile result = new QFile();
+      result.name = name;
+      result.inputFile = new File(queryDirectory, name + ".q");
+      result.rawOutputFile = new File(logDirectory, name + ".q.out.raw");
+      result.outputFile = new File(logDirectory, name + ".q.out");
+      result.expectedOutputFile = new File(resultsDirectory, name + ".q.out");
+      result.logFile = new File(logDirectory, name + ".q.beeline");
+      result.beforeExecuteLogFile = new File(logDirectory, name + ".q.beforeExecute.log");
+      result.afterExecuteLogFile = new File(logDirectory, name + ".q.afterExecute.log");
+      result.rewriteSourceTables = rewriteSourceTables;
+      result.specificFilterSet = new RegexFilterSet()
+          .addFilter("(PREHOOK|POSTHOOK): (Output|Input): database:" + name + "\n",
+              "$1: $2: database:default\n")
+          .addFilter("(PREHOOK|POSTHOOK): (Output|Input): " + name + "@", "$1: $2: default@")
+          .addFilter("name(:?) " + name + "\\.(.*)\n", "name$1 default.$2\n")
+          .addFilter("/" + name + ".db/", "/");
+      return result;
+    }
+  }
+}
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/QFileBeeLineClient.java b/itests/util/src/main/java/org/apache/hive/beeline/QFileBeeLineClient.java
new file mode 100644
index 0000000..dd15b9f
--- /dev/null
+++ b/itests/util/src/main/java/org/apache/hive/beeline/QFileBeeLineClient.java
@@ -0,0 +1,157 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hive.beeline;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.PrintStream;
+import java.sql.SQLException;
+
+/**
+ * QFile test client using BeeLine. It can be used to submit a list of command strings, or a QFile.
+ */
+public class QFileBeeLineClient implements AutoCloseable {
+  private BeeLine beeLine;
+  private PrintStream beelineOutputStream;
+  private File logFile;
+
+  protected QFileBeeLineClient(String jdbcUrl, String jdbcDriver, String username, String password,
+      File log) throws IOException {
+    logFile = log;
+    beeLine = new BeeLine();
+    beelineOutputStream = new PrintStream(logFile, "UTF-8");
+    beeLine.setOutputStream(beelineOutputStream);
+    beeLine.setErrorStream(beelineOutputStream);
+    beeLine.runCommands(
+        new String[] {
+          "!set verbose true",
+          "!set shownestederrs true",
+          "!set showwarnings true",
+          "!set showelapsedtime false",
+          "!set trimscripts false",
+          "!set maxwidth -1",
+          "!connect " + jdbcUrl + " " + username + " " + password + " " + jdbcDriver,
+          "set hive.stats.dbclass=fs"
+        });
+  }
+
+  public void execute(String[] commands, File resultFile) throws SQLException {
+    beeLine.runCommands(
+        new String[] {
+          "!record " + resultFile.getAbsolutePath()
+        });
+
+    int lastSuccessfulCommand = beeLine.runCommands(commands);
+    if (commands.length != lastSuccessfulCommand) {
+      throw new SQLException("Error executing SQL command: " + commands[lastSuccessfulCommand]);
+    }
+
+    beeLine.runCommands(new String[] {"!record"});
+  }
+
+  private void beforeExecute(QFile qFile) throws SQLException {
+    execute(
+        new String[] {
+          "!set outputformat tsv2",
+          "!set verbose false",
+          "!set silent true",
+          "!set showheader false",
+          "USE default;",
+          "SHOW TABLES;",
+          "DROP DATABASE IF EXISTS `" + qFile.getName() + "` CASCADE;",
+          "CREATE DATABASE `" + qFile.getName() + "`;",
+          "USE `" + qFile.getName() + "`;",
+          "set hive.in.test.short.logs=true;",
+          "set hive.in.test.remove.logs=false;",
+        },
+        qFile.getBeforeExecuteLogFile());
+    beeLine.setIsTestMode(true);
+  }
+
+  private void afterExecute(QFile qFile) throws SQLException {
+    beeLine.setIsTestMode(false);
+    execute(
+        new String[] {
+          "set hive.in.test.short.logs=false;",
+          "!set verbose true",
+          "!set silent false",
+          "!set showheader true",
+          "!set outputformat table",
+          "USE default;",
+          "DROP DATABASE IF EXISTS `" + qFile.getName() + "` CASCADE;",
+        },
+        qFile.getAfterExecuteLogFile());
+  }
+
+  public void execute(QFile qFile) throws SQLException, IOException {
+    beforeExecute(qFile);
+    String[] commands = beeLine.getCommands(qFile.getInputFile());
+    execute(qFile.filterCommands(commands), qFile.getRawOutputFile());
+    afterExecute(qFile);
+  }
+
+  public void close() {
+    if (beeLine != null) {
+      beeLine.runCommands(new String[] {
+        "!quit"
+      });
+    }
+    if (beelineOutputStream != null) {
+      beelineOutputStream.close();
+    }
+  }
+
+  /**
+   * Builder to generated QFileBeeLineClient objects. The after initializing the builder, it can be
+   * used to create new clients without any parameters.
+   */
+  public static class QFileClientBuilder {
+    private String username;
+    private String password;
+    private String jdbcUrl;
+    private String jdbcDriver;
+
+    public QFileClientBuilder() {
+    }
+
+    public QFileClientBuilder setUsername(String username) {
+      this.username = username;
+      return this;
+    }
+
+    public QFileClientBuilder setPassword(String password) {
+      this.password = password;
+      return this;
+    }
+
+    public QFileClientBuilder setJdbcUrl(String jdbcUrl) {
+      this.jdbcUrl = jdbcUrl;
+      return this;
+    }
+
+    public QFileClientBuilder setJdbcDriver(String jdbcDriver) {
+      this.jdbcDriver = jdbcDriver;
+      return this;
+    }
+
+    public QFileBeeLineClient getClient(File logFile) throws IOException {
+      return new QFileBeeLineClient(jdbcUrl, jdbcDriver, username, password, logFile);
+    }
+  }
+}
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/package-info.java b/itests/util/src/main/java/org/apache/hive/beeline/package-info.java
new file mode 100644
index 0000000..e05ac0a
--- /dev/null
+++ b/itests/util/src/main/java/org/apache/hive/beeline/package-info.java
@@ -0,0 +1,22 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+/**
+ * Package for the BeeLine specific QTest classes.
+ */
+package org.apache.hive.beeline;
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFile.java b/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFile.java
deleted file mode 100644
index 9fae194..0000000
--- a/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFile.java
+++ /dev/null
@@ -1,336 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.beeline;
-
-import org.apache.commons.io.FileUtils;
-import org.apache.hadoop.hive.ql.QTestProcessExecResult;
-import org.apache.hadoop.hive.ql.QTestUtil;
-import org.apache.hadoop.util.Shell;
-import org.apache.hive.common.util.StreamPrinter;
-
-import java.io.ByteArrayOutputStream;
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.nio.charset.StandardCharsets;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.Set;
-import java.util.regex.Pattern;
-
-/**
- * Class for representing a Query and the connected files. It provides accessors for the specific
- * input and output files, and provides methods for filtering the output of the runs.
- */
-public final class QFile {
-  private static final Set<String> srcTables = QTestUtil.getSrcTables();
-  private static final String DEBUG_HINT =
-      "The following files can help you identifying the problem:%n"
-      + " - Query file: %1s%n"
-      + " - Raw output file: %2s%n"
-      + " - Filtered output file: %3s%n"
-      + " - Expected output file: %4s%n"
-      + " - Client log file: %5s%n"
-      + " - Client log files before the test: %6s%n"
-      + " - Client log files after the test: %7s%n"
-      + " - Hiveserver2 log file: %8s%n";
-  private static final String USE_COMMAND_WARNING =
-      "The query file %1s contains \"%2s\" command.%n"
-      + "The source table name rewrite is turned on, so this might cause problems when the used "
-      + "database contains tables named any of the following: " + srcTables + "%n"
-      + "To turn off the table name rewrite use -Dtest.rewrite.source.tables=false%n";
-
-  private static final Pattern USE_PATTERN =
-      Pattern.compile("^\\s*use\\s.*", Pattern.CASE_INSENSITIVE);
-
-  private String name;
-  private File inputFile;
-  private File rawOutputFile;
-  private File outputFile;
-  private File expectedOutputFile;
-  private File logFile;
-  private File beforeExecuteLogFile;
-  private File afterExecuteLogFile;
-  private static RegexFilterSet filterSet = getFilterSet();
-  private boolean rewriteSourceTables;
-
-  private QFile() {}
-
-  public String getName() {
-    return name;
-  }
-
-  public File getInputFile() {
-    return inputFile;
-  }
-
-  public File getRawOutputFile() {
-    return rawOutputFile;
-  }
-
-  public File getOutputFile() {
-    return outputFile;
-  }
-
-  public File getExpectedOutputFile() {
-    return expectedOutputFile;
-  }
-
-  public File getLogFile() {
-    return logFile;
-  }
-
-  public File getBeforeExecuteLogFile() {
-    return beforeExecuteLogFile;
-  }
-
-  public File getAfterExecuteLogFile() {
-    return afterExecuteLogFile;
-  }
-
-  public String getDebugHint() {
-    return String.format(DEBUG_HINT, inputFile, rawOutputFile, outputFile, expectedOutputFile,
-        logFile, beforeExecuteLogFile, afterExecuteLogFile,
-        "./itests/qtest/target/tmp/log/hive.log");
-  }
-
-  /**
-   * Filters the sql commands if necessary.
-   * @param commands The array of the sql commands before filtering
-   * @return The filtered array of the sql command strings
-   * @throws IOException File read error
-   */
-  public String[] filterCommands(String[] commands) throws IOException {
-    if (rewriteSourceTables) {
-      for (int i=0; i<commands.length; i++) {
-        if (USE_PATTERN.matcher(commands[i]).matches()) {
-          System.err.println(String.format(USE_COMMAND_WARNING, inputFile, commands[i]));
-        }
-        commands[i] = replaceTableNames(commands[i]);
-      }
-    }
-    return commands;
-  }
-
-  /**
-   * Replace the default src database TABLE_NAMEs in the queries with default.TABLE_NAME, like
-   * src->default.src, srcpart->default.srcpart, so the queries could be run even if the used
-   * database is query specific. This change is only a best effort, since we do not want to parse
-   * the queries, we could not be sure that we do not replace other strings which are not
-   * tablenames. Like 'select src from othertable;'. The q files containing these commands should
-   * be excluded. Only replace the tablenames, if rewriteSourceTables is set.
-   * @param source The original query string
-   * @return The query string where the tablenames are replaced
-   */
-  private String replaceTableNames(String source) {
-    for (String table : srcTables) {
-      source = source.replaceAll("(?is)(\\s+)" + table + "([\\s;\\n\\)])", "$1default." + table
-          + "$2");
-    }
-    return source;
-  }
-
-  public void filterOutput() throws IOException {
-    String rawOutput = FileUtils.readFileToString(rawOutputFile, "UTF-8");
-    String filteredOutput = filterSet.filter(rawOutput);
-    FileUtils.writeStringToFile(outputFile, filteredOutput);
-  }
-
-  public QTestProcessExecResult compareResults() throws IOException, InterruptedException {
-    if (!expectedOutputFile.exists()) {
-      throw new IOException("Expected results file does not exist: " + expectedOutputFile);
-    }
-    return executeDiff();
-  }
-
-  public void overwriteResults() throws IOException {
-    FileUtils.copyFile(outputFile, expectedOutputFile);
-  }
-
-  private QTestProcessExecResult executeDiff() throws IOException, InterruptedException {
-    List<String> diffCommandArgs = new ArrayList<String>();
-    diffCommandArgs.add("diff");
-
-    // Text file comparison
-    diffCommandArgs.add("-a");
-
-    if (Shell.WINDOWS) {
-      // Ignore changes in the amount of white space
-      diffCommandArgs.add("-b");
-
-      // Files created on Windows machines have different line endings
-      // than files created on Unix/Linux. Windows uses carriage return and line feed
-      // ("\r\n") as a line ending, whereas Unix uses just line feed ("\n").
-      // Also StringBuilder.toString(), Stream to String conversions adds extra
-      // spaces at the end of the line.
-      diffCommandArgs.add("--strip-trailing-cr"); // Strip trailing carriage return on input
-      diffCommandArgs.add("-B"); // Ignore changes whose lines are all blank
-    }
-
-    // Add files to compare to the arguments list
-    diffCommandArgs.add(getQuotedString(expectedOutputFile));
-    diffCommandArgs.add(getQuotedString(outputFile));
-
-    System.out.println("Running: " + org.apache.commons.lang.StringUtils.join(diffCommandArgs,
-        ' '));
-    Process executor = Runtime.getRuntime().exec(diffCommandArgs.toArray(
-        new String[diffCommandArgs.size()]));
-
-    ByteArrayOutputStream bos = new ByteArrayOutputStream();
-    PrintStream out = new PrintStream(bos, true, "UTF-8");
-
-    StreamPrinter errPrinter = new StreamPrinter(executor.getErrorStream(), null, System.err);
-    StreamPrinter outPrinter = new StreamPrinter(executor.getInputStream(), null, System.out, out);
-
-    outPrinter.start();
-    errPrinter.start();
-
-    int result = executor.waitFor();
-
-    outPrinter.join();
-    errPrinter.join();
-
-    executor.waitFor();
-
-    return QTestProcessExecResult.create(result, new String(bos.toByteArray(),
-        StandardCharsets.UTF_8));
-  }
-
-  private static String getQuotedString(File file) {
-    return Shell.WINDOWS ? String.format("\"%s\"", file.getAbsolutePath()) : file.getAbsolutePath();
-  }
-
-  private static class Filter {
-    private final Pattern pattern;
-    private final String replacement;
-
-    public Filter(Pattern pattern, String replacement) {
-      this.pattern = pattern;
-      this.replacement = replacement;
-    }
-  }
-
-  private static class RegexFilterSet {
-    private final List<Filter> regexFilters = new ArrayList<Filter>();
-
-    public RegexFilterSet addFilter(String regex, String replacement) {
-      regexFilters.add(new Filter(Pattern.compile(regex), replacement));
-      return this;
-    }
-
-    public String filter(String input) {
-      for (Filter filter : regexFilters) {
-        input = filter.pattern.matcher(input).replaceAll(filter.replacement);
-      }
-      return input;
-    }
-  }
-
-  // These are the filters which are common for every QTest.
-  // Check specificFilterSet for QTest specific ones.
-  private static RegexFilterSet getFilterSet() {
-    // Extract the leading four digits from the unix time value.
-    // Use this as a prefix in order to increase the selectivity
-    // of the unix time stamp replacement regex.
-    String currentTimePrefix = Long.toString(System.currentTimeMillis()).substring(0, 4);
-
-    String userName = System.getProperty("user.name");
-
-    String timePattern = "(Mon|Tue|Wed|Thu|Fri|Sat|Sun) "
-        + "(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) "
-        + "\\d{2} \\d{2}:\\d{2}:\\d{2} \\w+ 20\\d{2}";
-    String operatorPattern = "\"(CONDITION|COPY|DEPENDENCY_COLLECTION|DDL"
-        + "|EXPLAIN|FETCH|FIL|FS|FUNCTION|GBY|HASHTABLEDUMMY|HASTTABLESINK|JOIN"
-        + "|LATERALVIEWFORWARD|LIM|LVJ|MAP|MAPJOIN|MAPRED|MAPREDLOCAL|MOVE|OP|RS"
-        + "|SCR|SEL|STATS|TS|UDTF|UNION)_\\d+\"";
-
-    return new RegexFilterSet()
-        .addFilter("(?s)\n[^\n]*Waiting to acquire compile lock.*?Acquired the compile lock.\n",
-            "\n")
-        .addFilter(".*Acquired the compile lock.\n", "")
-        .addFilter("Getting log thread is interrupted, since query is done!\n", "")
-        .addFilter("going to print operations logs\n", "")
-        .addFilter("printed operations logs\n", "")
-        .addFilter("\\(queryId=[^\\)]*\\)", "queryId=(!!{queryId}!!)")
-        .addFilter("Query ID = [\\w-]+", "Query ID = !!{queryId}!!")
-        .addFilter("file:/\\w\\S+", "file:/!!ELIDED!!")
-        .addFilter("pfile:/\\w\\S+", "pfile:/!!ELIDED!!")
-        .addFilter("hdfs:/\\w\\S+", "hdfs:/!!ELIDED!!")
-        .addFilter("last_modified_by=\\w+", "last_modified_by=!!ELIDED!!")
-        .addFilter(timePattern, "!!TIMESTAMP!!")
-        .addFilter("(\\D)" + currentTimePrefix + "\\d{6}(\\D)", "$1!!UNIXTIME!!$2")
-        .addFilter("(\\D)" + currentTimePrefix + "\\d{9}(\\D)", "$1!!UNIXTIMEMILLIS!!$2")
-        .addFilter(userName, "!!{user.name}!!")
-        .addFilter(operatorPattern, "\"$1_!!ELIDED!!\"")
-        .addFilter("(?i)Time taken: [0-9\\.]* sec", "Time taken: !!ELIDED!! sec")
-        .addFilter(" job(:?) job_\\w+([\\s\n])", " job$1 !!{jobId}}!!$2")
-        .addFilter("Ended Job = job_\\w+([\\s\n])", "Ended Job = !!{jobId}!!$1")
-        .addFilter(".*\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2},\\d{3}.* map = .*\n", "")
-        .addFilter("\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\\s+", "")
-        .addFilter("maximum memory = \\d*", "maximum memory = !!ELIDED!!");
-  }
-
-  /**
-   * Builder to generate QFile objects. After initializing the builder it is possible the
-   * generate the next QFile object using it's name only.
-   */
-  public static class QFileBuilder {
-    private File queryDirectory;
-    private File logDirectory;
-    private File resultsDirectory;
-    private boolean rewriteSourceTables;
-
-    public QFileBuilder() {
-    }
-
-    public QFileBuilder setQueryDirectory(File queryDirectory) {
-      this.queryDirectory = queryDirectory;
-      return this;
-    }
-
-    public QFileBuilder setLogDirectory(File logDirectory) {
-      this.logDirectory = logDirectory;
-      return this;
-    }
-
-    public QFileBuilder setResultsDirectory(File resultsDirectory) {
-      this.resultsDirectory = resultsDirectory;
-      return this;
-    }
-
-    public QFileBuilder setRewriteSourceTables(boolean rewriteSourceTables) {
-      this.rewriteSourceTables = rewriteSourceTables;
-      return this;
-    }
-
-    public QFile getQFile(String name) throws IOException {
-      QFile result = new QFile();
-      result.name = name;
-      result.inputFile = new File(queryDirectory, name + ".q");
-      result.rawOutputFile = new File(logDirectory, name + ".q.out.raw");
-      result.outputFile = new File(logDirectory, name + ".q.out");
-      result.expectedOutputFile = new File(resultsDirectory, name + ".q.out");
-      result.logFile = new File(logDirectory, name + ".q.beeline");
-      result.beforeExecuteLogFile = new File(logDirectory, name + ".q.beforeExecute.log");
-      result.afterExecuteLogFile = new File(logDirectory, name + ".q.afterExecute.log");
-      result.rewriteSourceTables = rewriteSourceTables;
-      return result;
-    }
-  }
-}
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFileBeeLineClient.java b/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFileBeeLineClient.java
deleted file mode 100644
index 3e6475c..0000000
--- a/itests/util/src/main/java/org/apache/hive/beeline/qfile/QFileBeeLineClient.java
+++ /dev/null
@@ -1,145 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-package org.apache.hive.beeline;
-
-import java.io.File;
-import java.io.IOException;
-import java.io.PrintStream;
-import java.sql.SQLException;
-
-/**
- * QFile test client using BeeLine. It can be used to submit a list of command strings, or a QFile.
- */
-public class QFileBeeLineClient implements AutoCloseable {
-  private BeeLine beeLine;
-  private PrintStream beelineOutputStream;
-  private File logFile;
-
-  protected QFileBeeLineClient(String jdbcUrl, String jdbcDriver, String username, String password,
-      File log) throws IOException {
-    logFile = log;
-    beeLine = new BeeLine();
-    beelineOutputStream = new PrintStream(logFile, "UTF-8");
-    beeLine.setOutputStream(beelineOutputStream);
-    beeLine.setErrorStream(beelineOutputStream);
-    beeLine.runCommands(
-        new String[] {
-          "!set verbose true",
-          "!set shownestederrs true",
-          "!set showwarnings true",
-          "!set showelapsedtime false",
-          "!set trimscripts false",
-          "!set maxwidth -1",
-          "!connect " + jdbcUrl + " " + username + " " + password + " " + jdbcDriver,
-          "set hive.stats.dbclass=fs"
-        });
-  }
-
-  public void execute(String[] commands, File resultFile) throws SQLException {
-    beeLine.runCommands(
-        new String[] {
-          "!set outputformat csv",
-          "!record " + resultFile.getAbsolutePath()
-        });
-
-    int lastSuccessfulCommand = beeLine.runCommands(commands);
-    if (commands.length != lastSuccessfulCommand) {
-      throw new SQLException("Error executing SQL command: " + commands[lastSuccessfulCommand]);
-    }
-
-    beeLine.runCommands(new String[] {"!record"});
-  }
-
-  private void beforeExecute(QFile qFile) throws SQLException {
-    execute(
-        new String[] {
-          "USE default;",
-          "SHOW TABLES;",
-          "DROP DATABASE IF EXISTS `" + qFile.getName() + "` CASCADE;",
-          "CREATE DATABASE `" + qFile.getName() + "`;",
-          "USE `" + qFile.getName() + "`;"
-        },
-        qFile.getBeforeExecuteLogFile());
-  }
-
-  private void afterExecute(QFile qFile) throws SQLException {
-    execute(
-        new String[] {
-          "USE default;",
-          "DROP DATABASE IF EXISTS `" + qFile.getName() + "` CASCADE;",
-        },
-        qFile.getAfterExecuteLogFile());
-  }
-
-  public void execute(QFile qFile) throws SQLException, IOException {
-    beforeExecute(qFile);
-    String[] commands = beeLine.getCommands(qFile.getInputFile());
-    execute(qFile.filterCommands(commands), qFile.getRawOutputFile());
-    afterExecute(qFile);
-  }
-
-  public void close() {
-    if (beeLine != null) {
-      beeLine.runCommands(new String[] {
-        "!quit"
-      });
-    }
-    if (beelineOutputStream != null) {
-      beelineOutputStream.close();
-    }
-  }
-
-  /**
-   * Builder to generated QFileBeeLineClient objects. The after initializing the builder, it can be
-   * used to create new clients without any parameters.
-   */
-  public static class QFileClientBuilder {
-    private String username;
-    private String password;
-    private String jdbcUrl;
-    private String jdbcDriver;
-
-    public QFileClientBuilder() {
-    }
-
-    public QFileClientBuilder setUsername(String username) {
-      this.username = username;
-      return this;
-    }
-
-    public QFileClientBuilder setPassword(String password) {
-      this.password = password;
-      return this;
-    }
-
-    public QFileClientBuilder setJdbcUrl(String jdbcUrl) {
-      this.jdbcUrl = jdbcUrl;
-      return this;
-    }
-
-    public QFileClientBuilder setJdbcDriver(String jdbcDriver) {
-      this.jdbcDriver = jdbcDriver;
-      return this;
-    }
-
-    public QFileBeeLineClient getClient(File logFile) throws IOException {
-      return new QFileBeeLineClient(jdbcUrl, jdbcDriver, username, password, logFile);
-    }
-  }
-}
diff --git a/itests/util/src/main/java/org/apache/hive/beeline/qfile/package-info.java b/itests/util/src/main/java/org/apache/hive/beeline/qfile/package-info.java
deleted file mode 100644
index e05ac0a..0000000
--- a/itests/util/src/main/java/org/apache/hive/beeline/qfile/package-info.java
+++ /dev/null
@@ -1,22 +0,0 @@
-/**
- * Licensed to the Apache Software Foundation (ASF) under one
- * or more contributor license agreements.  See the NOTICE file
- * distributed with this work for additional information
- * regarding copyright ownership.  The ASF licenses this file
- * to you under the Apache License, Version 2.0 (the
- * "License"); you may not use this file except in compliance
- * with the License.  You may obtain a copy of the License at
- *
- *     http://www.apache.org/licenses/LICENSE-2.0
- *
- * Unless required by applicable law or agreed to in writing, software
- * distributed under the License is distributed on an "AS IS" BASIS,
- * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
- * See the License for the specific language governing permissions and
- * limitations under the License.
- */
-
-/**
- * Package for the BeeLine specific QTest classes.
- */
-package org.apache.hive.beeline;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
index 4518315..5337154 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PostExecutePrinter.java
@@ -25,6 +25,7 @@
 import java.util.Map;
 import java.util.Set;
 
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.metastore.api.Partition;
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
@@ -97,15 +98,15 @@ else if (o1.getKey().getDataContainer().isPartition() &&
   @Override
   public void run(HookContext hookContext) throws Exception {
     assert(hookContext.getHookType() == HookType.POST_EXEC_HOOK);
-    SessionState ss = SessionState.get();
     Set<ReadEntity> inputs = hookContext.getInputs();
     Set<WriteEntity> outputs = hookContext.getOutputs();
     LineageInfo linfo = hookContext.getLinfo();
     UserGroupInformation ugi = hookContext.getUgi();
-    this.run(ss,inputs,outputs,linfo,ugi);
+    this.run(hookContext.getConf().get(HiveConf.ConfVars.HIVEQUERYSTRING.varname),
+        hookContext.getQueryPlan().getOperationName(),inputs,outputs,linfo,ugi);
   }
 
-  public void run(SessionState sess, Set<ReadEntity> inputs,
+  public void run(String query, String type, Set<ReadEntity> inputs,
       Set<WriteEntity> outputs, LineageInfo linfo,
       UserGroupInformation ugi) throws Exception {
 
@@ -115,9 +116,11 @@ public void run(SessionState sess, Set<ReadEntity> inputs,
       return;
     }
 
-    if (sess != null) {
-      console.printError("POSTHOOK: query: " + sess.getCmd().trim());
-      console.printError("POSTHOOK: type: " + sess.getCommandType());
+    if (query != null) {
+      console.printError("POSTHOOK: query: " + query.trim());
+    }
+    if (type != null) {
+      console.printError("POSTHOOK: type: " + type);
     }
 
     PreExecutePrinter.printEntities(console, inputs, "POSTHOOK: Input: ");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
index b5a26d8..bd3cc62 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/hooks/PreExecutePrinter.java
@@ -24,6 +24,7 @@
 import java.util.Set;
 
 import org.apache.hadoop.hive.common.io.FetchConverter;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.hooks.HookContext.HookType;
 import org.apache.hadoop.hive.ql.plan.HiveOperation;
 import org.apache.hadoop.hive.ql.session.SessionState;
@@ -49,10 +50,11 @@ public void run(HookContext hookContext) throws Exception {
     Set<ReadEntity> inputs = hookContext.getInputs();
     Set<WriteEntity> outputs = hookContext.getOutputs();
     UserGroupInformation ugi = hookContext.getUgi();
-    this.run(ss,inputs,outputs,ugi);
+    this.run(hookContext.getConf().get(HiveConf.ConfVars.HIVEQUERYSTRING.varname),
+        hookContext.getQueryPlan().getOperationName(),inputs,outputs,ugi);
   }
 
-  public void run(SessionState sess, Set<ReadEntity> inputs,
+  public void run(String query, String type, Set<ReadEntity> inputs,
       Set<WriteEntity> outputs, UserGroupInformation ugi)
     throws Exception {
 
@@ -62,9 +64,11 @@ public void run(SessionState sess, Set<ReadEntity> inputs,
       return;
     }
 
-    if (sess != null) {
-      console.printError("PREHOOK: query: " + sess.getCmd().trim());
-      console.printError("PREHOOK: type: " + sess.getCommandType());
+    if (query != null) {
+      console.printError("PREHOOK: query: " + query.trim());
+    }
+    if (type != null) {
+      console.printError("PREHOOK: type: " + type);
     }
 
     printEntities(console, inputs, "PREHOOK: Input: ");
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java b/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
index e92f18c..e86d6fe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/OperationLog.java
@@ -37,6 +37,14 @@
 
   private final String operationName;
   private final LogFile logFile;
+  // If in test mode then the LogDivertAppenderForTest created an extra log file containing only
+  // the output needed for the qfile results.
+  private final LogFile testLogFile;
+  // True if we are running test and the extra test file should be used when the logs are
+  // requested.
+  private final boolean isShortLogs;
+  // True if the logs should be removed after the operation. Should be used only in test mode
+  private final boolean isRemoveLogs;
   private LoggingLevel opLoggingLevel = LoggingLevel.UNKNOWN;
 
   public PrintStream getPrintStream() {
@@ -55,6 +63,31 @@ public OperationLog(String name, File file, HiveConf hiveConf) throws FileNotFou
       String logLevel = hiveConf.getVar(HiveConf.ConfVars.HIVE_SERVER2_LOGGING_OPERATION_LEVEL);
       opLoggingLevel = getLoggingLevel(logLevel);
     }
+
+    // If in test mod create a test log file which will contain only logs which are supposed to
+    // be written to the qtest output
+    if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST)) {
+      isRemoveLogs = hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST_REMOVE_LOGS);
+      if (hiveConf.getBoolVar(HiveConf.ConfVars.HIVE_IN_TEST_SHORT_LOGS)) {
+        File testFile = new File(file.getAbsolutePath() + ".test");
+        try {
+          if (!testFile.createNewFile()) {
+            LOG.error("The testLogFile already exists");
+          }
+        } catch(IOException e) {
+          LOG.error("Unable to create testLogFile", e);
+        }
+        testLogFile = new LogFile(testFile);
+        isShortLogs = true;
+      } else {
+        testLogFile = null;
+        isShortLogs = false;
+      }
+    } else {
+      testLogFile = null;
+      isShortLogs = false;
+      isRemoveLogs = true;
+    }
   }
 
   public static LoggingLevel getLoggingLevel (String mode) {
@@ -107,6 +140,16 @@ public void writeOperationLog(String operationLogMessage) {
   }
 
   /**
+   * Write operation execution logs into log file
+   * @param operationLogMessage one line of log emitted from log4j
+   */
+  public void writeOperationLogForTest(String operationLogMessage) {
+    if (testLogFile != null) {
+      testLogFile.write(operationLogMessage);
+    }
+  }
+
+  /**
    * Read operation execution logs from log file
    * @param isFetchFirst true if the Enum FetchOrientation value is Fetch_First
    * @param maxRows the max number of fetched lines from log
@@ -114,15 +157,25 @@ public void writeOperationLog(String operationLogMessage) {
    * @throws java.sql.SQLException
    */
   public List<String> readOperationLog(boolean isFetchFirst, long maxRows)
-      throws SQLException{
-    return logFile.read(isFetchFirst, maxRows);
+      throws SQLException {
+    if (isShortLogs) {
+      return testLogFile.read(isFetchFirst, maxRows);
+    } else {
+      return logFile.read(isFetchFirst, maxRows);
+    }
   }
 
   /**
    * Close this OperationLog when operation is closed. The log file will be removed.
    */
   public void close() {
-    logFile.remove();
+    if (isShortLogs) {
+      // In case of test, do just close the log files, do not remove them.
+      logFile.close(isRemoveLogs);
+      testLogFile.close(isRemoveLogs);
+    } else {
+      logFile.close(true);
+    }
   }
 
   /**
@@ -156,7 +209,11 @@ synchronized void write(String msg) {
       return readResults(maxRows);
     }
 
-    synchronized void remove() {
+    /**
+     * Close the logs, and remove them if specified.
+     * @param removeLog If true, remove the log file
+     */
+    synchronized void close(boolean removeLog) {
       try {
         if (in != null) {
           in.close();
@@ -164,7 +221,7 @@ synchronized void remove() {
         if (out != null) {
           out.close();
         }
-        if (!isRemoved) {
+        if (!isRemoved && removeLog) {
           FileUtils.forceDelete(file);
           isRemoved = true;
         }
@@ -181,9 +238,16 @@ private void resetIn() {
     }
 
     private List<String> readResults(long nLines) throws SQLException {
+      List<String> logs = new ArrayList<String>();
       if (in == null) {
         try {
           in = new BufferedReader(new InputStreamReader(new FileInputStream(file)));
+          // Adding name of the log file in an extra log line, so it is easier to find
+          // the original if there is a test error
+          if (isShortLogs) {
+            logs.add("Reading log file: " + file);
+            nLines--;
+          }
         } catch (FileNotFoundException e) {
           if (isRemoved) {
             throw new SQLException("The operation has been closed and its log file " +
@@ -195,7 +259,6 @@ private void resetIn() {
         }
       }
 
-      List<String> logs = new ArrayList<String>();
       String line = "";
       // if nLines <= 0, read all lines in log file.
       for (int i = 0; i < nLines || nLines <= 0; i++) {
diff --git a/ql/src/test/results/clientpositive/beeline/drop_with_concurrency.q.out b/ql/src/test/results/clientpositive/beeline/drop_with_concurrency.q.out
index 8341b32..2c00f51 100644
--- a/ql/src/test/results/clientpositive/beeline/drop_with_concurrency.q.out
+++ b/ql/src/test/results/clientpositive/beeline/drop_with_concurrency.q.out
@@ -1,69 +1,20 @@
->>>  set hive.lock.numretries=1;
-No rows affected 
->>>  set hive.lock.sleep.between.retries=1;
-No rows affected 
->>>  set hive.support.concurrency=true;
-No rows affected 
->>>  set hive.lock.manager=org.apache.hadoop.hive.ql.lockmgr.EmbeddedLockManager;
-No rows affected 
->>>  
->>>  drop table if exists drop_with_concurrency_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): drop table if exists drop_with_concurrency_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): drop table if exists drop_with_concurrency_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query drop table if exists drop_with_concurrency_1
-No rows affected 
->>>  create table drop_with_concurrency_1 (c1 int);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table drop_with_concurrency_1 (c1 int)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table drop_with_concurrency_1 (c1 int)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:drop_with_concurrency
-ERROR : PREHOOK: Output: drop_with_concurrency@drop_with_concurrency_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:drop_with_concurrency
-ERROR : POSTHOOK: Output: drop_with_concurrency@drop_with_concurrency_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table drop_with_concurrency_1 (c1 int)
-No rows affected 
->>>  drop table drop_with_concurrency_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): drop table drop_with_concurrency_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): drop table drop_with_concurrency_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: drop_with_concurrency@drop_with_concurrency_1
-ERROR : PREHOOK: Output: drop_with_concurrency@drop_with_concurrency_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: drop_with_concurrency@drop_with_concurrency_1
-ERROR : POSTHOOK: Output: drop_with_concurrency@drop_with_concurrency_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query drop table drop_with_concurrency_1
-No rows affected 
->>>  !record
+PREHOOK: query: drop table if exists drop_with_concurrency_1
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: drop table if exists drop_with_concurrency_1
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table drop_with_concurrency_1 (c1 int)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@drop_with_concurrency_1
+POSTHOOK: query: create table drop_with_concurrency_1 (c1 int)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@drop_with_concurrency_1
+PREHOOK: query: drop table drop_with_concurrency_1
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@drop_with_concurrency_1
+PREHOOK: Output: default@drop_with_concurrency_1
+POSTHOOK: query: drop table drop_with_concurrency_1
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@drop_with_concurrency_1
+POSTHOOK: Output: default@drop_with_concurrency_1
diff --git a/ql/src/test/results/clientpositive/beeline/escape_comments.q.out b/ql/src/test/results/clientpositive/beeline/escape_comments.q.out
index 43f6a06..d1470a7 100644
--- a/ql/src/test/results/clientpositive/beeline/escape_comments.q.out
+++ b/ql/src/test/results/clientpositive/beeline/escape_comments.q.out
@@ -1,431 +1,218 @@
->>>  create database escape_comments_db comment 'a\nb';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create database escape_comments_db comment 'a\nb'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create database escape_comments_db comment 'a\nb'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:escape_comments_db
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:escape_comments_db
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create database escape_comments_db comment 'a\nb'
-No rows affected 
->>>  use escape_comments_db;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): use escape_comments_db
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): use escape_comments_db
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: database:escape_comments_db
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: database:escape_comments_db
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query use escape_comments_db
-No rows affected 
->>>  create table escape_comments_tbl1
-(col1 string comment 'a\nb\'\;') comment 'a\nb'
-partitioned by (p1 string comment 'a\nb');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table escape_comments_tbl1
+PREHOOK: query: create database escape_comments_db comment 'a\nb'
+PREHOOK: type: CREATEDATABASE
+PREHOOK: Output: database:escape_comments_db
+POSTHOOK: query: create database escape_comments_db comment 'a\nb'
+POSTHOOK: type: CREATEDATABASE
+POSTHOOK: Output: database:escape_comments_db
+PREHOOK: query: use escape_comments_db
+PREHOOK: type: SWITCHDATABASE
+PREHOOK: Input: database:escape_comments_db
+POSTHOOK: query: use escape_comments_db
+POSTHOOK: type: SWITCHDATABASE
+POSTHOOK: Input: database:escape_comments_db
+PREHOOK: query: create table escape_comments_tbl1
 (col1 string comment 'a\nb\';') comment 'a\nb'
 partitioned by (p1 string comment 'a\nb')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table escape_comments_tbl1
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:escape_comments_db
+PREHOOK: Output: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: create table escape_comments_tbl1
 (col1 string comment 'a\nb\';') comment 'a\nb'
 partitioned by (p1 string comment 'a\nb')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:escape_comments_db
-ERROR : PREHOOK: Output: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:escape_comments_db
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_tbl1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table escape_comments_tbl1
-(col1 string comment 'a\nb\';') comment 'a\nb'
-partitioned by (p1 string comment 'a\nb')
-No rows affected 
->>>  create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
-as select col1 from escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
-as select col1 from escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:escape_comments_db
+POSTHOOK: Output: escape_comments_db@escape_comments_tbl1
+PREHOOK: query: create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
 as select col1 from escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-ERROR : PREHOOK: Output: database:escape_comments_db
-ERROR : PREHOOK: Output: escape_comments_db@escape_comments_view1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-ERROR : POSTHOOK: Output: database:escape_comments_db
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_view1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
+PREHOOK: type: CREATEVIEW
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+PREHOOK: Output: database:escape_comments_db
+PREHOOK: Output: escape_comments_db@escape_comments_view1
+POSTHOOK: query: create view escape_comments_view1 (col1 comment 'a\nb') comment 'a\nb'
 as select col1 from escape_comments_tbl1
-No rows affected 
->>>  create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb'
-No rows affected 
->>>  
->>>  describe database extended escape_comments_db;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe database extended escape_comments_db
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:db_name, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer), FieldSchema(name:location, type:string, comment:from deserializer), FieldSchema(name:owner_name, type:string, comment:from deserializer), FieldSchema(name:owner_type, type:string, comment:from deserializer), FieldSchema(name:parameters, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe database extended escape_comments_db
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe database extended escape_comments_db
-'db_name','comment','location','owner_name','owner_type','parameters'
-'escape_comments_db','a','NULL','NULL','NULL','NULL'
-'b','location/in/test','user','USER','','NULL'
-2 rows selected 
->>>  describe database escape_comments_db;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe database escape_comments_db
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:db_name, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer), FieldSchema(name:location, type:string, comment:from deserializer), FieldSchema(name:owner_name, type:string, comment:from deserializer), FieldSchema(name:owner_type, type:string, comment:from deserializer), FieldSchema(name:parameters, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe database escape_comments_db
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe database escape_comments_db
-'db_name','comment','location','owner_name','owner_type','parameters'
-'escape_comments_db','a','NULL','NULL','NULL','NULL'
-'b','location/in/test','user','USER','','NULL'
-2 rows selected 
->>>  show create table escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): show create table escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:createtab_stmt, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): show create table escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query show create table escape_comments_tbl1
-'createtab_stmt'
-'CREATE TABLE `escape_comments_tbl1`('
-'  `col1` string COMMENT 'a'
-'b\'\;')'
-'COMMENT 'a'
-'b''
-'PARTITIONED BY ( '
-'  `p1` string COMMENT 'a'
-'b')'
-'ROW FORMAT SERDE '
-'  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' '
-'STORED AS INPUTFORMAT '
-'  'org.apache.hadoop.mapred.TextInputFormat' '
-'OUTPUTFORMAT '
-'  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat''
-'LOCATION'
-'  'file:/!!ELIDED!!
-'TBLPROPERTIES ('
-'  'transient_lastDdlTime'='!!UNIXTIME!!')'
-18 rows selected 
->>>  describe formatted escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe formatted escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col_name, type:string, comment:from deserializer), FieldSchema(name:data_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe formatted escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe formatted escape_comments_tbl1
-'col_name','data_type','comment'
-'# col_name            ','data_type           ','comment             '
-'','NULL','NULL'
-'col1','string','a'
-'b';','NULL','NULL'
-'','NULL','NULL'
-'# Partition Information','NULL','NULL'
-'# col_name            ','data_type           ','comment             '
-'','NULL','NULL'
-'p1','string','a'
-'b','NULL','NULL'
-'','NULL','NULL'
-'# Detailed Table Information','NULL','NULL'
-'Database:           ','escape_comments_db  ','NULL'
-'Owner:              ','user                ','NULL'
-'CreateTime:         ','!!TIMESTAMP!!','NULL'
-'LastAccessTime:     ','UNKNOWN             ','NULL'
-'Protect Mode:       ','None                ','NULL'
-'Retention:          ','0                   ','NULL'
-'Location:           ','file:/!!ELIDED!!
-'Table Type:         ','MANAGED_TABLE       ','NULL'
-'Table Parameters:','NULL','NULL'
-'','comment             ','a\nb                '
-'','transient_lastDdlTime','!!UNIXTIME!!          '
-'','NULL','NULL'
-'# Storage Information','NULL','NULL'
-'SerDe Library:      ','org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe','NULL'
-'InputFormat:        ','org.apache.hadoop.mapred.TextInputFormat','NULL'
-'OutputFormat:       ','org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat','NULL'
-'Compressed:         ','No                  ','NULL'
-'Num Buckets:        ','-1                  ','NULL'
-'Bucket Columns:     ','[]                  ','NULL'
-'Sort Columns:       ','[]                  ','NULL'
-'Storage Desc Params:','NULL','NULL'
-'','serialization.format','1                   '
-34 rows selected 
->>>  describe pretty escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe pretty escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col_name, type:string, comment:from deserializer), FieldSchema(name:data_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe pretty escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe pretty escape_comments_tbl1
-'col_name','data_type','comment'
-'col_name data_type     comment','NULL','NULL'
-'','NULL','NULL'
-'col1     string        a','NULL','NULL'
-'                       b';','NULL','NULL'
-'p1       string        a','NULL','NULL'
-'                       b','NULL','NULL'
-'','NULL','NULL'
-'# Partition Information','NULL','NULL'
-'col_name data_type     comment','NULL','NULL'
-'','NULL','NULL'
-'p1       string        a','NULL','NULL'
-'                       b','NULL','NULL'
-12 rows selected 
->>>  describe escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col_name, type:string, comment:from deserializer), FieldSchema(name:data_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe escape_comments_tbl1
-'col_name','data_type','comment'
-'col1','string','a'
-'b';','NULL','NULL'
-'p1','string','a'
-'b','NULL','NULL'
-'','NULL','NULL'
-'# Partition Information','NULL','NULL'
-'# col_name            ','data_type           ','comment             '
-'','NULL','NULL'
-'p1','string','a'
-'b','NULL','NULL'
-10 rows selected 
->>>  show create table escape_comments_view1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): show create table escape_comments_view1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:createtab_stmt, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): show create table escape_comments_view1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_view1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_view1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query show create table escape_comments_view1
-'createtab_stmt'
-'CREATE VIEW `escape_comments_view1` AS SELECT `col1` AS `col1` FROM (select `escape_comments_tbl1`.`col1` from `escape_comments_db`.`escape_comments_tbl1`) `escape_comments_db.escape_comments_view1`'
-1 row selected 
->>>  describe formatted escape_comments_view1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): describe formatted escape_comments_view1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:col_name, type:string, comment:from deserializer), FieldSchema(name:data_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): describe formatted escape_comments_view1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: escape_comments_db@escape_comments_view1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: escape_comments_db@escape_comments_view1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query describe formatted escape_comments_view1
-'col_name','data_type','comment'
-'# col_name            ','data_type           ','comment             '
-'','NULL','NULL'
-'col1','string','a'
-'b','NULL','NULL'
-'','NULL','NULL'
-'# Detailed Table Information','NULL','NULL'
-'Database:           ','escape_comments_db  ','NULL'
-'Owner:              ','user                ','NULL'
-'CreateTime:         ','!!TIMESTAMP!!','NULL'
-'LastAccessTime:     ','UNKNOWN             ','NULL'
-'Protect Mode:       ','None                ','NULL'
-'Retention:          ','0                   ','NULL'
-'Table Type:         ','VIRTUAL_VIEW        ','NULL'
-'Table Parameters:','NULL','NULL'
-'','comment             ','a\nb                '
-'','transient_lastDdlTime','!!UNIXTIME!!          '
-'','NULL','NULL'
-'# Storage Information','NULL','NULL'
-'SerDe Library:      ','null                ','NULL'
-'InputFormat:        ','org.apache.hadoop.mapred.TextInputFormat','NULL'
-'OutputFormat:       ','org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat','NULL'
-'Compressed:         ','No                  ','NULL'
-'Num Buckets:        ','-1                  ','NULL'
-'Bucket Columns:     ','[]                  ','NULL'
-'Sort Columns:       ','[]                  ','NULL'
-'','NULL','NULL'
-'# View Information','NULL','NULL'
-'View Original Text: ','select col1 from escape_comments_tbl1','NULL'
-'View Expanded Text: ','SELECT `col1` AS `col1` FROM (select `escape_comments_tbl1`.`col1` from `escape_comments_db`.`escape_comments_tbl1`) `escape_comments_db.escape_comments_view1`','NULL'
-29 rows selected 
->>>  show formatted index on escape_comments_tbl1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): show formatted index on escape_comments_tbl1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:idx_name, type:string, comment:from deserializer), FieldSchema(name:tab_name, type:string, comment:from deserializer), FieldSchema(name:col_names, type:string, comment:from deserializer), FieldSchema(name:idx_tab_name, type:string, comment:from deserializer), FieldSchema(name:idx_type, type:string, comment:from deserializer), FieldSchema(name:comment, type:string, comment:from deserializer)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): show formatted index on escape_comments_tbl1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query show formatted index on escape_comments_tbl1
-'idx_name','tab_name','col_names','idx_tab_name','idx_type','comment'
-'idx_name            ','tab_name            ','col_names           ','idx_tab_name        ','idx_type            ','comment             '
-'','NULL','NULL','NULL','NULL','NULL'
-'','NULL','NULL','NULL','NULL','NULL'
-'index2              ','escape_comments_tbl1','col1                ','escape_comments_db__escape_comments_tbl1_index2__','compact             ','a'
-'b                 ','','NULL','NULL','NULL','NULL'
-5 rows selected 
->>>  
->>>  drop database escape_comments_db cascade;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): drop database escape_comments_db cascade
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): drop database escape_comments_db cascade
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: database:escape_comments_db
-ERROR : PREHOOK: Output: database:escape_comments_db
-ERROR : PREHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
-ERROR : PREHOOK: Output: escape_comments_db@escape_comments_tbl1
-ERROR : PREHOOK: Output: escape_comments_db@escape_comments_view1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: database:escape_comments_db
-ERROR : POSTHOOK: Output: database:escape_comments_db
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_tbl1
-ERROR : POSTHOOK: Output: escape_comments_db@escape_comments_view1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query drop database escape_comments_db cascade
-No rows affected 
->>>  !record
+POSTHOOK: type: CREATEVIEW
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: Output: database:escape_comments_db
+POSTHOOK: Output: escape_comments_db@escape_comments_view1
+PREHOOK: query: create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb'
+PREHOOK: type: CREATEINDEX
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: create index index2 on table escape_comments_tbl1(col1) as 'COMPACT' with deferred rebuild comment 'a\nb'
+POSTHOOK: type: CREATEINDEX
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
+PREHOOK: query: describe database extended escape_comments_db
+PREHOOK: type: DESCDATABASE
+POSTHOOK: query: describe database extended escape_comments_db
+POSTHOOK: type: DESCDATABASE
+escape_comments_db	a	NULL	NULL	NULL	NULL
+b	location/in/test	user	USER		NULL
+PREHOOK: query: describe database escape_comments_db
+PREHOOK: type: DESCDATABASE
+POSTHOOK: query: describe database escape_comments_db
+POSTHOOK: type: DESCDATABASE
+escape_comments_db	a	NULL	NULL	NULL	NULL
+b	location/in/test	user	USER		NULL
+PREHOOK: query: show create table escape_comments_tbl1
+PREHOOK: type: SHOW_CREATETABLE
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: show create table escape_comments_tbl1
+POSTHOOK: type: SHOW_CREATETABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+CREATE TABLE `escape_comments_tbl1`(
+  `col1` string COMMENT 'a
+b\'\;')
+COMMENT 'a
+b'
+PARTITIONED BY ( 
+  `p1` string COMMENT 'a
+b')
+ROW FORMAT SERDE 
+  'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' 
+STORED AS INPUTFORMAT 
+  'org.apache.hadoop.mapred.TextInputFormat' 
+OUTPUTFORMAT 
+  'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
+LOCATION
+#### A masked pattern was here ####
+TBLPROPERTIES (
+#### A masked pattern was here ####
+PREHOOK: query: describe formatted escape_comments_tbl1
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: describe formatted escape_comments_tbl1
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+# col_name            	data_type           	comment             
+	NULL	NULL
+col1	string	a
+b';	NULL	NULL
+	NULL	NULL
+# Partition Information	NULL	NULL
+# col_name            	data_type           	comment             
+	NULL	NULL
+p1	string	a
+b	NULL	NULL
+	NULL	NULL
+# Detailed Table Information	NULL	NULL
+Database:           	escape_comments_db  	NULL
+Owner:              	user                	NULL
+#### A masked pattern was here ####
+LastAccessTime:     	UNKNOWN             	NULL
+Protect Mode:       	None                	NULL
+Retention:          	0                   	NULL
+#### A masked pattern was here ####
+Table Type:         	MANAGED_TABLE       	NULL
+Table Parameters:	NULL	NULL
+	comment             	a\nb                
+#### A masked pattern was here ####
+	NULL	NULL
+# Storage Information	NULL	NULL
+SerDe Library:      	org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe	NULL
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	NULL
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	NULL
+Compressed:         	No                  	NULL
+Num Buckets:        	-1                  	NULL
+Bucket Columns:     	[]                  	NULL
+Sort Columns:       	[]                  	NULL
+Storage Desc Params:	NULL	NULL
+	serialization.format	1                   
+PREHOOK: query: describe pretty escape_comments_tbl1
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: describe pretty escape_comments_tbl1
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+col_name data_type     comment	NULL	NULL
+	NULL	NULL
+col1     string        a	NULL	NULL
+                       b';	NULL	NULL
+p1       string        a	NULL	NULL
+                       b	NULL	NULL
+	NULL	NULL
+# Partition Information	NULL	NULL
+col_name data_type     comment	NULL	NULL
+	NULL	NULL
+p1       string        a	NULL	NULL
+                       b	NULL	NULL
+PREHOOK: query: describe escape_comments_tbl1
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: escape_comments_db@escape_comments_tbl1
+POSTHOOK: query: describe escape_comments_tbl1
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_tbl1
+col1	string	a
+b';	NULL	NULL
+p1	string	a
+b	NULL	NULL
+	NULL	NULL
+# Partition Information	NULL	NULL
+# col_name            	data_type           	comment             
+	NULL	NULL
+p1	string	a
+b	NULL	NULL
+PREHOOK: query: show create table escape_comments_view1
+PREHOOK: type: SHOW_CREATETABLE
+PREHOOK: Input: escape_comments_db@escape_comments_view1
+POSTHOOK: query: show create table escape_comments_view1
+POSTHOOK: type: SHOW_CREATETABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_view1
+CREATE VIEW `escape_comments_view1` AS SELECT `col1` AS `col1` FROM (select `escape_comments_tbl1`.`col1` from `escape_comments_db`.`escape_comments_tbl1`) `escape_comments_db.escape_comments_view1`
+PREHOOK: query: describe formatted escape_comments_view1
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: escape_comments_db@escape_comments_view1
+POSTHOOK: query: describe formatted escape_comments_view1
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: escape_comments_db@escape_comments_view1
+# col_name            	data_type           	comment             
+	NULL	NULL
+col1	string	a
+b	NULL	NULL
+	NULL	NULL
+# Detailed Table Information	NULL	NULL
+Database:           	escape_comments_db  	NULL
+Owner:              	user                	NULL
+#### A masked pattern was here ####
+LastAccessTime:     	UNKNOWN             	NULL
+Protect Mode:       	None                	NULL
+Retention:          	0                   	NULL
+Table Type:         	VIRTUAL_VIEW        	NULL
+Table Parameters:	NULL	NULL
+	comment             	a\nb                
+#### A masked pattern was here ####
+	NULL	NULL
+# Storage Information	NULL	NULL
+SerDe Library:      	null                	NULL
+InputFormat:        	org.apache.hadoop.mapred.TextInputFormat	NULL
+OutputFormat:       	org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat	NULL
+Compressed:         	No                  	NULL
+Num Buckets:        	-1                  	NULL
+Bucket Columns:     	[]                  	NULL
+Sort Columns:       	[]                  	NULL
+	NULL	NULL
+# View Information	NULL	NULL
+View Original Text: 	select col1 from escape_comments_tbl1	NULL
+View Expanded Text: 	SELECT `col1` AS `col1` FROM (select `escape_comments_tbl1`.`col1` from `escape_comments_db`.`escape_comments_tbl1`) `escape_comments_db.escape_comments_view1`	NULL
+PREHOOK: query: show formatted index on escape_comments_tbl1
+PREHOOK: type: SHOWINDEXES
+POSTHOOK: query: show formatted index on escape_comments_tbl1
+POSTHOOK: type: SHOWINDEXES
+idx_name            	tab_name            	col_names           	idx_tab_name        	idx_type            	comment             
+	NULL	NULL	NULL	NULL	NULL
+	NULL	NULL	NULL	NULL	NULL
+index2              	escape_comments_tbl1	col1                	escape_comments_db__escape_comments_tbl1_index2__	compact             	a
+b                 		NULL	NULL	NULL	NULL
+PREHOOK: query: drop database escape_comments_db cascade
+PREHOOK: type: DROPDATABASE
+PREHOOK: Input: database:escape_comments_db
+PREHOOK: Output: database:escape_comments_db
+PREHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
+PREHOOK: Output: escape_comments_db@escape_comments_tbl1
+PREHOOK: Output: escape_comments_db@escape_comments_view1
+POSTHOOK: query: drop database escape_comments_db cascade
+POSTHOOK: type: DROPDATABASE
+POSTHOOK: Input: database:escape_comments_db
+POSTHOOK: Output: database:escape_comments_db
+POSTHOOK: Output: escape_comments_db@escape_comments_db__escape_comments_tbl1_index2__
+POSTHOOK: Output: escape_comments_db@escape_comments_tbl1
+POSTHOOK: Output: escape_comments_db@escape_comments_view1
diff --git a/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out b/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out
new file mode 100644
index 0000000..08609f4
--- /dev/null
+++ b/ql/src/test/results/clientpositive/beeline/select_dummy_source.q.out
@@ -0,0 +1,251 @@
+PREHOOK: query: explain
+select 'a', 100
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select 'a', 100
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: 'a' (type: string), 100 (type: int)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 1 Data size: 89 Basic stats: COMPLETE Column stats: COMPLETE
+            ListSink
+
+PREHOOK: query: select 'a', 100
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select 'a', 100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+a	100
+PREHOOK: query: explain
+select 1 + 1
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select 1 + 1
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: 2 (type: int)
+            outputColumnNames: _col0
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+            ListSink
+
+PREHOOK: query: select 1 + 1
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select 1 + 1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+2
+PREHOOK: query: explain
+select explode(array('a', 'b'))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select explode(array('a', 'b'))
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: _dummy_table
+            Row Limit Per Split: 1
+            Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+            Select Operator
+              expressions: array('a','b') (type: array<string>)
+              outputColumnNames: _col0
+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+              UDTF Operator
+                Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                function name: explode
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select explode(array('a', 'b'))
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select explode(array('a', 'b'))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+a
+b
+PREHOOK: query: explain
+select 'a', 100
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select 'a', 100
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: 'a' (type: string), 100 (type: int)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 1 Data size: 89 Basic stats: COMPLETE Column stats: COMPLETE
+            ListSink
+
+PREHOOK: query: select 'a', 100
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select 'a', 100
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+a	100
+PREHOOK: query: explain
+select 1 + 1
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select 1 + 1
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: 2 (type: int)
+            outputColumnNames: _col0
+            Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: COMPLETE
+            ListSink
+
+PREHOOK: query: select 1 + 1
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select 1 + 1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+2
+PREHOOK: query: explain
+select explode(array('a', 'b'))
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select explode(array('a', 'b'))
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: array('a','b') (type: array<string>)
+            outputColumnNames: _col0
+            Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+            UDTF Operator
+              Statistics: Num rows: 1 Data size: 0 Basic stats: PARTIAL Column stats: COMPLETE
+              function name: explode
+              ListSink
+
+PREHOOK: query: select explode(array('a', 'b'))
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select explode(array('a', 'b'))
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+a
+b
+PREHOOK: query: explain
+select 2 + 3,x from (select 1 + 2 x) X
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
+select 2 + 3,x from (select 1 + 2 x) X
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        TableScan
+          alias: _dummy_table
+          Row Limit Per Split: 1
+          Statistics: Num rows: 1 Data size: 1 Basic stats: COMPLETE Column stats: COMPLETE
+          Select Operator
+            expressions: 5 (type: int), 3 (type: int)
+            outputColumnNames: _col0, _col1
+            Statistics: Num rows: 1 Data size: 8 Basic stats: COMPLETE Column stats: COMPLETE
+            ListSink
+
+PREHOOK: query: select 2 + 3,x from (select 1 + 2 x) X
+PREHOOK: type: QUERY
+PREHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+POSTHOOK: query: select 2 + 3,x from (select 1 + 2 x) X
+POSTHOOK: type: QUERY
+POSTHOOK: Input: _dummy_database@_dummy_table
+#### A masked pattern was here ####
+5	3
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_1.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_1.q.out
index 7e50833..7918405 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_1.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_1.q.out
@@ -1,965 +1,490 @@
->>>  
->>>  
->>>  
->>>  
->>>  create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_1
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_1
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_1
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_1
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_1
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_1
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  
->>>  load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_1.smb_bucket_1 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_1.smb_bucket_1 stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_2
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_1.smb_bucket_2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_1.smb_bucket_2 stats: [numFiles=1, numRows=0, totalSize=206, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_1@smb_bucket_3
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_1.smb_bucket_3 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_1.smb_bucket_3 stats: [numFiles=1, numRows=0, totalSize=222, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_1@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-No rows affected 
->>>  
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  
->>>  >>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-No rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','NULL','NULL'
-'5','val_5','NULL','NULL'
-'10','val_10','NULL','NULL'
-5 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	NULL	NULL
+5	val_5	NULL	NULL
+10	val_10	NULL	NULL
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-'NULL','NULL','25','val_25'
-'NULL','NULL','30','val_30'
-4 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+NULL	NULL	25	val_25
+NULL	NULL	30	val_30
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','NULL','NULL'
-'5','val_5','NULL','NULL'
-'10','val_10','NULL','NULL'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-'NULL','NULL','25','val_25'
-'NULL','NULL','30','val_30'
-9 rows selected 
->>>  
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	NULL	NULL
+5	val_5	NULL	NULL
+10	val_10	NULL	NULL
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+NULL	NULL	25	val_25
+NULL	NULL	30	val_30
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-No rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','NULL','NULL'
-'5','val_5','NULL','NULL'
-'10','val_10','NULL','NULL'
-5 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	NULL	NULL
+5	val_5	NULL	NULL
+10	val_10	NULL	NULL
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-'NULL','NULL','25','val_25'
-'NULL','NULL','30','val_30'
-4 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+NULL	NULL	25	val_25
+NULL	NULL	30	val_30
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_1@smb_bucket_2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','NULL','NULL'
-'5','val_5','NULL','NULL'
-'10','val_10','NULL','NULL'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-'NULL','NULL','25','val_25'
-'NULL','NULL','30','val_30'
-9 rows selected 
->>>  
->>>   
->>>  
->>>  
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_2
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	NULL	NULL
+5	val_5	NULL	NULL
+10	val_10	NULL	NULL
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+NULL	NULL	25	val_25
+NULL	NULL	30	val_30
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_10.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_10.q.out
index 9a528c7..a3c2428 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_10.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_10.q.out
@@ -1,251 +1,107 @@
->>>  
->>>  create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_10
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_10
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  
->>>  alter table tmp_smb_bucket_10 add partition (ds = '1');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): alter table tmp_smb_bucket_10 add partition (ds = '1')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): alter table tmp_smb_bucket_10 add partition (ds = '1')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query alter table tmp_smb_bucket_10 add partition (ds = '1')
-No rows affected 
->>>  alter table tmp_smb_bucket_10 add partition (ds = '2');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): alter table tmp_smb_bucket_10 add partition (ds = '2')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): alter table tmp_smb_bucket_10 add partition (ds = '2')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query alter table tmp_smb_bucket_10 add partition (ds = '2')
-No rows affected 
->>>  
->>>  >>>   
->>>  load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_10.tmp_smb_bucket_10 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Partition smb_mapjoin_10.tmp_smb_bucket_10{ds=1} stats: [numFiles=1, totalSize=208]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_10.tmp_smb_bucket_10 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Partition smb_mapjoin_10.tmp_smb_bucket_10{ds=1} stats: [numFiles=2, totalSize=414]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
-No rows affected 
->>>  
->>>  load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=2
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_10.tmp_smb_bucket_10 partition (ds=2) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Partition smb_mapjoin_10.tmp_smb_bucket_10{ds=2} stats: [numFiles=1, totalSize=208]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2');
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=2
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_10.tmp_smb_bucket_10 partition (ds=2) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Partition smb_mapjoin_10.tmp_smb_bucket_10{ds=2} stats: [numFiles=2, totalSize=414]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_10@tmp_smb_bucket_10@ds=2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
-No rows affected 
->>>  
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
-on (a.ds = '1' and b.ds = '2' and
-    a.userid = b.userid and
-    a.pageid = b.pageid and
-    a.postid = b.postid and
-    a.type = b.type);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
-on (a.ds = '1' and b.ds = '2' and
-    a.userid = b.userid and
-    a.pageid = b.pageid and
-    a.postid = b.postid and
-    a.type = b.type)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: query: create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@tmp_smb_bucket_10
+POSTHOOK: query: create table tmp_smb_bucket_10(userid int, pageid int, postid int, type string) partitioned by (ds string) CLUSTERED BY (userid) SORTED BY (pageid, postid, type, userid) INTO 2 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@tmp_smb_bucket_10
+PREHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '1')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Output: default@tmp_smb_bucket_10
+POSTHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '1')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Output: default@tmp_smb_bucket_10
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
+PREHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '2')
+PREHOOK: type: ALTERTABLE_ADDPARTS
+PREHOOK: Output: default@tmp_smb_bucket_10
+POSTHOOK: query: alter table tmp_smb_bucket_10 add partition (ds = '2')
+POSTHOOK: type: ALTERTABLE_ADDPARTS
+POSTHOOK: Output: default@tmp_smb_bucket_10
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@tmp_smb_bucket_10@ds=1
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@tmp_smb_bucket_10@ds=1
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='1')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=1
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@tmp_smb_bucket_10@ds=2
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@tmp_smb_bucket_10@ds=2
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' INTO TABLE tmp_smb_bucket_10 partition(ds='2')
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@tmp_smb_bucket_10@ds=2
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
 on (a.ds = '1' and b.ds = '2' and
     a.userid = b.userid and
     a.pageid = b.pageid and
     a.postid = b.postid and
     a.type = b.type)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from tmp_smb_bucket_10 a join tmp_smb_bucket_10 b 
 on (a.ds = '1' and b.ds = '2' and
     a.userid = b.userid and
     a.pageid = b.pageid and
     a.postid = b.postid and
     a.type = b.type)
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 3 Data size: 414 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: (((userid is not null and pageid is not null) and postid is not null) and type is not null) (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 138 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 userid (type: int), pageid (type: int), postid (type: int), type (type: string)'
-'                  1 userid (type: int), pageid (type: int), postid (type: int), type (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3, _col8, _col9, _col10, _col11'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: string), '1' (type: string), _col8 (type: int), _col9 (type: int), _col10 (type: int), _col11 (type: string), '2' (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 3 Data size: 414 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: (((userid is not null and pageid is not null) and postid is not null) and type is not null) (type: boolean)
+              Statistics: Num rows: 1 Data size: 138 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 userid (type: int), pageid (type: int), postid (type: int), type (type: string)
+                  1 userid (type: int), pageid (type: int), postid (type: int), type (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3, _col8, _col9, _col10, _col11
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: int), _col2 (type: int), _col3 (type: string), '1' (type: string), _col8 (type: int), _col9 (type: int), _col10 (type: int), _col11 (type: string), '2' (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8, _col9
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
index 30e3ef7..1e14877 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
@@ -1,2646 +1,2239 @@
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  set hive.enforce.bucketing=true;
-No rows affected 
->>>  set hive.enforce.sorting=true;
-No rows affected 
->>>  set hive.exec.reducers.max = 1;
-No rows affected 
->>>  set hive.merge.mapfiles=false;
-No rows affected 
->>>  set hive.merge.mapredfiles=false; 
-No rows affected 
->>>  
->>>  >>>  
->>>  >>>  CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_11
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_11
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_11
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_11
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  
->>>  FROM default.src
-INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): FROM default.src
-INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:int, comment:null), FieldSchema(name:_col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): FROM default.src
+PREHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table1
+POSTHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table1
+PREHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table2
+POSTHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table2
+PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: default@src
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table1@ds=1
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table2@ds=1
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 2
-INFO  : Launching Job 1 out of 2
-INFO  : Starting task [Stage-2:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_11.test_table1 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Launching Job 2 out of 2
-INFO  : Starting task [Stage-4:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-3:STATS] in serial mode
-INFO  : Partition smb_mapjoin_11.test_table1{ds=1} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-1:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_11.test_table2 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-5:STATS] in serial mode
-INFO  : Partition smb_mapjoin_11.test_table2{ds=1} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: default@src
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table1@ds=1
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table2@ds=1
-ERROR : POSTHOOK: Lineage: test_table1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-4:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query FROM default.src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@test_table1@ds=1
+PREHOOK: Output: default@test_table2@ds=1
+POSTHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
-No rows affected 
->>>  
->>>  set hive.enforce.bucketing=false;
-No rows affected 
->>>  set hive.enforce.sorting=false;
-No rows affected 
->>>  
->>>  >>>  CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_11
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_11
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  
->>>  >>>  EXPLAIN EXTENDED
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@test_table1@ds=1
+POSTHOOK: Output: default@test_table2@ds=1
+POSTHOOK: Lineage: test_table1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table3
+POSTHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table3
+PREHOOK: query: EXPLAIN EXTENDED
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-4:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_JOIN'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table1'
-'            a'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table2'
-'            b'
-'         AND'
-'            AND'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     key'
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        b'
-'                     key'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     ds'
-'                  '1''
-'            ='
-'               .'
-'                  TOK_TABLE_OR_COL'
-'                     b'
-'                  ds'
-'               '1''
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_TAB'
-'            TOK_TABNAME'
-'               test_table3'
-'            TOK_PARTSPEC'
-'               TOK_PARTVAL'
-'                  ds'
-'                  '1''
-'      TOK_SELECT'
-'         TOK_HINTLIST'
-'            TOK_HINT'
-'               TOK_MAPJOIN'
-'               TOK_HINTARGLIST'
-'                  b'
-'         TOK_SELEXPR'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'         TOK_SELEXPR'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  b'
-'               value'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-'  Stage-2 depends on stages: Stage-0'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col7'
-'                Position of Big Table: 0'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col7 (type: string)'
-'                  outputColumnNames: _col0, _col1'
-'                  File Output Operator'
-'                    compressed: false'
-'                    GlobalTableId: 1'
-'                    directory: file:/!!ELIDED!!
-'                    NumFilesPerFileSink: 1'
-'                    Static Partition Specification: ds=1/'
-'                    Stats Publishing Key Prefix: file:/!!ELIDED!!
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        properties:'
-'                          bucket_count 16'
-'                          bucket_field_name key'
-'                          columns key,value'
-'                          columns.comments '
-'                          columns.types int:string'
-'                          file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                          file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                          location file:/!!ELIDED!!
-'                          name smb_mapjoin_11.test_table3'
-'                          partition_columns ds'
-'                          partition_columns.types string'
-'                          serialization.ddl struct test_table3 { i32 key, string value}'
-'                          serialization.format 1'
-'                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                          transient_lastDdlTime !!UNIXTIME!!'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                        name: smb_mapjoin_11.test_table3'
-'                    TotalFiles: 1'
-'                    GatherStats: true'
-'                    MultiFileSpray: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [a]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: ds=1'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            partition values:'
-'              ds 1'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_11.test_table1'
-'              numFiles 16'
-'              numRows 0'
-'              partition_columns ds'
-'              partition_columns.types string'
-'              rawDataSize 0'
-'              serialization.ddl struct test_table1 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 5812'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_11.test_table1'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table1 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_11.test_table1'
-'            name: smb_mapjoin_11.test_table1'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_11.db/test_table1/ds=1 [a]'
-''
-'  Stage: Stage-0'
-'    Move Operator'
-'      tables:'
-'          partition:'
-'            ds 1'
-'          replace: true'
-'          source: file:/!!ELIDED!!
-'          table:'
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_11.test_table3'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_11.test_table3'
-''
-'  Stage: Stage-2'
-'    Stats-Aggr Operator'
-'      Stats Aggregation Key Prefix: file:/!!ELIDED!!
-''
-213 rows selected 
->>>  
->>>  INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table2
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table2@ds=1
-ERROR : PREHOOK: Output: smb_mapjoin_11@test_table3@ds=1
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_11.test_table3 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Partition smb_mapjoin_11.test_table3{ds=1} stats: [numFiles=16, numRows=1028, totalSize=11996, rawDataSize=10968]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table2
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table2@ds=1
-ERROR : POSTHOOK: Output: smb_mapjoin_11@test_table3@ds=1
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=1).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value SIMPLE [(test_table2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
-No rows affected 
->>>  
->>>  SELECT * FROM test_table1 ORDER BY key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT * FROM test_table1 ORDER BY key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.key, type:int, comment:null), FieldSchema(name:test_table1.value, type:string, comment:null), FieldSchema(name:test_table1.ds, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT * FROM test_table1 ORDER BY key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT * FROM test_table1 ORDER BY key
-'test_table1.key','test_table1.value','test_table1.ds'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'2','val_2','1'
-'4','val_4','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'8','val_8','1'
-'9','val_9','1'
-'10','val_10','1'
-'11','val_11','1'
-'12','val_12','1'
-'12','val_12','1'
-'15','val_15','1'
-'15','val_15','1'
-'17','val_17','1'
-'18','val_18','1'
-'18','val_18','1'
-'19','val_19','1'
-'20','val_20','1'
-'24','val_24','1'
-'24','val_24','1'
-'26','val_26','1'
-'26','val_26','1'
-'27','val_27','1'
-'28','val_28','1'
-'30','val_30','1'
-'33','val_33','1'
-'34','val_34','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'37','val_37','1'
-'37','val_37','1'
-'41','val_41','1'
-'42','val_42','1'
-'42','val_42','1'
-'43','val_43','1'
-'44','val_44','1'
-'47','val_47','1'
-'51','val_51','1'
-'51','val_51','1'
-'53','val_53','1'
-'54','val_54','1'
-'57','val_57','1'
-'58','val_58','1'
-'58','val_58','1'
-'64','val_64','1'
-'65','val_65','1'
-'66','val_66','1'
-'67','val_67','1'
-'67','val_67','1'
-'69','val_69','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'72','val_72','1'
-'72','val_72','1'
-'74','val_74','1'
-'76','val_76','1'
-'76','val_76','1'
-'77','val_77','1'
-'78','val_78','1'
-'80','val_80','1'
-'82','val_82','1'
-'83','val_83','1'
-'83','val_83','1'
-'84','val_84','1'
-'84','val_84','1'
-'85','val_85','1'
-'86','val_86','1'
-'87','val_87','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'92','val_92','1'
-'95','val_95','1'
-'95','val_95','1'
-'96','val_96','1'
-'97','val_97','1'
-'97','val_97','1'
-'98','val_98','1'
-'98','val_98','1'
-'100','val_100','1'
-'100','val_100','1'
-'103','val_103','1'
-'103','val_103','1'
-'104','val_104','1'
-'104','val_104','1'
-'105','val_105','1'
-'111','val_111','1'
-'113','val_113','1'
-'113','val_113','1'
-'114','val_114','1'
-'116','val_116','1'
-'118','val_118','1'
-'118','val_118','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'120','val_120','1'
-'120','val_120','1'
-'125','val_125','1'
-'125','val_125','1'
-'126','val_126','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'129','val_129','1'
-'129','val_129','1'
-'131','val_131','1'
-'133','val_133','1'
-'134','val_134','1'
-'134','val_134','1'
-'136','val_136','1'
-'137','val_137','1'
-'137','val_137','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'143','val_143','1'
-'145','val_145','1'
-'146','val_146','1'
-'146','val_146','1'
-'149','val_149','1'
-'149','val_149','1'
-'150','val_150','1'
-'152','val_152','1'
-'152','val_152','1'
-'153','val_153','1'
-'155','val_155','1'
-'156','val_156','1'
-'157','val_157','1'
-'158','val_158','1'
-'160','val_160','1'
-'162','val_162','1'
-'163','val_163','1'
-'164','val_164','1'
-'164','val_164','1'
-'165','val_165','1'
-'165','val_165','1'
-'166','val_166','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'168','val_168','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'170','val_170','1'
-'172','val_172','1'
-'172','val_172','1'
-'174','val_174','1'
-'174','val_174','1'
-'175','val_175','1'
-'175','val_175','1'
-'176','val_176','1'
-'176','val_176','1'
-'177','val_177','1'
-'178','val_178','1'
-'179','val_179','1'
-'179','val_179','1'
-'180','val_180','1'
-'181','val_181','1'
-'183','val_183','1'
-'186','val_186','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'189','val_189','1'
-'190','val_190','1'
-'191','val_191','1'
-'191','val_191','1'
-'192','val_192','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'194','val_194','1'
-'195','val_195','1'
-'195','val_195','1'
-'196','val_196','1'
-'197','val_197','1'
-'197','val_197','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'200','val_200','1'
-'200','val_200','1'
-'201','val_201','1'
-'202','val_202','1'
-'203','val_203','1'
-'203','val_203','1'
-'205','val_205','1'
-'205','val_205','1'
-'207','val_207','1'
-'207','val_207','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'209','val_209','1'
-'209','val_209','1'
-'213','val_213','1'
-'213','val_213','1'
-'214','val_214','1'
-'216','val_216','1'
-'216','val_216','1'
-'217','val_217','1'
-'217','val_217','1'
-'218','val_218','1'
-'219','val_219','1'
-'219','val_219','1'
-'221','val_221','1'
-'221','val_221','1'
-'222','val_222','1'
-'223','val_223','1'
-'223','val_223','1'
-'224','val_224','1'
-'224','val_224','1'
-'226','val_226','1'
-'228','val_228','1'
-'229','val_229','1'
-'229','val_229','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'233','val_233','1'
-'233','val_233','1'
-'235','val_235','1'
-'237','val_237','1'
-'237','val_237','1'
-'238','val_238','1'
-'238','val_238','1'
-'239','val_239','1'
-'239','val_239','1'
-'241','val_241','1'
-'242','val_242','1'
-'242','val_242','1'
-'244','val_244','1'
-'247','val_247','1'
-'248','val_248','1'
-'249','val_249','1'
-'252','val_252','1'
-'255','val_255','1'
-'255','val_255','1'
-'256','val_256','1'
-'256','val_256','1'
-'257','val_257','1'
-'258','val_258','1'
-'260','val_260','1'
-'262','val_262','1'
-'263','val_263','1'
-'265','val_265','1'
-'265','val_265','1'
-'266','val_266','1'
-'272','val_272','1'
-'272','val_272','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'274','val_274','1'
-'275','val_275','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'278','val_278','1'
-'278','val_278','1'
-'280','val_280','1'
-'280','val_280','1'
-'281','val_281','1'
-'281','val_281','1'
-'282','val_282','1'
-'282','val_282','1'
-'283','val_283','1'
-'284','val_284','1'
-'285','val_285','1'
-'286','val_286','1'
-'287','val_287','1'
-'288','val_288','1'
-'288','val_288','1'
-'289','val_289','1'
-'291','val_291','1'
-'292','val_292','1'
-'296','val_296','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'302','val_302','1'
-'305','val_305','1'
-'306','val_306','1'
-'307','val_307','1'
-'307','val_307','1'
-'308','val_308','1'
-'309','val_309','1'
-'309','val_309','1'
-'310','val_310','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'315','val_315','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'317','val_317','1'
-'317','val_317','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'321','val_321','1'
-'321','val_321','1'
-'322','val_322','1'
-'322','val_322','1'
-'323','val_323','1'
-'325','val_325','1'
-'325','val_325','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'331','val_331','1'
-'331','val_331','1'
-'332','val_332','1'
-'333','val_333','1'
-'333','val_333','1'
-'335','val_335','1'
-'336','val_336','1'
-'338','val_338','1'
-'339','val_339','1'
-'341','val_341','1'
-'342','val_342','1'
-'342','val_342','1'
-'344','val_344','1'
-'344','val_344','1'
-'345','val_345','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'351','val_351','1'
-'353','val_353','1'
-'353','val_353','1'
-'356','val_356','1'
-'360','val_360','1'
-'362','val_362','1'
-'364','val_364','1'
-'365','val_365','1'
-'366','val_366','1'
-'367','val_367','1'
-'367','val_367','1'
-'368','val_368','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'373','val_373','1'
-'374','val_374','1'
-'375','val_375','1'
-'377','val_377','1'
-'378','val_378','1'
-'379','val_379','1'
-'382','val_382','1'
-'382','val_382','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'386','val_386','1'
-'389','val_389','1'
-'392','val_392','1'
-'393','val_393','1'
-'394','val_394','1'
-'395','val_395','1'
-'395','val_395','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'397','val_397','1'
-'397','val_397','1'
-'399','val_399','1'
-'399','val_399','1'
-'400','val_400','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'402','val_402','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'404','val_404','1'
-'404','val_404','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'407','val_407','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'411','val_411','1'
-'413','val_413','1'
-'413','val_413','1'
-'414','val_414','1'
-'414','val_414','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'418','val_418','1'
-'419','val_419','1'
-'421','val_421','1'
-'424','val_424','1'
-'424','val_424','1'
-'427','val_427','1'
-'429','val_429','1'
-'429','val_429','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'432','val_432','1'
-'435','val_435','1'
-'436','val_436','1'
-'437','val_437','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'439','val_439','1'
-'439','val_439','1'
-'443','val_443','1'
-'444','val_444','1'
-'446','val_446','1'
-'448','val_448','1'
-'449','val_449','1'
-'452','val_452','1'
-'453','val_453','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'455','val_455','1'
-'457','val_457','1'
-'458','val_458','1'
-'458','val_458','1'
-'459','val_459','1'
-'459','val_459','1'
-'460','val_460','1'
-'462','val_462','1'
-'462','val_462','1'
-'463','val_463','1'
-'463','val_463','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'467','val_467','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'470','val_470','1'
-'472','val_472','1'
-'475','val_475','1'
-'477','val_477','1'
-'478','val_478','1'
-'478','val_478','1'
-'479','val_479','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'481','val_481','1'
-'482','val_482','1'
-'483','val_483','1'
-'484','val_484','1'
-'485','val_485','1'
-'487','val_487','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'490','val_490','1'
-'491','val_491','1'
-'492','val_492','1'
-'492','val_492','1'
-'493','val_493','1'
-'494','val_494','1'
-'495','val_495','1'
-'496','val_496','1'
-'497','val_497','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-500 rows selected 
->>>  SELECT * FROM test_table3 ORDER BY key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT * FROM test_table3 ORDER BY key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table3.key, type:int, comment:null), FieldSchema(name:test_table3.value, type:string, comment:null), FieldSchema(name:test_table3.ds, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT * FROM test_table3 ORDER BY key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT * FROM test_table3 ORDER BY key
-'test_table3.key','test_table3.value','test_table3.ds'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'0','val_0','1'
-'2','val_2','1'
-'4','val_4','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'5','val_5','1'
-'8','val_8','1'
-'9','val_9','1'
-'10','val_10','1'
-'11','val_11','1'
-'12','val_12','1'
-'12','val_12','1'
-'12','val_12','1'
-'12','val_12','1'
-'15','val_15','1'
-'15','val_15','1'
-'15','val_15','1'
-'15','val_15','1'
-'17','val_17','1'
-'18','val_18','1'
-'18','val_18','1'
-'18','val_18','1'
-'18','val_18','1'
-'19','val_19','1'
-'20','val_20','1'
-'24','val_24','1'
-'24','val_24','1'
-'24','val_24','1'
-'24','val_24','1'
-'26','val_26','1'
-'26','val_26','1'
-'26','val_26','1'
-'26','val_26','1'
-'27','val_27','1'
-'28','val_28','1'
-'30','val_30','1'
-'33','val_33','1'
-'34','val_34','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'35','val_35','1'
-'37','val_37','1'
-'37','val_37','1'
-'37','val_37','1'
-'37','val_37','1'
-'41','val_41','1'
-'42','val_42','1'
-'42','val_42','1'
-'42','val_42','1'
-'42','val_42','1'
-'43','val_43','1'
-'44','val_44','1'
-'47','val_47','1'
-'51','val_51','1'
-'51','val_51','1'
-'51','val_51','1'
-'51','val_51','1'
-'53','val_53','1'
-'54','val_54','1'
-'57','val_57','1'
-'58','val_58','1'
-'58','val_58','1'
-'58','val_58','1'
-'58','val_58','1'
-'64','val_64','1'
-'65','val_65','1'
-'66','val_66','1'
-'67','val_67','1'
-'67','val_67','1'
-'67','val_67','1'
-'67','val_67','1'
-'69','val_69','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'70','val_70','1'
-'72','val_72','1'
-'72','val_72','1'
-'72','val_72','1'
-'72','val_72','1'
-'74','val_74','1'
-'76','val_76','1'
-'76','val_76','1'
-'76','val_76','1'
-'76','val_76','1'
-'77','val_77','1'
-'78','val_78','1'
-'80','val_80','1'
-'82','val_82','1'
-'83','val_83','1'
-'83','val_83','1'
-'83','val_83','1'
-'83','val_83','1'
-'84','val_84','1'
-'84','val_84','1'
-'84','val_84','1'
-'84','val_84','1'
-'85','val_85','1'
-'86','val_86','1'
-'87','val_87','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'90','val_90','1'
-'92','val_92','1'
-'95','val_95','1'
-'95','val_95','1'
-'95','val_95','1'
-'95','val_95','1'
-'96','val_96','1'
-'97','val_97','1'
-'97','val_97','1'
-'97','val_97','1'
-'97','val_97','1'
-'98','val_98','1'
-'98','val_98','1'
-'98','val_98','1'
-'98','val_98','1'
-'100','val_100','1'
-'100','val_100','1'
-'100','val_100','1'
-'100','val_100','1'
-'103','val_103','1'
-'103','val_103','1'
-'103','val_103','1'
-'103','val_103','1'
-'104','val_104','1'
-'104','val_104','1'
-'104','val_104','1'
-'104','val_104','1'
-'105','val_105','1'
-'111','val_111','1'
-'113','val_113','1'
-'113','val_113','1'
-'113','val_113','1'
-'113','val_113','1'
-'114','val_114','1'
-'116','val_116','1'
-'118','val_118','1'
-'118','val_118','1'
-'118','val_118','1'
-'118','val_118','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'119','val_119','1'
-'120','val_120','1'
-'120','val_120','1'
-'120','val_120','1'
-'120','val_120','1'
-'125','val_125','1'
-'125','val_125','1'
-'125','val_125','1'
-'125','val_125','1'
-'126','val_126','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'128','val_128','1'
-'129','val_129','1'
-'129','val_129','1'
-'129','val_129','1'
-'129','val_129','1'
-'131','val_131','1'
-'133','val_133','1'
-'134','val_134','1'
-'134','val_134','1'
-'134','val_134','1'
-'134','val_134','1'
-'136','val_136','1'
-'137','val_137','1'
-'137','val_137','1'
-'137','val_137','1'
-'137','val_137','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'138','val_138','1'
-'143','val_143','1'
-'145','val_145','1'
-'146','val_146','1'
-'146','val_146','1'
-'146','val_146','1'
-'146','val_146','1'
-'149','val_149','1'
-'149','val_149','1'
-'149','val_149','1'
-'149','val_149','1'
-'150','val_150','1'
-'152','val_152','1'
-'152','val_152','1'
-'152','val_152','1'
-'152','val_152','1'
-'153','val_153','1'
-'155','val_155','1'
-'156','val_156','1'
-'157','val_157','1'
-'158','val_158','1'
-'160','val_160','1'
-'162','val_162','1'
-'163','val_163','1'
-'164','val_164','1'
-'164','val_164','1'
-'164','val_164','1'
-'164','val_164','1'
-'165','val_165','1'
-'165','val_165','1'
-'165','val_165','1'
-'165','val_165','1'
-'166','val_166','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'167','val_167','1'
-'168','val_168','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'169','val_169','1'
-'170','val_170','1'
-'172','val_172','1'
-'172','val_172','1'
-'172','val_172','1'
-'172','val_172','1'
-'174','val_174','1'
-'174','val_174','1'
-'174','val_174','1'
-'174','val_174','1'
-'175','val_175','1'
-'175','val_175','1'
-'175','val_175','1'
-'175','val_175','1'
-'176','val_176','1'
-'176','val_176','1'
-'176','val_176','1'
-'176','val_176','1'
-'177','val_177','1'
-'178','val_178','1'
-'179','val_179','1'
-'179','val_179','1'
-'179','val_179','1'
-'179','val_179','1'
-'180','val_180','1'
-'181','val_181','1'
-'183','val_183','1'
-'186','val_186','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'187','val_187','1'
-'189','val_189','1'
-'190','val_190','1'
-'191','val_191','1'
-'191','val_191','1'
-'191','val_191','1'
-'191','val_191','1'
-'192','val_192','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'194','val_194','1'
-'195','val_195','1'
-'195','val_195','1'
-'195','val_195','1'
-'195','val_195','1'
-'196','val_196','1'
-'197','val_197','1'
-'197','val_197','1'
-'197','val_197','1'
-'197','val_197','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'199','val_199','1'
-'200','val_200','1'
-'200','val_200','1'
-'200','val_200','1'
-'200','val_200','1'
-'201','val_201','1'
-'202','val_202','1'
-'203','val_203','1'
-'203','val_203','1'
-'203','val_203','1'
-'203','val_203','1'
-'205','val_205','1'
-'205','val_205','1'
-'205','val_205','1'
-'205','val_205','1'
-'207','val_207','1'
-'207','val_207','1'
-'207','val_207','1'
-'207','val_207','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'208','val_208','1'
-'209','val_209','1'
-'209','val_209','1'
-'209','val_209','1'
-'209','val_209','1'
-'213','val_213','1'
-'213','val_213','1'
-'213','val_213','1'
-'213','val_213','1'
-'214','val_214','1'
-'216','val_216','1'
-'216','val_216','1'
-'216','val_216','1'
-'216','val_216','1'
-'217','val_217','1'
-'217','val_217','1'
-'217','val_217','1'
-'217','val_217','1'
-'218','val_218','1'
-'219','val_219','1'
-'219','val_219','1'
-'219','val_219','1'
-'219','val_219','1'
-'221','val_221','1'
-'221','val_221','1'
-'221','val_221','1'
-'221','val_221','1'
-'222','val_222','1'
-'223','val_223','1'
-'223','val_223','1'
-'223','val_223','1'
-'223','val_223','1'
-'224','val_224','1'
-'224','val_224','1'
-'224','val_224','1'
-'224','val_224','1'
-'226','val_226','1'
-'228','val_228','1'
-'229','val_229','1'
-'229','val_229','1'
-'229','val_229','1'
-'229','val_229','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'230','val_230','1'
-'233','val_233','1'
-'233','val_233','1'
-'233','val_233','1'
-'233','val_233','1'
-'235','val_235','1'
-'237','val_237','1'
-'237','val_237','1'
-'237','val_237','1'
-'237','val_237','1'
-'238','val_238','1'
-'238','val_238','1'
-'238','val_238','1'
-'238','val_238','1'
-'239','val_239','1'
-'239','val_239','1'
-'239','val_239','1'
-'239','val_239','1'
-'241','val_241','1'
-'242','val_242','1'
-'242','val_242','1'
-'242','val_242','1'
-'242','val_242','1'
-'244','val_244','1'
-'247','val_247','1'
-'248','val_248','1'
-'249','val_249','1'
-'252','val_252','1'
-'255','val_255','1'
-'255','val_255','1'
-'255','val_255','1'
-'255','val_255','1'
-'256','val_256','1'
-'256','val_256','1'
-'256','val_256','1'
-'256','val_256','1'
-'257','val_257','1'
-'258','val_258','1'
-'260','val_260','1'
-'262','val_262','1'
-'263','val_263','1'
-'265','val_265','1'
-'265','val_265','1'
-'265','val_265','1'
-'265','val_265','1'
-'266','val_266','1'
-'272','val_272','1'
-'272','val_272','1'
-'272','val_272','1'
-'272','val_272','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'274','val_274','1'
-'275','val_275','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'277','val_277','1'
-'278','val_278','1'
-'278','val_278','1'
-'278','val_278','1'
-'278','val_278','1'
-'280','val_280','1'
-'280','val_280','1'
-'280','val_280','1'
-'280','val_280','1'
-'281','val_281','1'
-'281','val_281','1'
-'281','val_281','1'
-'281','val_281','1'
-'282','val_282','1'
-'282','val_282','1'
-'282','val_282','1'
-'282','val_282','1'
-'283','val_283','1'
-'284','val_284','1'
-'285','val_285','1'
-'286','val_286','1'
-'287','val_287','1'
-'288','val_288','1'
-'288','val_288','1'
-'288','val_288','1'
-'288','val_288','1'
-'289','val_289','1'
-'291','val_291','1'
-'292','val_292','1'
-'296','val_296','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'298','val_298','1'
-'302','val_302','1'
-'305','val_305','1'
-'306','val_306','1'
-'307','val_307','1'
-'307','val_307','1'
-'307','val_307','1'
-'307','val_307','1'
-'308','val_308','1'
-'309','val_309','1'
-'309','val_309','1'
-'309','val_309','1'
-'309','val_309','1'
-'310','val_310','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'311','val_311','1'
-'315','val_315','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'316','val_316','1'
-'317','val_317','1'
-'317','val_317','1'
-'317','val_317','1'
-'317','val_317','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'318','val_318','1'
-'321','val_321','1'
-'321','val_321','1'
-'321','val_321','1'
-'321','val_321','1'
-'322','val_322','1'
-'322','val_322','1'
-'322','val_322','1'
-'322','val_322','1'
-'323','val_323','1'
-'325','val_325','1'
-'325','val_325','1'
-'325','val_325','1'
-'325','val_325','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'327','val_327','1'
-'331','val_331','1'
-'331','val_331','1'
-'331','val_331','1'
-'331','val_331','1'
-'332','val_332','1'
-'333','val_333','1'
-'333','val_333','1'
-'333','val_333','1'
-'333','val_333','1'
-'335','val_335','1'
-'336','val_336','1'
-'338','val_338','1'
-'339','val_339','1'
-'341','val_341','1'
-'342','val_342','1'
-'342','val_342','1'
-'342','val_342','1'
-'342','val_342','1'
-'344','val_344','1'
-'344','val_344','1'
-'344','val_344','1'
-'344','val_344','1'
-'345','val_345','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'348','val_348','1'
-'351','val_351','1'
-'353','val_353','1'
-'353','val_353','1'
-'353','val_353','1'
-'353','val_353','1'
-'356','val_356','1'
-'360','val_360','1'
-'362','val_362','1'
-'364','val_364','1'
-'365','val_365','1'
-'366','val_366','1'
-'367','val_367','1'
-'367','val_367','1'
-'367','val_367','1'
-'367','val_367','1'
-'368','val_368','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'373','val_373','1'
-'374','val_374','1'
-'375','val_375','1'
-'377','val_377','1'
-'378','val_378','1'
-'379','val_379','1'
-'382','val_382','1'
-'382','val_382','1'
-'382','val_382','1'
-'382','val_382','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'384','val_384','1'
-'386','val_386','1'
-'389','val_389','1'
-'392','val_392','1'
-'393','val_393','1'
-'394','val_394','1'
-'395','val_395','1'
-'395','val_395','1'
-'395','val_395','1'
-'395','val_395','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'396','val_396','1'
-'397','val_397','1'
-'397','val_397','1'
-'397','val_397','1'
-'397','val_397','1'
-'399','val_399','1'
-'399','val_399','1'
-'399','val_399','1'
-'399','val_399','1'
-'400','val_400','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'402','val_402','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'403','val_403','1'
-'404','val_404','1'
-'404','val_404','1'
-'404','val_404','1'
-'404','val_404','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'406','val_406','1'
-'407','val_407','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'409','val_409','1'
-'411','val_411','1'
-'413','val_413','1'
-'413','val_413','1'
-'413','val_413','1'
-'413','val_413','1'
-'414','val_414','1'
-'414','val_414','1'
-'414','val_414','1'
-'414','val_414','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'418','val_418','1'
-'419','val_419','1'
-'421','val_421','1'
-'424','val_424','1'
-'424','val_424','1'
-'424','val_424','1'
-'424','val_424','1'
-'427','val_427','1'
-'429','val_429','1'
-'429','val_429','1'
-'429','val_429','1'
-'429','val_429','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'430','val_430','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'431','val_431','1'
-'432','val_432','1'
-'435','val_435','1'
-'436','val_436','1'
-'437','val_437','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'438','val_438','1'
-'439','val_439','1'
-'439','val_439','1'
-'439','val_439','1'
-'439','val_439','1'
-'443','val_443','1'
-'444','val_444','1'
-'446','val_446','1'
-'448','val_448','1'
-'449','val_449','1'
-'452','val_452','1'
-'453','val_453','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'454','val_454','1'
-'455','val_455','1'
-'457','val_457','1'
-'458','val_458','1'
-'458','val_458','1'
-'458','val_458','1'
-'458','val_458','1'
-'459','val_459','1'
-'459','val_459','1'
-'459','val_459','1'
-'459','val_459','1'
-'460','val_460','1'
-'462','val_462','1'
-'462','val_462','1'
-'462','val_462','1'
-'462','val_462','1'
-'463','val_463','1'
-'463','val_463','1'
-'463','val_463','1'
-'463','val_463','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'466','val_466','1'
-'467','val_467','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'468','val_468','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'469','val_469','1'
-'470','val_470','1'
-'472','val_472','1'
-'475','val_475','1'
-'477','val_477','1'
-'478','val_478','1'
-'478','val_478','1'
-'478','val_478','1'
-'478','val_478','1'
-'479','val_479','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'480','val_480','1'
-'481','val_481','1'
-'482','val_482','1'
-'483','val_483','1'
-'484','val_484','1'
-'485','val_485','1'
-'487','val_487','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'489','val_489','1'
-'490','val_490','1'
-'491','val_491','1'
-'492','val_492','1'
-'492','val_492','1'
-'492','val_492','1'
-'492','val_492','1'
-'493','val_493','1'
-'494','val_494','1'
-'495','val_495','1'
-'496','val_496','1'
-'497','val_497','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-'498','val_498','1'
-1,028 rows selected 
->>>  EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-2:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_TABREF'
-'         TOK_TABNAME'
-'            test_table1'
-'         TOK_TABLEBUCKETSAMPLE'
-'            2'
-'            16'
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_DIR'
-'            TOK_TMP_FILE'
-'      TOK_SELECT'
-'         TOK_SELEXPR'
-'            TOK_ALLCOLREF'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: test_table1'
-'            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: true'
-'              predicate: (((hash(key) & 2147483647) % 16) = 1) (type: boolean)'
-'              sampleDesc: BUCKET 2 OUT OF 16'
-'              Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE'
-'              Select Operator'
-'                expressions: key (type: int), value (type: string), ds (type: string)'
-'                outputColumnNames: _col0, _col1, _col2'
-'                Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE'
-'                File Output Operator'
-'                  compressed: false'
-'                  GlobalTableId: 0'
-'                  directory: file:/!!ELIDED!!
-'                  NumFilesPerFileSink: 1'
-'                  Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE'
-'                  Stats Publishing Key Prefix: file:/!!ELIDED!!
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      properties:'
-'                        columns _col0,_col1,_col2'
-'                        columns.types int:string:string'
-'                        escape.delim \'
-'                        hive.serialization.extend.additional.nesting.levels true'
-'                        serialization.format 1'
-'                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  TotalFiles: 1'
-'                  GatherStats: false'
-'                  MultiFileSpray: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [test_table1]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: 000001_0'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            partition values:'
-'              ds 1'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_11.test_table1'
-'              numFiles 16'
-'              numRows 0'
-'              partition_columns ds'
-'              partition_columns.types string'
-'              rawDataSize 0'
-'              serialization.ddl struct test_table1 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 5812'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_11.test_table1'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table1 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_11.test_table1'
-'            name: smb_mapjoin_11.test_table1'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_11.db/test_table1/ds=1/000001_0 [test_table1]'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-125 rows selected 
->>>  EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-2:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_TABREF'
-'         TOK_TABNAME'
-'            test_table3'
-'         TOK_TABLEBUCKETSAMPLE'
-'            2'
-'            16'
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_DIR'
-'            TOK_TMP_FILE'
-'      TOK_SELECT'
-'         TOK_SELEXPR'
-'            TOK_ALLCOLREF'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: test_table3'
-'            Statistics: Num rows: 1028 Data size: 10968 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: true'
-'              predicate: (((hash(key) & 2147483647) % 16) = 1) (type: boolean)'
-'              sampleDesc: BUCKET 2 OUT OF 16'
-'              Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE'
-'              Select Operator'
-'                expressions: key (type: int), value (type: string), ds (type: string)'
-'                outputColumnNames: _col0, _col1, _col2'
-'                Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE'
-'                File Output Operator'
-'                  compressed: false'
-'                  GlobalTableId: 0'
-'                  directory: file:/!!ELIDED!!
-'                  NumFilesPerFileSink: 1'
-'                  Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE'
-'                  Stats Publishing Key Prefix: file:/!!ELIDED!!
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      properties:'
-'                        columns _col0,_col1,_col2'
-'                        columns.types int:string:string'
-'                        escape.delim \'
-'                        hive.serialization.extend.additional.nesting.levels true'
-'                        serialization.format 1'
-'                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  TotalFiles: 1'
-'                  GatherStats: false'
-'                  MultiFileSpray: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [test_table3]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: (ds%3D1)000001_0'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            partition values:'
-'              ds 1'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_11.test_table3'
-'              numFiles 16'
-'              numRows 1028'
-'              partition_columns ds'
-'              partition_columns.types string'
-'              rawDataSize 10968'
-'              serialization.ddl struct test_table3 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 11996'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_11.test_table3'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_11.test_table3'
-'            name: smb_mapjoin_11.test_table3'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_11.db/test_table3/ds=1/(ds%3D1)000001_0 [test_table3]'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-124 rows selected 
->>>  SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table1.key, type:int, comment:null), FieldSchema(name:test_table1.value, type:string, comment:null), FieldSchema(name:test_table1.ds, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
-'test_table1.key','test_table1.value','test_table1.ds'
-'17','val_17','1'
-'33','val_33','1'
-'65','val_65','1'
-'97','val_97','1'
-'97','val_97','1'
-'113','val_113','1'
-'113','val_113','1'
-'129','val_129','1'
-'129','val_129','1'
-'145','val_145','1'
-'177','val_177','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'209','val_209','1'
-'209','val_209','1'
-'241','val_241','1'
-'257','val_257','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'289','val_289','1'
-'305','val_305','1'
-'321','val_321','1'
-'321','val_321','1'
-'353','val_353','1'
-'353','val_353','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'449','val_449','1'
-'481','val_481','1'
-'497','val_497','1'
-41 rows selected 
->>>  SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:test_table3.key, type:int, comment:null), FieldSchema(name:test_table3.value, type:string, comment:null), FieldSchema(name:test_table3.ds, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
-'test_table3.key','test_table3.value','test_table3.ds'
-'17','val_17','1'
-'33','val_33','1'
-'65','val_65','1'
-'97','val_97','1'
-'97','val_97','1'
-'97','val_97','1'
-'97','val_97','1'
-'113','val_113','1'
-'113','val_113','1'
-'113','val_113','1'
-'113','val_113','1'
-'129','val_129','1'
-'129','val_129','1'
-'129','val_129','1'
-'129','val_129','1'
-'145','val_145','1'
-'177','val_177','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'193','val_193','1'
-'209','val_209','1'
-'209','val_209','1'
-'209','val_209','1'
-'209','val_209','1'
-'241','val_241','1'
-'257','val_257','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'273','val_273','1'
-'289','val_289','1'
-'305','val_305','1'
-'321','val_321','1'
-'321','val_321','1'
-'321','val_321','1'
-'321','val_321','1'
-'353','val_353','1'
-'353','val_353','1'
-'353','val_353','1'
-'353','val_353','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'369','val_369','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'401','val_401','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'417','val_417','1'
-'449','val_449','1'
-'481','val_481','1'
-'497','val_497','1'
-97 rows selected 
->>>  
->>>  >>>  SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 2
-INFO  : Launching Job 1 out of 2
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 2 out of 2
-INFO  : Starting task [Stage-2:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table1@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_11@test_table3@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-'_c0'
-'293'
-1 row selected 
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_JOIN
+         TOK_TABREF
+            TOK_TABNAME
+               test_table1
+            a
+         TOK_TABREF
+            TOK_TABNAME
+               test_table2
+            b
+         AND
+            AND
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     key
+                  .
+                     TOK_TABLE_OR_COL
+                        b
+                     key
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     ds
+                  '1'
+            =
+               .
+                  TOK_TABLE_OR_COL
+                     b
+                  ds
+               '1'
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_TAB
+            TOK_TABNAME
+               test_table3
+            TOK_PARTSPEC
+               TOK_PARTVAL
+                  ds
+                  '1'
+      TOK_SELECT
+         TOK_HINTLIST
+            TOK_HINT
+               TOK_MAPJOIN
+               TOK_HINTARGLIST
+                  b
+         TOK_SELEXPR
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+         TOK_SELEXPR
+            .
+               TOK_TABLE_OR_COL
+                  b
+               value
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col7
+                Position of Big Table: 0
+                Select Operator
+                  expressions: _col0 (type: int), _col7 (type: string)
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 1
+#### A masked pattern was here ####
+                    NumFilesPerFileSink: 1
+                    Static Partition Specification: ds=1/
+#### A masked pattern was here ####
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        properties:
+                          bucket_count 16
+                          bucket_field_name key
+                          columns key,value
+                          columns.comments 
+                          columns.types int:string
+#### A masked pattern was here ####
+                          name default.test_table3
+                          partition_columns ds
+                          partition_columns.types string
+                          serialization.ddl struct test_table3 { i32 key, string value}
+                          serialization.format 1
+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.test_table3
+                    TotalFiles: 1
+                    GatherStats: true
+                    MultiFileSpray: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds=1
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table1
+              numFiles 16
+              numRows 0
+              partition_columns ds
+              partition_columns.types string
+              rawDataSize 0
+              serialization.ddl struct test_table1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table1
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table1
+            name: default.test_table1
+      Truncated Path -> Alias:
+        /test_table1/ds=1 [a]
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ds 1
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+PREHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+PREHOOK: Input: default@test_table2
+PREHOOK: Input: default@test_table2@ds=1
+PREHOOK: Output: default@test_table3@ds=1
+POSTHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds = '1'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+POSTHOOK: Input: default@test_table2
+POSTHOOK: Input: default@test_table2@ds=1
+POSTHOOK: Output: default@test_table3@ds=1
+POSTHOOK: Lineage: test_table3 PARTITION(ds=1).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value SIMPLE [(test_table2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: SELECT * FROM test_table1 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM test_table1 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+#### A masked pattern was here ####
+0	val_0	1
+0	val_0	1
+0	val_0	1
+2	val_2	1
+4	val_4	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+8	val_8	1
+9	val_9	1
+10	val_10	1
+11	val_11	1
+12	val_12	1
+12	val_12	1
+15	val_15	1
+15	val_15	1
+17	val_17	1
+18	val_18	1
+18	val_18	1
+19	val_19	1
+20	val_20	1
+24	val_24	1
+24	val_24	1
+26	val_26	1
+26	val_26	1
+27	val_27	1
+28	val_28	1
+30	val_30	1
+33	val_33	1
+34	val_34	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+37	val_37	1
+37	val_37	1
+41	val_41	1
+42	val_42	1
+42	val_42	1
+43	val_43	1
+44	val_44	1
+47	val_47	1
+51	val_51	1
+51	val_51	1
+53	val_53	1
+54	val_54	1
+57	val_57	1
+58	val_58	1
+58	val_58	1
+64	val_64	1
+65	val_65	1
+66	val_66	1
+67	val_67	1
+67	val_67	1
+69	val_69	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+72	val_72	1
+72	val_72	1
+74	val_74	1
+76	val_76	1
+76	val_76	1
+77	val_77	1
+78	val_78	1
+80	val_80	1
+82	val_82	1
+83	val_83	1
+83	val_83	1
+84	val_84	1
+84	val_84	1
+85	val_85	1
+86	val_86	1
+87	val_87	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+92	val_92	1
+95	val_95	1
+95	val_95	1
+96	val_96	1
+97	val_97	1
+97	val_97	1
+98	val_98	1
+98	val_98	1
+100	val_100	1
+100	val_100	1
+103	val_103	1
+103	val_103	1
+104	val_104	1
+104	val_104	1
+105	val_105	1
+111	val_111	1
+113	val_113	1
+113	val_113	1
+114	val_114	1
+116	val_116	1
+118	val_118	1
+118	val_118	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+120	val_120	1
+120	val_120	1
+125	val_125	1
+125	val_125	1
+126	val_126	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+129	val_129	1
+129	val_129	1
+131	val_131	1
+133	val_133	1
+134	val_134	1
+134	val_134	1
+136	val_136	1
+137	val_137	1
+137	val_137	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+143	val_143	1
+145	val_145	1
+146	val_146	1
+146	val_146	1
+149	val_149	1
+149	val_149	1
+150	val_150	1
+152	val_152	1
+152	val_152	1
+153	val_153	1
+155	val_155	1
+156	val_156	1
+157	val_157	1
+158	val_158	1
+160	val_160	1
+162	val_162	1
+163	val_163	1
+164	val_164	1
+164	val_164	1
+165	val_165	1
+165	val_165	1
+166	val_166	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+168	val_168	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+170	val_170	1
+172	val_172	1
+172	val_172	1
+174	val_174	1
+174	val_174	1
+175	val_175	1
+175	val_175	1
+176	val_176	1
+176	val_176	1
+177	val_177	1
+178	val_178	1
+179	val_179	1
+179	val_179	1
+180	val_180	1
+181	val_181	1
+183	val_183	1
+186	val_186	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+189	val_189	1
+190	val_190	1
+191	val_191	1
+191	val_191	1
+192	val_192	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+194	val_194	1
+195	val_195	1
+195	val_195	1
+196	val_196	1
+197	val_197	1
+197	val_197	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+200	val_200	1
+200	val_200	1
+201	val_201	1
+202	val_202	1
+203	val_203	1
+203	val_203	1
+205	val_205	1
+205	val_205	1
+207	val_207	1
+207	val_207	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+209	val_209	1
+209	val_209	1
+213	val_213	1
+213	val_213	1
+214	val_214	1
+216	val_216	1
+216	val_216	1
+217	val_217	1
+217	val_217	1
+218	val_218	1
+219	val_219	1
+219	val_219	1
+221	val_221	1
+221	val_221	1
+222	val_222	1
+223	val_223	1
+223	val_223	1
+224	val_224	1
+224	val_224	1
+226	val_226	1
+228	val_228	1
+229	val_229	1
+229	val_229	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+233	val_233	1
+233	val_233	1
+235	val_235	1
+237	val_237	1
+237	val_237	1
+238	val_238	1
+238	val_238	1
+239	val_239	1
+239	val_239	1
+241	val_241	1
+242	val_242	1
+242	val_242	1
+244	val_244	1
+247	val_247	1
+248	val_248	1
+249	val_249	1
+252	val_252	1
+255	val_255	1
+255	val_255	1
+256	val_256	1
+256	val_256	1
+257	val_257	1
+258	val_258	1
+260	val_260	1
+262	val_262	1
+263	val_263	1
+265	val_265	1
+265	val_265	1
+266	val_266	1
+272	val_272	1
+272	val_272	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+274	val_274	1
+275	val_275	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+278	val_278	1
+278	val_278	1
+280	val_280	1
+280	val_280	1
+281	val_281	1
+281	val_281	1
+282	val_282	1
+282	val_282	1
+283	val_283	1
+284	val_284	1
+285	val_285	1
+286	val_286	1
+287	val_287	1
+288	val_288	1
+288	val_288	1
+289	val_289	1
+291	val_291	1
+292	val_292	1
+296	val_296	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+302	val_302	1
+305	val_305	1
+306	val_306	1
+307	val_307	1
+307	val_307	1
+308	val_308	1
+309	val_309	1
+309	val_309	1
+310	val_310	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+315	val_315	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+317	val_317	1
+317	val_317	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+321	val_321	1
+321	val_321	1
+322	val_322	1
+322	val_322	1
+323	val_323	1
+325	val_325	1
+325	val_325	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+331	val_331	1
+331	val_331	1
+332	val_332	1
+333	val_333	1
+333	val_333	1
+335	val_335	1
+336	val_336	1
+338	val_338	1
+339	val_339	1
+341	val_341	1
+342	val_342	1
+342	val_342	1
+344	val_344	1
+344	val_344	1
+345	val_345	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+351	val_351	1
+353	val_353	1
+353	val_353	1
+356	val_356	1
+360	val_360	1
+362	val_362	1
+364	val_364	1
+365	val_365	1
+366	val_366	1
+367	val_367	1
+367	val_367	1
+368	val_368	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+373	val_373	1
+374	val_374	1
+375	val_375	1
+377	val_377	1
+378	val_378	1
+379	val_379	1
+382	val_382	1
+382	val_382	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+386	val_386	1
+389	val_389	1
+392	val_392	1
+393	val_393	1
+394	val_394	1
+395	val_395	1
+395	val_395	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+397	val_397	1
+397	val_397	1
+399	val_399	1
+399	val_399	1
+400	val_400	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+402	val_402	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+404	val_404	1
+404	val_404	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+407	val_407	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+411	val_411	1
+413	val_413	1
+413	val_413	1
+414	val_414	1
+414	val_414	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+418	val_418	1
+419	val_419	1
+421	val_421	1
+424	val_424	1
+424	val_424	1
+427	val_427	1
+429	val_429	1
+429	val_429	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+432	val_432	1
+435	val_435	1
+436	val_436	1
+437	val_437	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+439	val_439	1
+439	val_439	1
+443	val_443	1
+444	val_444	1
+446	val_446	1
+448	val_448	1
+449	val_449	1
+452	val_452	1
+453	val_453	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+455	val_455	1
+457	val_457	1
+458	val_458	1
+458	val_458	1
+459	val_459	1
+459	val_459	1
+460	val_460	1
+462	val_462	1
+462	val_462	1
+463	val_463	1
+463	val_463	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+467	val_467	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+470	val_470	1
+472	val_472	1
+475	val_475	1
+477	val_477	1
+478	val_478	1
+478	val_478	1
+479	val_479	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+481	val_481	1
+482	val_482	1
+483	val_483	1
+484	val_484	1
+485	val_485	1
+487	val_487	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+490	val_490	1
+491	val_491	1
+492	val_492	1
+492	val_492	1
+493	val_493	1
+494	val_494	1
+495	val_495	1
+496	val_496	1
+497	val_497	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+PREHOOK: query: SELECT * FROM test_table3 ORDER BY key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM test_table3 ORDER BY key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+0	val_0	1
+2	val_2	1
+4	val_4	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+5	val_5	1
+8	val_8	1
+9	val_9	1
+10	val_10	1
+11	val_11	1
+12	val_12	1
+12	val_12	1
+12	val_12	1
+12	val_12	1
+15	val_15	1
+15	val_15	1
+15	val_15	1
+15	val_15	1
+17	val_17	1
+18	val_18	1
+18	val_18	1
+18	val_18	1
+18	val_18	1
+19	val_19	1
+20	val_20	1
+24	val_24	1
+24	val_24	1
+24	val_24	1
+24	val_24	1
+26	val_26	1
+26	val_26	1
+26	val_26	1
+26	val_26	1
+27	val_27	1
+28	val_28	1
+30	val_30	1
+33	val_33	1
+34	val_34	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+35	val_35	1
+37	val_37	1
+37	val_37	1
+37	val_37	1
+37	val_37	1
+41	val_41	1
+42	val_42	1
+42	val_42	1
+42	val_42	1
+42	val_42	1
+43	val_43	1
+44	val_44	1
+47	val_47	1
+51	val_51	1
+51	val_51	1
+51	val_51	1
+51	val_51	1
+53	val_53	1
+54	val_54	1
+57	val_57	1
+58	val_58	1
+58	val_58	1
+58	val_58	1
+58	val_58	1
+64	val_64	1
+65	val_65	1
+66	val_66	1
+67	val_67	1
+67	val_67	1
+67	val_67	1
+67	val_67	1
+69	val_69	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+70	val_70	1
+72	val_72	1
+72	val_72	1
+72	val_72	1
+72	val_72	1
+74	val_74	1
+76	val_76	1
+76	val_76	1
+76	val_76	1
+76	val_76	1
+77	val_77	1
+78	val_78	1
+80	val_80	1
+82	val_82	1
+83	val_83	1
+83	val_83	1
+83	val_83	1
+83	val_83	1
+84	val_84	1
+84	val_84	1
+84	val_84	1
+84	val_84	1
+85	val_85	1
+86	val_86	1
+87	val_87	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+90	val_90	1
+92	val_92	1
+95	val_95	1
+95	val_95	1
+95	val_95	1
+95	val_95	1
+96	val_96	1
+97	val_97	1
+97	val_97	1
+97	val_97	1
+97	val_97	1
+98	val_98	1
+98	val_98	1
+98	val_98	1
+98	val_98	1
+100	val_100	1
+100	val_100	1
+100	val_100	1
+100	val_100	1
+103	val_103	1
+103	val_103	1
+103	val_103	1
+103	val_103	1
+104	val_104	1
+104	val_104	1
+104	val_104	1
+104	val_104	1
+105	val_105	1
+111	val_111	1
+113	val_113	1
+113	val_113	1
+113	val_113	1
+113	val_113	1
+114	val_114	1
+116	val_116	1
+118	val_118	1
+118	val_118	1
+118	val_118	1
+118	val_118	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+119	val_119	1
+120	val_120	1
+120	val_120	1
+120	val_120	1
+120	val_120	1
+125	val_125	1
+125	val_125	1
+125	val_125	1
+125	val_125	1
+126	val_126	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+128	val_128	1
+129	val_129	1
+129	val_129	1
+129	val_129	1
+129	val_129	1
+131	val_131	1
+133	val_133	1
+134	val_134	1
+134	val_134	1
+134	val_134	1
+134	val_134	1
+136	val_136	1
+137	val_137	1
+137	val_137	1
+137	val_137	1
+137	val_137	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+138	val_138	1
+143	val_143	1
+145	val_145	1
+146	val_146	1
+146	val_146	1
+146	val_146	1
+146	val_146	1
+149	val_149	1
+149	val_149	1
+149	val_149	1
+149	val_149	1
+150	val_150	1
+152	val_152	1
+152	val_152	1
+152	val_152	1
+152	val_152	1
+153	val_153	1
+155	val_155	1
+156	val_156	1
+157	val_157	1
+158	val_158	1
+160	val_160	1
+162	val_162	1
+163	val_163	1
+164	val_164	1
+164	val_164	1
+164	val_164	1
+164	val_164	1
+165	val_165	1
+165	val_165	1
+165	val_165	1
+165	val_165	1
+166	val_166	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+167	val_167	1
+168	val_168	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+169	val_169	1
+170	val_170	1
+172	val_172	1
+172	val_172	1
+172	val_172	1
+172	val_172	1
+174	val_174	1
+174	val_174	1
+174	val_174	1
+174	val_174	1
+175	val_175	1
+175	val_175	1
+175	val_175	1
+175	val_175	1
+176	val_176	1
+176	val_176	1
+176	val_176	1
+176	val_176	1
+177	val_177	1
+178	val_178	1
+179	val_179	1
+179	val_179	1
+179	val_179	1
+179	val_179	1
+180	val_180	1
+181	val_181	1
+183	val_183	1
+186	val_186	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+187	val_187	1
+189	val_189	1
+190	val_190	1
+191	val_191	1
+191	val_191	1
+191	val_191	1
+191	val_191	1
+192	val_192	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+194	val_194	1
+195	val_195	1
+195	val_195	1
+195	val_195	1
+195	val_195	1
+196	val_196	1
+197	val_197	1
+197	val_197	1
+197	val_197	1
+197	val_197	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+199	val_199	1
+200	val_200	1
+200	val_200	1
+200	val_200	1
+200	val_200	1
+201	val_201	1
+202	val_202	1
+203	val_203	1
+203	val_203	1
+203	val_203	1
+203	val_203	1
+205	val_205	1
+205	val_205	1
+205	val_205	1
+205	val_205	1
+207	val_207	1
+207	val_207	1
+207	val_207	1
+207	val_207	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+208	val_208	1
+209	val_209	1
+209	val_209	1
+209	val_209	1
+209	val_209	1
+213	val_213	1
+213	val_213	1
+213	val_213	1
+213	val_213	1
+214	val_214	1
+216	val_216	1
+216	val_216	1
+216	val_216	1
+216	val_216	1
+217	val_217	1
+217	val_217	1
+217	val_217	1
+217	val_217	1
+218	val_218	1
+219	val_219	1
+219	val_219	1
+219	val_219	1
+219	val_219	1
+221	val_221	1
+221	val_221	1
+221	val_221	1
+221	val_221	1
+222	val_222	1
+223	val_223	1
+223	val_223	1
+223	val_223	1
+223	val_223	1
+224	val_224	1
+224	val_224	1
+224	val_224	1
+224	val_224	1
+226	val_226	1
+228	val_228	1
+229	val_229	1
+229	val_229	1
+229	val_229	1
+229	val_229	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+230	val_230	1
+233	val_233	1
+233	val_233	1
+233	val_233	1
+233	val_233	1
+235	val_235	1
+237	val_237	1
+237	val_237	1
+237	val_237	1
+237	val_237	1
+238	val_238	1
+238	val_238	1
+238	val_238	1
+238	val_238	1
+239	val_239	1
+239	val_239	1
+239	val_239	1
+239	val_239	1
+241	val_241	1
+242	val_242	1
+242	val_242	1
+242	val_242	1
+242	val_242	1
+244	val_244	1
+247	val_247	1
+248	val_248	1
+249	val_249	1
+252	val_252	1
+255	val_255	1
+255	val_255	1
+255	val_255	1
+255	val_255	1
+256	val_256	1
+256	val_256	1
+256	val_256	1
+256	val_256	1
+257	val_257	1
+258	val_258	1
+260	val_260	1
+262	val_262	1
+263	val_263	1
+265	val_265	1
+265	val_265	1
+265	val_265	1
+265	val_265	1
+266	val_266	1
+272	val_272	1
+272	val_272	1
+272	val_272	1
+272	val_272	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+274	val_274	1
+275	val_275	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+277	val_277	1
+278	val_278	1
+278	val_278	1
+278	val_278	1
+278	val_278	1
+280	val_280	1
+280	val_280	1
+280	val_280	1
+280	val_280	1
+281	val_281	1
+281	val_281	1
+281	val_281	1
+281	val_281	1
+282	val_282	1
+282	val_282	1
+282	val_282	1
+282	val_282	1
+283	val_283	1
+284	val_284	1
+285	val_285	1
+286	val_286	1
+287	val_287	1
+288	val_288	1
+288	val_288	1
+288	val_288	1
+288	val_288	1
+289	val_289	1
+291	val_291	1
+292	val_292	1
+296	val_296	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+298	val_298	1
+302	val_302	1
+305	val_305	1
+306	val_306	1
+307	val_307	1
+307	val_307	1
+307	val_307	1
+307	val_307	1
+308	val_308	1
+309	val_309	1
+309	val_309	1
+309	val_309	1
+309	val_309	1
+310	val_310	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+311	val_311	1
+315	val_315	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+316	val_316	1
+317	val_317	1
+317	val_317	1
+317	val_317	1
+317	val_317	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+318	val_318	1
+321	val_321	1
+321	val_321	1
+321	val_321	1
+321	val_321	1
+322	val_322	1
+322	val_322	1
+322	val_322	1
+322	val_322	1
+323	val_323	1
+325	val_325	1
+325	val_325	1
+325	val_325	1
+325	val_325	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+327	val_327	1
+331	val_331	1
+331	val_331	1
+331	val_331	1
+331	val_331	1
+332	val_332	1
+333	val_333	1
+333	val_333	1
+333	val_333	1
+333	val_333	1
+335	val_335	1
+336	val_336	1
+338	val_338	1
+339	val_339	1
+341	val_341	1
+342	val_342	1
+342	val_342	1
+342	val_342	1
+342	val_342	1
+344	val_344	1
+344	val_344	1
+344	val_344	1
+344	val_344	1
+345	val_345	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+348	val_348	1
+351	val_351	1
+353	val_353	1
+353	val_353	1
+353	val_353	1
+353	val_353	1
+356	val_356	1
+360	val_360	1
+362	val_362	1
+364	val_364	1
+365	val_365	1
+366	val_366	1
+367	val_367	1
+367	val_367	1
+367	val_367	1
+367	val_367	1
+368	val_368	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+373	val_373	1
+374	val_374	1
+375	val_375	1
+377	val_377	1
+378	val_378	1
+379	val_379	1
+382	val_382	1
+382	val_382	1
+382	val_382	1
+382	val_382	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+384	val_384	1
+386	val_386	1
+389	val_389	1
+392	val_392	1
+393	val_393	1
+394	val_394	1
+395	val_395	1
+395	val_395	1
+395	val_395	1
+395	val_395	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+396	val_396	1
+397	val_397	1
+397	val_397	1
+397	val_397	1
+397	val_397	1
+399	val_399	1
+399	val_399	1
+399	val_399	1
+399	val_399	1
+400	val_400	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+402	val_402	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+403	val_403	1
+404	val_404	1
+404	val_404	1
+404	val_404	1
+404	val_404	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+406	val_406	1
+407	val_407	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+409	val_409	1
+411	val_411	1
+413	val_413	1
+413	val_413	1
+413	val_413	1
+413	val_413	1
+414	val_414	1
+414	val_414	1
+414	val_414	1
+414	val_414	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+418	val_418	1
+419	val_419	1
+421	val_421	1
+424	val_424	1
+424	val_424	1
+424	val_424	1
+424	val_424	1
+427	val_427	1
+429	val_429	1
+429	val_429	1
+429	val_429	1
+429	val_429	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+430	val_430	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+431	val_431	1
+432	val_432	1
+435	val_435	1
+436	val_436	1
+437	val_437	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+438	val_438	1
+439	val_439	1
+439	val_439	1
+439	val_439	1
+439	val_439	1
+443	val_443	1
+444	val_444	1
+446	val_446	1
+448	val_448	1
+449	val_449	1
+452	val_452	1
+453	val_453	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+454	val_454	1
+455	val_455	1
+457	val_457	1
+458	val_458	1
+458	val_458	1
+458	val_458	1
+458	val_458	1
+459	val_459	1
+459	val_459	1
+459	val_459	1
+459	val_459	1
+460	val_460	1
+462	val_462	1
+462	val_462	1
+462	val_462	1
+462	val_462	1
+463	val_463	1
+463	val_463	1
+463	val_463	1
+463	val_463	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+466	val_466	1
+467	val_467	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+468	val_468	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+469	val_469	1
+470	val_470	1
+472	val_472	1
+475	val_475	1
+477	val_477	1
+478	val_478	1
+478	val_478	1
+478	val_478	1
+478	val_478	1
+479	val_479	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+480	val_480	1
+481	val_481	1
+482	val_482	1
+483	val_483	1
+484	val_484	1
+485	val_485	1
+487	val_487	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+489	val_489	1
+490	val_490	1
+491	val_491	1
+492	val_492	1
+492	val_492	1
+492	val_492	1
+492	val_492	1
+493	val_493	1
+494	val_494	1
+495	val_495	1
+496	val_496	1
+497	val_497	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+498	val_498	1
+PREHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_TABREF
+         TOK_TABNAME
+            test_table1
+         TOK_TABLEBUCKETSAMPLE
+            2
+            16
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_SELEXPR
+            TOK_ALLCOLREF
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: test_table1
+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: true
+              predicate: (((hash(key) & 2147483647) % 16) = 1) (type: boolean)
+              sampleDesc: BUCKET 2 OUT OF 16
+              Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: key (type: int), value (type: string), ds (type: string)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+#### A masked pattern was here ####
+                  NumFilesPerFileSink: 1
+                  Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      properties:
+                        columns _col0,_col1,_col2
+                        columns.types int:string:string
+                        escape.delim \
+                        hive.serialization.extend.additional.nesting.levels true
+                        serialization.format 1
+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  TotalFiles: 1
+                  GatherStats: false
+                  MultiFileSpray: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: 000001_0
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table1
+              numFiles 16
+              numRows 0
+              partition_columns ds
+              partition_columns.types string
+              rawDataSize 0
+              serialization.ddl struct test_table1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table1
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table1
+            name: default.test_table1
+      Truncated Path -> Alias:
+        /test_table1/ds=1/000001_0 [test_table1]
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_TABREF
+         TOK_TABNAME
+            test_table3
+         TOK_TABLEBUCKETSAMPLE
+            2
+            16
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_SELEXPR
+            TOK_ALLCOLREF
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: test_table3
+            Statistics: Num rows: 1028 Data size: 10968 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: true
+              predicate: (((hash(key) & 2147483647) % 16) = 1) (type: boolean)
+              sampleDesc: BUCKET 2 OUT OF 16
+              Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE
+              Select Operator
+                expressions: key (type: int), value (type: string), ds (type: string)
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  GlobalTableId: 0
+#### A masked pattern was here ####
+                  NumFilesPerFileSink: 1
+                  Statistics: Num rows: 514 Data size: 5484 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      properties:
+                        columns _col0,_col1,_col2
+                        columns.types int:string:string
+                        escape.delim \
+                        hive.serialization.extend.additional.nesting.levels true
+                        serialization.format 1
+                        serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  TotalFiles: 1
+                  GatherStats: false
+                  MultiFileSpray: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: (ds%3D1)000001_0
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table3
+              numFiles 16
+              numRows 1028
+              partition_columns ds
+              partition_columns.types string
+              rawDataSize 10968
+              serialization.ddl struct test_table3 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 11996
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+            name: default.test_table3
+      Truncated Path -> Alias:
+        /test_table3/ds=1/(ds%3D1)000001_0 [test_table3]
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+#### A masked pattern was here ####
+17	val_17	1
+33	val_33	1
+65	val_65	1
+97	val_97	1
+97	val_97	1
+113	val_113	1
+113	val_113	1
+129	val_129	1
+129	val_129	1
+145	val_145	1
+177	val_177	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+209	val_209	1
+209	val_209	1
+241	val_241	1
+257	val_257	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+289	val_289	1
+305	val_305	1
+321	val_321	1
+321	val_321	1
+353	val_353	1
+353	val_353	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+449	val_449	1
+481	val_481	1
+497	val_497	1
+PREHOOK: query: SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16)
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+17	val_17	1
+33	val_33	1
+65	val_65	1
+97	val_97	1
+97	val_97	1
+97	val_97	1
+97	val_97	1
+113	val_113	1
+113	val_113	1
+113	val_113	1
+113	val_113	1
+129	val_129	1
+129	val_129	1
+129	val_129	1
+129	val_129	1
+145	val_145	1
+177	val_177	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+193	val_193	1
+209	val_209	1
+209	val_209	1
+209	val_209	1
+209	val_209	1
+241	val_241	1
+257	val_257	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+273	val_273	1
+289	val_289	1
+305	val_305	1
+321	val_321	1
+321	val_321	1
+321	val_321	1
+321	val_321	1
+353	val_353	1
+353	val_353	1
+353	val_353	1
+353	val_353	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+369	val_369	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+401	val_401	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+417	val_417	1
+449	val_449	1
+481	val_481	1
+497	val_497	1
+PREHOOK: query: SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+293
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
index 6a63ae2..d6663c0 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
@@ -1,946 +1,552 @@
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  set hive.enforce.bucketing=true;
-No rows affected 
->>>  set hive.enforce.sorting=true;
-No rows affected 
->>>  set hive.exec.reducers.max = 1;
-No rows affected 
->>>  set hive.merge.mapfiles=false;
-No rows affected 
->>>  set hive.merge.mapredfiles=false; 
-No rows affected 
->>>  
->>>  >>>  
->>>  >>>  CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_12
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_12
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_12
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_12
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  
->>>  FROM default.src
-INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3') SELECT *;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): FROM default.src
-INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT *
-INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3') SELECT *
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:int, comment:null), FieldSchema(name:_col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): FROM default.src
+PREHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table1
+POSTHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table1
+PREHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table2
+POSTHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table2
+PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3') SELECT *
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: default@src
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table1@ds=1
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table2@ds=1
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table2@ds=2
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table2@ds=3
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 4
-INFO  : Launching Job 1 out of 4
-INFO  : Starting task [Stage-4:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table1 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Launching Job 2 out of 4
-INFO  : Starting task [Stage-6:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 3 out of 4
-INFO  : Starting task [Stage-8:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 4 out of 4
-INFO  : Starting task [Stage-10:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-5:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table1{ds=1} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-1:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table2 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table2 partition (ds=2) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-3:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table2 partition (ds=3) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-7:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table2{ds=1} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-9:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table2{ds=2} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-11:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table2{ds=3} stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: default@src
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table1@ds=1
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table2@ds=1
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table2@ds=2
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table2@ds=3
-ERROR : POSTHOOK: Lineage: test_table1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2 PARTITION(ds=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-4:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-6:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-8:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-10:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query FROM default.src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@test_table1@ds=1
+PREHOOK: Output: default@test_table2@ds=1
+PREHOOK: Output: default@test_table2@ds=2
+PREHOOK: Output: default@test_table2@ds=3
+POSTHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '1') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '2') SELECT *
 INSERT OVERWRITE TABLE test_table2 PARTITION (ds = '3') SELECT *
-No rows affected 
->>>  
->>>  set hive.enforce.bucketing=false;
-No rows affected 
->>>  set hive.enforce.sorting=false;
-No rows affected 
->>>  
->>>  >>>  CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_12
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_12
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
-No rows affected 
->>>  
->>>  >>>  EXPLAIN EXTENDED
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@test_table1@ds=1
+POSTHOOK: Output: default@test_table2@ds=1
+POSTHOOK: Output: default@test_table2@ds=2
+POSTHOOK: Output: default@test_table2@ds=3
+POSTHOOK: Lineage: test_table1 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table1 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=1).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=1).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=2).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=2).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=3).key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2 PARTITION(ds=3).value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table3
+POSTHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) PARTITIONED BY (ds STRING) CLUSTERED BY (key) SORTED BY (key) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table3
+PREHOOK: query: EXPLAIN EXTENDED
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-4:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_JOIN'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table1'
-'            a'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table2'
-'            b'
-'         AND'
-'            AND'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     key'
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        b'
-'                     key'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     ds'
-'                  '1''
-'            >='
-'               .'
-'                  TOK_TABLE_OR_COL'
-'                     b'
-'                  ds'
-'               '1''
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_TAB'
-'            TOK_TABNAME'
-'               test_table3'
-'            TOK_PARTSPEC'
-'               TOK_PARTVAL'
-'                  ds'
-'                  '1''
-'      TOK_SELECT'
-'         TOK_HINTLIST'
-'            TOK_HINT'
-'               TOK_MAPJOIN'
-'               TOK_HINTARGLIST'
-'                  b'
-'         TOK_SELEXPR'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'         TOK_SELEXPR'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  b'
-'               value'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-'  Stage-2 depends on stages: Stage-0'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col7'
-'                Position of Big Table: 0'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col7 (type: string)'
-'                  outputColumnNames: _col0, _col1'
-'                  File Output Operator'
-'                    compressed: false'
-'                    GlobalTableId: 1'
-'                    directory: file:/!!ELIDED!!
-'                    NumFilesPerFileSink: 1'
-'                    Static Partition Specification: ds=1/'
-'                    Stats Publishing Key Prefix: file:/!!ELIDED!!
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        properties:'
-'                          SORTBUCKETCOLSPREFIX TRUE'
-'                          bucket_count 16'
-'                          bucket_field_name key'
-'                          columns key,value'
-'                          columns.comments '
-'                          columns.types int:string'
-'                          file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                          file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                          location file:/!!ELIDED!!
-'                          name smb_mapjoin_12.test_table3'
-'                          partition_columns ds'
-'                          partition_columns.types string'
-'                          serialization.ddl struct test_table3 { i32 key, string value}'
-'                          serialization.format 1'
-'                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                          transient_lastDdlTime !!UNIXTIME!!'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                        name: smb_mapjoin_12.test_table3'
-'                    TotalFiles: 1'
-'                    GatherStats: true'
-'                    MultiFileSpray: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [a]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: ds=1'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            partition values:'
-'              ds 1'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_12.test_table1'
-'              numFiles 16'
-'              numRows 0'
-'              partition_columns ds'
-'              partition_columns.types string'
-'              rawDataSize 0'
-'              serialization.ddl struct test_table1 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 5812'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_12.test_table1'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table1 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_12.test_table1'
-'            name: smb_mapjoin_12.test_table1'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_12.db/test_table1/ds=1 [a]'
-''
-'  Stage: Stage-0'
-'    Move Operator'
-'      tables:'
-'          partition:'
-'            ds 1'
-'          replace: true'
-'          source: file:/!!ELIDED!!
-'          table:'
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_12.test_table3'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_12.test_table3'
-''
-'  Stage: Stage-2'
-'    Stats-Aggr Operator'
-'      Stats Aggregation Key Prefix: file:/!!ELIDED!!
-''
-215 rows selected 
->>>  
->>>  INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table2
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table2@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table2@ds=2
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table2@ds=3
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table3@ds=1
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table3 partition (ds=1) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table3{ds=1} stats: [numFiles=16, numRows=3084, totalSize=35988, rawDataSize=32904]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table2
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table2@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table2@ds=2
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table2@ds=3
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table3@ds=1
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=1).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value SIMPLE [(test_table2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
-No rows affected 
->>>  
->>>  >>>  SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3@ds=1
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 2
-INFO  : Launching Job 1 out of 2
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 2 out of 2
-INFO  : Starting task [Stage-2:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3@ds=1
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-'_c0'
-'879'
-1 row selected 
->>>  
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  
->>>  >>>  explain extended
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
-SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain extended
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_JOIN
+         TOK_TABREF
+            TOK_TABNAME
+               test_table1
+            a
+         TOK_TABREF
+            TOK_TABNAME
+               test_table2
+            b
+         AND
+            AND
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     key
+                  .
+                     TOK_TABLE_OR_COL
+                        b
+                     key
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     ds
+                  '1'
+            >=
+               .
+                  TOK_TABLE_OR_COL
+                     b
+                  ds
+               '1'
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_TAB
+            TOK_TABNAME
+               test_table3
+            TOK_PARTSPEC
+               TOK_PARTVAL
+                  ds
+                  '1'
+      TOK_SELECT
+         TOK_HINTLIST
+            TOK_HINT
+               TOK_MAPJOIN
+               TOK_HINTARGLIST
+                  b
+         TOK_SELEXPR
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+         TOK_SELEXPR
+            .
+               TOK_TABLE_OR_COL
+                  b
+               value
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col7
+                Position of Big Table: 0
+                Select Operator
+                  expressions: _col0 (type: int), _col7 (type: string)
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 1
+#### A masked pattern was here ####
+                    NumFilesPerFileSink: 1
+                    Static Partition Specification: ds=1/
+#### A masked pattern was here ####
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        properties:
+                          SORTBUCKETCOLSPREFIX TRUE
+                          bucket_count 16
+                          bucket_field_name key
+                          columns key,value
+                          columns.comments 
+                          columns.types int:string
+#### A masked pattern was here ####
+                          name default.test_table3
+                          partition_columns ds
+                          partition_columns.types string
+                          serialization.ddl struct test_table3 { i32 key, string value}
+                          serialization.format 1
+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.test_table3
+                    TotalFiles: 1
+                    GatherStats: true
+                    MultiFileSpray: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds=1
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table1
+              numFiles 16
+              numRows 0
+              partition_columns ds
+              partition_columns.types string
+              rawDataSize 0
+              serialization.ddl struct test_table1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table1
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table1
+            name: default.test_table1
+      Truncated Path -> Alias:
+        /test_table1/ds=1 [a]
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ds 1
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+PREHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+PREHOOK: Input: default@test_table2
+PREHOOK: Input: default@test_table2@ds=1
+PREHOOK: Input: default@test_table2@ds=2
+PREHOOK: Input: default@test_table2@ds=3
+PREHOOK: Output: default@test_table3@ds=1
+POSTHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '1') SELECT /*+ MAPJOIN(b) */ a.key, b.value FROM test_table1 a JOIN test_table2 b ON a.key = b.key AND a.ds = '1' AND b.ds >= '1'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+POSTHOOK: Input: default@test_table2
+POSTHOOK: Input: default@test_table2@ds=1
+POSTHOOK: Input: default@test_table2@ds=2
+POSTHOOK: Input: default@test_table2@ds=3
+POSTHOOK: Output: default@test_table3@ds=1
+POSTHOOK: Lineage: test_table3 PARTITION(ds=1).key SIMPLE [(test_table1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: test_table3 PARTITION(ds=1).value SIMPLE [(test_table2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT COUNT(*) FROM test_table3 TABLESAMPLE(BUCKET 2 OUT OF 16) a JOIN test_table1 TABLESAMPLE(BUCKET 2 OUT OF 16) b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=1
+#### A masked pattern was here ####
+879
+PREHOOK: query: explain extended
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
 SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain extended
+PREHOOK: type: QUERY
+POSTHOOK: query: explain extended
 INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
 SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-4:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain extended
-INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
-SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_JOIN'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table3'
-'            a'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table1'
-'            b'
-'         AND'
-'            AND'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     key'
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        b'
-'                     key'
-'               ='
-'                  .'
-'                     TOK_TABLE_OR_COL'
-'                        a'
-'                     ds'
-'                  '1''
-'            ='
-'               .'
-'                  TOK_TABLE_OR_COL'
-'                     b'
-'                  ds'
-'               '1''
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_TAB'
-'            TOK_TABNAME'
-'               test_table3'
-'            TOK_PARTSPEC'
-'               TOK_PARTVAL'
-'                  ds'
-'                  '2''
-'      TOK_SELECT'
-'         TOK_HINTLIST'
-'            TOK_HINT'
-'               TOK_MAPJOIN'
-'               TOK_HINTARGLIST'
-'                  b'
-'         TOK_SELEXPR'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'         TOK_SELEXPR'
-'            TOK_FUNCTION'
-'               concat'
-'               .'
-'                  TOK_TABLE_OR_COL'
-'                     a'
-'                  value'
-'               .'
-'                  TOK_TABLE_OR_COL'
-'                     b'
-'                  value'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-'  Stage-2 depends on stages: Stage-0'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 3084 Data size: 32904 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1542 Data size: 16452 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col7'
-'                Position of Big Table: 0'
-'                Select Operator'
-'                  expressions: _col0 (type: int), concat(_col1, _col7) (type: string)'
-'                  outputColumnNames: _col0, _col1'
-'                  File Output Operator'
-'                    compressed: false'
-'                    GlobalTableId: 1'
-'                    directory: file:/!!ELIDED!!
-'                    NumFilesPerFileSink: 1'
-'                    Static Partition Specification: ds=2/'
-'                    Stats Publishing Key Prefix: file:/!!ELIDED!!
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        properties:'
-'                          SORTBUCKETCOLSPREFIX TRUE'
-'                          bucket_count 16'
-'                          bucket_field_name key'
-'                          columns key,value'
-'                          columns.comments '
-'                          columns.types int:string'
-'                          file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                          file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                          location file:/!!ELIDED!!
-'                          name smb_mapjoin_12.test_table3'
-'                          partition_columns ds'
-'                          partition_columns.types string'
-'                          serialization.ddl struct test_table3 { i32 key, string value}'
-'                          serialization.format 1'
-'                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                          transient_lastDdlTime !!UNIXTIME!!'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                        name: smb_mapjoin_12.test_table3'
-'                    TotalFiles: 1'
-'                    GatherStats: true'
-'                    MultiFileSpray: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [a]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: ds=1'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            partition values:'
-'              ds 1'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_12.test_table3'
-'              numFiles 16'
-'              numRows 3084'
-'              partition_columns ds'
-'              partition_columns.types string'
-'              rawDataSize 32904'
-'              serialization.ddl struct test_table3 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 35988'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_12.test_table3'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_12.test_table3'
-'            name: smb_mapjoin_12.test_table3'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_12.db/test_table3/ds=1 [a]'
-''
-'  Stage: Stage-0'
-'    Move Operator'
-'      tables:'
-'          partition:'
-'            ds 2'
-'          replace: true'
-'          source: file:/!!ELIDED!!
-'          table:'
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_12.test_table3'
-'                partition_columns ds'
-'                partition_columns.types string'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_12.test_table3'
-''
-'  Stage: Stage-2'
-'    Stats-Aggr Operator'
-'      Stats Aggregation Key Prefix: file:/!!ELIDED!!
-''
-221 rows selected 
->>>  
->>>  INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
-SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
-SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:_c2, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_JOIN
+         TOK_TABREF
+            TOK_TABNAME
+               test_table3
+            a
+         TOK_TABREF
+            TOK_TABNAME
+               test_table1
+            b
+         AND
+            AND
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     key
+                  .
+                     TOK_TABLE_OR_COL
+                        b
+                     key
+               =
+                  .
+                     TOK_TABLE_OR_COL
+                        a
+                     ds
+                  '1'
+            =
+               .
+                  TOK_TABLE_OR_COL
+                     b
+                  ds
+               '1'
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_TAB
+            TOK_TABNAME
+               test_table3
+            TOK_PARTSPEC
+               TOK_PARTVAL
+                  ds
+                  '2'
+      TOK_SELECT
+         TOK_HINTLIST
+            TOK_HINT
+               TOK_MAPJOIN
+               TOK_HINTARGLIST
+                  b
+         TOK_SELEXPR
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+         TOK_SELEXPR
+            TOK_FUNCTION
+               concat
+               .
+                  TOK_TABLE_OR_COL
+                     a
+                  value
+               .
+                  TOK_TABLE_OR_COL
+                     b
+                  value
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+  Stage-2 depends on stages: Stage-0
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 3084 Data size: 32904 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1542 Data size: 16452 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col7
+                Position of Big Table: 0
+                Select Operator
+                  expressions: _col0 (type: int), concat(_col1, _col7) (type: string)
+                  outputColumnNames: _col0, _col1
+                  File Output Operator
+                    compressed: false
+                    GlobalTableId: 1
+#### A masked pattern was here ####
+                    NumFilesPerFileSink: 1
+                    Static Partition Specification: ds=2/
+#### A masked pattern was here ####
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        properties:
+                          SORTBUCKETCOLSPREFIX TRUE
+                          bucket_count 16
+                          bucket_field_name key
+                          columns key,value
+                          columns.comments 
+                          columns.types int:string
+#### A masked pattern was here ####
+                          name default.test_table3
+                          partition_columns ds
+                          partition_columns.types string
+                          serialization.ddl struct test_table3 { i32 key, string value}
+                          serialization.format 1
+                          serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                        name: default.test_table3
+                    TotalFiles: 1
+                    GatherStats: true
+                    MultiFileSpray: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: ds=1
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            partition values:
+              ds 1
+            properties:
+              COLUMN_STATS_ACCURATE true
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table3
+              numFiles 16
+              numRows 3084
+              partition_columns ds
+              partition_columns.types string
+              rawDataSize 32904
+              serialization.ddl struct test_table3 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 35988
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+            name: default.test_table3
+      Truncated Path -> Alias:
+        /test_table3/ds=1 [a]
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          partition:
+            ds 2
+          replace: true
+#### A masked pattern was here ####
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                partition_columns ds
+                partition_columns.types string
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+
+  Stage: Stage-2
+    Stats-Aggr Operator
+#### A masked pattern was here ####
+
+PREHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
 SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3@ds=1
-ERROR : PREHOOK: Output: smb_mapjoin_12@test_table3@ds=2
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_12.test_table3 partition (ds=2) from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Partition smb_mapjoin_12.test_table3{ds=2} stats: [numFiles=16, numRows=7962, totalSize=148002, rawDataSize=140040]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table1@ds=1
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3@ds=1
-ERROR : POSTHOOK: Output: smb_mapjoin_12@test_table3@ds=2
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=2).key SIMPLE [(test_table3)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: test_table3 PARTITION(ds=2).value EXPRESSION [(test_table3)a.FieldSchema(name:value, type:string, comment:null), (test_table1)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table1@ds=1
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=1
+PREHOOK: Output: default@test_table3@ds=2
+POSTHOOK: query: INSERT OVERWRITE TABLE test_table3 PARTITION (ds = '2') 
 SELECT /*+mapjoin(b)*/ a.key, concat(a.value, b.value) FROM test_table3 a JOIN test_table1 b ON a.key = b.key AND a.ds = '1' AND b.ds='1'
-No rows affected 
->>>  
->>>  SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2';
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2'
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c0, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2'
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_12@test_table3@ds=2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_12@test_table3@ds=2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2'
-'_c0'
-'879'
-1 row selected 
->>>  !record
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table1@ds=1
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=1
+POSTHOOK: Output: default@test_table3@ds=2
+POSTHOOK: Lineage: test_table3 PARTITION(ds=2).key SIMPLE [(test_table3)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: test_table3 PARTITION(ds=2).value EXPRESSION [(test_table3)a.FieldSchema(name:value, type:string, comment:null), (test_table1)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2'
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table3@ds=2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT count(*) from test_table3 tablesample (bucket 2 out of 16) a where ds = '2'
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table3@ds=2
+#### A masked pattern was here ####
+879
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
index cb684cb..6833e26 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
@@ -1,764 +1,463 @@
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  set hive.enforce.bucketing=true;
-No rows affected 
->>>  set hive.enforce.sorting=true;
-No rows affected 
->>>  set hive.exec.reducers.max = 1;
-No rows affected 
->>>  set hive.merge.mapfiles=false;
-No rows affected 
->>>  set hive.merge.mapredfiles=false; 
-No rows affected 
->>>  
->>>  >>>  
->>>  >>>  CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_13
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_13
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_13
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_13
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_13
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_13
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_13
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table4
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_13
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table4
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS
-No rows affected 
->>>  
->>>  FROM default.src
-INSERT OVERWRITE TABLE test_table1 SELECT *
-INSERT OVERWRITE TABLE test_table2 SELECT *
-INSERT OVERWRITE TABLE test_table3 SELECT *
-INSERT OVERWRITE TABLE test_table4 SELECT *;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): FROM default.src
+PREHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table1
+POSTHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key ASC) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table1
+PREHOOK: query: CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table2
+POSTHOOK: query: CREATE TABLE test_table2 (value INT, key STRING) CLUSTERED BY (value) SORTED BY (value ASC) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table2
+PREHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table3
+POSTHOOK: query: CREATE TABLE test_table3 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (key ASC, value ASC) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table3
+PREHOOK: query: CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table4
+POSTHOOK: query: CREATE TABLE test_table4 (key INT, value STRING) CLUSTERED BY (key, value) SORTED BY (value ASC, key ASC) INTO 16 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table4
+PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 SELECT *
 INSERT OVERWRITE TABLE test_table2 SELECT *
 INSERT OVERWRITE TABLE test_table3 SELECT *
 INSERT OVERWRITE TABLE test_table4 SELECT *
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:int, comment:null), FieldSchema(name:_col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): FROM default.src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@test_table1
+PREHOOK: Output: default@test_table2
+PREHOOK: Output: default@test_table3
+PREHOOK: Output: default@test_table4
+POSTHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 SELECT *
 INSERT OVERWRITE TABLE test_table2 SELECT *
 INSERT OVERWRITE TABLE test_table3 SELECT *
 INSERT OVERWRITE TABLE test_table4 SELECT *
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: default@src
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table1
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table2
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table3
-ERROR : PREHOOK: Output: smb_mapjoin_13@test_table4
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 4
-INFO  : Launching Job 1 out of 4
-INFO  : Starting task [Stage-4:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_13.test_table1 from file:/!!ELIDED!!
-INFO  : Launching Job 2 out of 4
-INFO  : Starting task [Stage-6:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 3 out of 4
-INFO  : Starting task [Stage-8:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Launching Job 4 out of 4
-INFO  : Starting task [Stage-10:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-5:STATS] in serial mode
-INFO  : Table smb_mapjoin_13.test_table1 stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-1:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_13.test_table2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_13.test_table3 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-3:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_13.test_table4 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-7:STATS] in serial mode
-INFO  : Table smb_mapjoin_13.test_table2 stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-9:STATS] in serial mode
-INFO  : Table smb_mapjoin_13.test_table3 stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-11:STATS] in serial mode
-INFO  : Table smb_mapjoin_13.test_table4 stats: [numFiles=16, numRows=0, totalSize=5812, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: default@src
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table1
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table2
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table3
-ERROR : POSTHOOK: Output: smb_mapjoin_13@test_table4
-ERROR : POSTHOOK: Lineage: test_table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2.value EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-4:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-6:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-8:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-10:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query FROM default.src
-INSERT OVERWRITE TABLE test_table1 SELECT *
-INSERT OVERWRITE TABLE test_table2 SELECT *
-INSERT OVERWRITE TABLE test_table3 SELECT *
-INSERT OVERWRITE TABLE test_table4 SELECT *
-No rows affected 
->>>  
->>>  >>>  >>>  EXPLAIN EXTENDED
-SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@test_table1
+POSTHOOK: Output: default@test_table2
+POSTHOOK: Output: default@test_table3
+POSTHOOK: Output: default@test_table4
+POSTHOOK: Lineage: test_table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2.key SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2.value EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table3.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table3.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table4.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table4.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: EXPLAIN EXTENDED
 SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED
 SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED
-SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_JOIN'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table1'
-'            a'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table2'
-'            b'
-'         ='
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  b'
-'               value'
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_DIR'
-'            TOK_TMP_FILE'
-'      TOK_SELECT'
-'         TOK_HINTLIST'
-'            TOK_HINT'
-'               TOK_MAPJOIN'
-'               TOK_HINTARGLIST'
-'                  b'
-'         TOK_SELEXPR'
-'            TOK_ALLCOLREF'
-'      TOK_ORDERBY'
-'         TOK_TABSORTCOLNAMEASC'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'      TOK_LIMIT'
-'         10'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 value (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Position of Big Table: 0'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  Reduce Output Operator'
-'                    key expressions: _col0 (type: int)'
-'                    sort order: +'
-'                    tag: -1'
-'                    value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)'
-'                    auto parallelism: false'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [a]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: test_table1'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              SORTBUCKETCOLSPREFIX TRUE'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_13.test_table1'
-'              numFiles 16'
-'              numRows 0'
-'              rawDataSize 0'
-'              serialization.ddl struct test_table1 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 5812'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                COLUMN_STATS_ACCURATE true'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_13.test_table1'
-'                numFiles 16'
-'                numRows 0'
-'                rawDataSize 0'
-'                serialization.ddl struct test_table1 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                totalSize 5812'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_13.test_table1'
-'            name: smb_mapjoin_13.test_table1'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_13.db/test_table1 [a]'
-'      Needs Tagging: false'
-'      Reduce Operator Tree:'
-'        Select Operator'
-'          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)'
-'          outputColumnNames: _col0, _col1, _col2, _col3'
-'          Limit'
-'            Number of rows: 10'
-'            File Output Operator'
-'              compressed: false'
-'              GlobalTableId: 0'
-'              directory: file:/!!ELIDED!!
-'              NumFilesPerFileSink: 1'
-'              Stats Publishing Key Prefix: file:/!!ELIDED!!
-'              table:'
-'                  input format: org.apache.hadoop.mapred.TextInputFormat'
-'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                  properties:'
-'                    columns _col0,_col1,_col2,_col3'
-'                    columns.types int:string:int:string'
-'                    escape.delim \'
-'                    hive.serialization.extend.additional.nesting.levels true'
-'                    serialization.format 1'
-'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              TotalFiles: 1'
-'              GatherStats: false'
-'              MultiFileSpray: false'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: 10'
-'      Processor Tree:'
-'        ListSink'
-''
-168 rows selected 
->>>  
->>>  SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.value, type:int, comment:null), FieldSchema(name:b.key, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_13@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_13@test_table2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_13@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_13@test_table2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
-'a.key','a.value','b.value','b.key'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'0','val_0','0','val_0'
-'2','val_2','2','val_2'
-10 rows selected 
->>>  
->>>  >>>  >>>  EXPLAIN EXTENDED
-SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
-SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN EXTENDED
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_JOIN
+         TOK_TABREF
+            TOK_TABNAME
+               test_table1
+            a
+         TOK_TABREF
+            TOK_TABNAME
+               test_table2
+            b
+         =
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+            .
+               TOK_TABLE_OR_COL
+                  b
+               value
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_HINTLIST
+            TOK_HINT
+               TOK_MAPJOIN
+               TOK_HINTARGLIST
+                  b
+         TOK_SELEXPR
+            TOK_ALLCOLREF
+      TOK_ORDERBY
+         TOK_TABSORTCOLNAMEASC
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+      TOK_LIMIT
+         10
+
+
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 value (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Position of Big Table: 0
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    tag: -1
+                    value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)
+                    auto parallelism: false
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: test_table1
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            properties:
+              COLUMN_STATS_ACCURATE true
+              SORTBUCKETCOLSPREFIX TRUE
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table1
+              numFiles 16
+              numRows 0
+              rawDataSize 0
+              serialization.ddl struct test_table1 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                COLUMN_STATS_ACCURATE true
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table1
+                numFiles 16
+                numRows 0
+                rawDataSize 0
+                serialization.ddl struct test_table1 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 5812
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table1
+            name: default.test_table1
+      Truncated Path -> Alias:
+        /test_table1 [a]
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Limit
+            Number of rows: 10
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+#### A masked pattern was here ####
+              NumFilesPerFileSink: 1
+#### A masked pattern was here ####
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  properties:
+                    columns _col0,_col1,_col2,_col3
+                    columns.types int:string:int:string
+                    escape.delim \
+                    hive.serialization.extend.additional.nesting.levels true
+                    serialization.format 1
+                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 10
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT /*+mapjoin(b)*/ * FROM test_table1 a JOIN test_table2 b ON a.key = b.value ORDER BY a.key LIMIT 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table2
+#### A masked pattern was here ####
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+0	val_0	0	val_0
+2	val_2	2	val_2
+PREHOOK: query: EXPLAIN EXTENDED
 SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-4:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN EXTENDED
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN EXTENDED
 SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-'Explain'
-'ABSTRACT SYNTAX TREE:'
-'  '
-'TOK_QUERY'
-'   TOK_FROM'
-'      TOK_JOIN'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table3'
-'            a'
-'         TOK_TABREF'
-'            TOK_TABNAME'
-'               test_table4'
-'            b'
-'         ='
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  b'
-'               value'
-'   TOK_INSERT'
-'      TOK_DESTINATION'
-'         TOK_DIR'
-'            TOK_TMP_FILE'
-'      TOK_SELECT'
-'         TOK_HINTLIST'
-'            TOK_HINT'
-'               TOK_MAPJOIN'
-'               TOK_HINTARGLIST'
-'                  b'
-'         TOK_SELEXPR'
-'            TOK_ALLCOLREF'
-'      TOK_ORDERBY'
-'         TOK_TABSORTCOLNAMEASC'
-'            .'
-'               TOK_TABLE_OR_COL'
-'                  a'
-'               key'
-'      TOK_LIMIT'
-'         10'
-''
-''
-'STAGE DEPENDENCIES:'
-'  Stage-3 is a root stage'
-'  Stage-1 depends on stages: Stage-3'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-3'
-'    Map Reduce Local Work'
-'      Alias -> Map Local Tables:'
-'        b '
-'          Fetch Operator'
-'            limit: -1'
-'      Alias -> Map Local Operator Tree:'
-'        b '
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: UDFToDouble(value) is not null (type: boolean)'
-'              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE'
-'              HashTable Sink Operator'
-'                keys:'
-'                  0 UDFToDouble(key) (type: double)'
-'                  1 UDFToDouble(value) (type: double)'
-'                Position of Big Table: 0'
-''
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            GatherStats: false'
-'            Filter Operator'
-'              isSamplingPred: false'
-'              predicate: UDFToDouble(key) is not null (type: boolean)'
-'              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE'
-'              Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 UDFToDouble(key) (type: double)'
-'                  1 UDFToDouble(value) (type: double)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Position of Big Table: 0'
-'                Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE'
-'                  Reduce Output Operator'
-'                    key expressions: _col0 (type: int)'
-'                    sort order: +'
-'                    Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE'
-'                    tag: -1'
-'                    value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)'
-'                    auto parallelism: false'
-'      Local Work:'
-'        Map Reduce Local Work'
-'      Path -> Alias:'
-'        file:/!!ELIDED!! [a]'
-'      Path -> Partition:'
-'        file:/!!ELIDED!! '
-'          Partition'
-'            base file name: test_table3'
-'            input format: org.apache.hadoop.mapred.TextInputFormat'
-'            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'            properties:'
-'              COLUMN_STATS_ACCURATE true'
-'              SORTBUCKETCOLSPREFIX TRUE'
-'              bucket_count 16'
-'              bucket_field_name key'
-'              columns key,value'
-'              columns.comments '
-'              columns.types int:string'
-'              file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'              file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              location file:/!!ELIDED!!
-'              name smb_mapjoin_13.test_table3'
-'              numFiles 16'
-'              numRows 0'
-'              rawDataSize 0'
-'              serialization.ddl struct test_table3 { i32 key, string value}'
-'              serialization.format 1'
-'              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              totalSize 5812'
-'              transient_lastDdlTime !!UNIXTIME!!'
-'            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'          '
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              properties:'
-'                COLUMN_STATS_ACCURATE true'
-'                SORTBUCKETCOLSPREFIX TRUE'
-'                bucket_count 16'
-'                bucket_field_name key'
-'                columns key,value'
-'                columns.comments '
-'                columns.types int:string'
-'                file.inputformat org.apache.hadoop.mapred.TextInputFormat'
-'                file.outputformat org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                location file:/!!ELIDED!!
-'                name smb_mapjoin_13.test_table3'
-'                numFiles 16'
-'                numRows 0'
-'                rawDataSize 0'
-'                serialization.ddl struct test_table3 { i32 key, string value}'
-'                serialization.format 1'
-'                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                totalSize 5812'
-'                transient_lastDdlTime !!UNIXTIME!!'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_13.test_table3'
-'            name: smb_mapjoin_13.test_table3'
-'      Truncated Path -> Alias:'
-'        /smb_mapjoin_13.db/test_table3 [a]'
-'      Needs Tagging: false'
-'      Reduce Operator Tree:'
-'        Select Operator'
-'          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)'
-'          outputColumnNames: _col0, _col1, _col2, _col3'
-'          Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE'
-'          Limit'
-'            Number of rows: 10'
-'            Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE'
-'            File Output Operator'
-'              compressed: false'
-'              GlobalTableId: 0'
-'              directory: file:/!!ELIDED!!
-'              NumFilesPerFileSink: 1'
-'              Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE'
-'              Stats Publishing Key Prefix: file:/!!ELIDED!!
-'              table:'
-'                  input format: org.apache.hadoop.mapred.TextInputFormat'
-'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                  properties:'
-'                    columns _col0,_col1,_col2,_col3'
-'                    columns.types int:string:int:string'
-'                    escape.delim \'
-'                    hive.serialization.extend.additional.nesting.levels true'
-'                    serialization.format 1'
-'                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              TotalFiles: 1'
-'              GatherStats: false'
-'              MultiFileSpray: false'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: 10'
-'      Processor Tree:'
-'        ListSink'
-''
-199 rows selected 
->>>  
->>>  SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_13@test_table3
-ERROR : PREHOOK: Input: smb_mapjoin_13@test_table4
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Starting task [Stage-3:MAPREDLOCAL] in serial mode
-INFO  : Starting to launch local task to process map join;	maximum memory = !!ELIDED!!
-INFO  : End of local task; Time taken: !!ELIDED!! sec.
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:16
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_13@test_table3
-ERROR : POSTHOOK: Input: smb_mapjoin_13@test_table4
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
-'a.key','a.value','b.key','b.value'
-No rows selected 
->>>  !record
+POSTHOOK: type: QUERY
+ABSTRACT SYNTAX TREE:
+  
+TOK_QUERY
+   TOK_FROM
+      TOK_JOIN
+         TOK_TABREF
+            TOK_TABNAME
+               test_table3
+            a
+         TOK_TABREF
+            TOK_TABNAME
+               test_table4
+            b
+         =
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+            .
+               TOK_TABLE_OR_COL
+                  b
+               value
+   TOK_INSERT
+      TOK_DESTINATION
+         TOK_DIR
+            TOK_TMP_FILE
+      TOK_SELECT
+         TOK_HINTLIST
+            TOK_HINT
+               TOK_MAPJOIN
+               TOK_HINTARGLIST
+                  b
+         TOK_SELEXPR
+            TOK_ALLCOLREF
+      TOK_ORDERBY
+         TOK_TABSORTCOLNAMEASC
+            .
+               TOK_TABLE_OR_COL
+                  a
+               key
+      TOK_LIMIT
+         10
+
+
+STAGE DEPENDENCIES:
+  Stage-3 is a root stage
+  Stage-1 depends on stages: Stage-3
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-3
+    Map Reduce Local Work
+      Alias -> Map Local Tables:
+        b 
+          Fetch Operator
+            limit: -1
+      Alias -> Map Local Operator Tree:
+        b 
+          TableScan
+            alias: b
+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: UDFToDouble(value) is not null (type: boolean)
+              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              HashTable Sink Operator
+                keys:
+                  0 UDFToDouble(key) (type: double)
+                  1 UDFToDouble(value) (type: double)
+                Position of Big Table: 0
+
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            GatherStats: false
+            Filter Operator
+              isSamplingPred: false
+              predicate: UDFToDouble(key) is not null (type: boolean)
+              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 UDFToDouble(key) (type: double)
+                  1 UDFToDouble(value) (type: double)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Position of Big Table: 0
+                Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                  Reduce Output Operator
+                    key expressions: _col0 (type: int)
+                    sort order: +
+                    Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                    tag: -1
+                    value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)
+                    auto parallelism: false
+      Local Work:
+        Map Reduce Local Work
+      Path -> Alias:
+#### A masked pattern was here ####
+      Path -> Partition:
+#### A masked pattern was here ####
+          Partition
+            base file name: test_table3
+            input format: org.apache.hadoop.mapred.TextInputFormat
+            output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+            properties:
+              COLUMN_STATS_ACCURATE true
+              SORTBUCKETCOLSPREFIX TRUE
+              bucket_count 16
+              bucket_field_name key
+              columns key,value
+              columns.comments 
+              columns.types int:string
+#### A masked pattern was here ####
+              name default.test_table3
+              numFiles 16
+              numRows 0
+              rawDataSize 0
+              serialization.ddl struct test_table3 { i32 key, string value}
+              serialization.format 1
+              serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              totalSize 5812
+#### A masked pattern was here ####
+            serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+          
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              properties:
+                COLUMN_STATS_ACCURATE true
+                SORTBUCKETCOLSPREFIX TRUE
+                bucket_count 16
+                bucket_field_name key
+                columns key,value
+                columns.comments 
+                columns.types int:string
+#### A masked pattern was here ####
+                name default.test_table3
+                numFiles 16
+                numRows 0
+                rawDataSize 0
+                serialization.ddl struct test_table3 { i32 key, string value}
+                serialization.format 1
+                serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                totalSize 5812
+#### A masked pattern was here ####
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.test_table3
+            name: default.test_table3
+      Truncated Path -> Alias:
+        /test_table3 [a]
+      Needs Tagging: false
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)
+          outputColumnNames: _col0, _col1, _col2, _col3
+          Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 10
+            Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              GlobalTableId: 0
+#### A masked pattern was here ####
+              NumFilesPerFileSink: 1
+              Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+#### A masked pattern was here ####
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  properties:
+                    columns _col0,_col1,_col2,_col3
+                    columns.types int:string:int:string
+                    escape.delim \
+                    hive.serialization.extend.additional.nesting.levels true
+                    serialization.format 1
+                    serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              TotalFiles: 1
+              GatherStats: false
+              MultiFileSpray: false
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 10
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table3
+PREHOOK: Input: default@test_table4
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT /*+mapjoin(b)*/ * FROM test_table3 a JOIN test_table4 b ON a.key = b.value ORDER BY a.key LIMIT 10
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table3
+POSTHOOK: Input: default@test_table4
+#### A masked pattern was here ####
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
index 3b369af..db00dde 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
@@ -1,257 +1,96 @@
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  set hive.enforce.bucketing=true;
-No rows affected 
->>>  set hive.enforce.sorting=true;
-No rows affected 
->>>  set hive.exec.reducers.max = 1;
-No rows affected 
->>>  set hive.merge.mapfiles=false;
-No rows affected 
->>>  set hive.merge.mapredfiles=false; 
-No rows affected 
->>>  
->>>  >>>  CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_16
-ERROR : PREHOOK: Output: smb_mapjoin_16@test_table1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_16
-ERROR : POSTHOOK: Output: smb_mapjoin_16@test_table1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-No rows affected 
->>>  CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_16
-ERROR : PREHOOK: Output: smb_mapjoin_16@test_table2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_16
-ERROR : POSTHOOK: Output: smb_mapjoin_16@test_table2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-No rows affected 
->>>  
->>>  FROM default.src
-INSERT OVERWRITE TABLE test_table1 SELECT *
-INSERT OVERWRITE TABLE test_table2 SELECT *;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): FROM default.src
-INSERT OVERWRITE TABLE test_table1 SELECT *
-INSERT OVERWRITE TABLE test_table2 SELECT *
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:int, comment:null), FieldSchema(name:_col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): FROM default.src
+PREHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table1
+POSTHOOK: query: CREATE TABLE test_table1 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table1
+PREHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test_table2
+POSTHOOK: query: CREATE TABLE test_table2 (key INT, value STRING) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test_table2
+PREHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 SELECT *
 INSERT OVERWRITE TABLE test_table2 SELECT *
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: default@src
-ERROR : PREHOOK: Output: smb_mapjoin_16@test_table1
-ERROR : PREHOOK: Output: smb_mapjoin_16@test_table2
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 2
-INFO  : Launching Job 1 out of 2
-INFO  : Starting task [Stage-2:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_16.test_table1 from file:/!!ELIDED!!
-INFO  : Launching Job 2 out of 2
-INFO  : Starting task [Stage-4:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-3:STATS] in serial mode
-INFO  : Table smb_mapjoin_16.test_table1 stats: [numFiles=2, numRows=0, totalSize=5812, rawDataSize=0]
-INFO  : Starting task [Stage-1:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_16.test_table2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-5:STATS] in serial mode
-INFO  : Table smb_mapjoin_16.test_table2 stats: [numFiles=2, numRows=0, totalSize=5812, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: default@src
-ERROR : POSTHOOK: Output: smb_mapjoin_16@test_table1
-ERROR : POSTHOOK: Output: smb_mapjoin_16@test_table2
-ERROR : POSTHOOK: Lineage: test_table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: test_table2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-2:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-4:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query FROM default.src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@test_table1
+PREHOOK: Output: default@test_table2
+POSTHOOK: query: FROM src
 INSERT OVERWRITE TABLE test_table1 SELECT *
 INSERT OVERWRITE TABLE test_table2 SELECT *
-No rows affected 
->>>  
->>>  >>>  EXPLAIN
-SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): EXPLAIN
-SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): EXPLAIN
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@test_table1
+POSTHOOK: Output: default@test_table2
+POSTHOOK: Lineage: test_table1.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table1.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: test_table2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: EXPLAIN
 SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query EXPLAIN
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
 SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                Group By Operator'
-'                  aggregations: count()'
-'                  mode: hash'
-'                  outputColumnNames: _col0'
-'                  Reduce Output Operator'
-'                    sort order: '
-'                    value expressions: _col0 (type: bigint)'
-'      Reduce Operator Tree:'
-'        Group By Operator'
-'          aggregations: count(VALUE._col0)'
-'          mode: mergepartial'
-'          outputColumnNames: _col0'
-'          File Output Operator'
-'            compressed: false'
-'            table:'
-'                input format: org.apache.hadoop.mapred.TextInputFormat'
-'                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-45 rows selected 
->>>  SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_c1, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_16@test_table1
-ERROR : PREHOOK: Input: smb_mapjoin_16@test_table2
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_16@test_table1
-ERROR : POSTHOOK: Input: smb_mapjoin_16@test_table2
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
-'_c1'
-'1028'
-1 row selected 
->>>  !record
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                Group By Operator
+                  aggregations: count()
+                  mode: hash
+                  outputColumnNames: _col0
+                  Reduce Output Operator
+                    sort order: 
+                    value expressions: _col0 (type: bigint)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0)
+          mode: mergepartial
+          outputColumnNames: _col0
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test_table1
+PREHOOK: Input: default@test_table2
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT /*+mapjoin(b)*/ count(*) FROM test_table1 a JOIN test_table2 b ON a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test_table1
+POSTHOOK: Input: default@test_table2
+#### A masked pattern was here ####
+1028
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
index 61f174c..aa04760 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
@@ -1,973 +1,499 @@
->>>  
->>>  
->>>  
->>>  
->>>  create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_2
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_2
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_2
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_2
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_2
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_2
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  
->>>  load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_2.smb_bucket_1 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_2.smb_bucket_1 stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_2
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_2.smb_bucket_2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_2.smb_bucket_2 stats: [numFiles=1, numRows=0, totalSize=206, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_2@smb_bucket_3
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_2.smb_bucket_3 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_2.smb_bucket_3 stats: [numFiles=1, numRows=0, totalSize=222, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_2@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-No rows affected 
->>>   
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  
->>>  >>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 111 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'4','val_4','4','val_4'
-'10','val_10','10','val_10'
-2 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 111 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+Warning: org.apache.hive.jdbc.ClosedOrCancelledStatementException: Method getQueryLog() failed. The statement has been closed or cancelled. (state=,code=0)
+4	val_4	4	val_4
+10	val_10	10	val_10
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','4','val_4'
-'5','val_5','NULL','NULL'
-'10','val_10','10','val_10'
-5 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	4	val_4
+5	val_5	NULL	NULL
+10	val_10	10	val_10
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'4','val_4','4','val_4'
-'10','val_10','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-6 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+4	val_4	4	val_4
+10	val_10	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','4','val_4'
-'5','val_5','NULL','NULL'
-'10','val_10','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-9 rows selected 
->>>  
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	4	val_4
+5	val_5	NULL	NULL
+10	val_10	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'4','val_4','4','val_4'
-'10','val_10','10','val_10'
-2 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 104 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+4	val_4	4	val_4
+10	val_10	10	val_10
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','4','val_4'
-'5','val_5','NULL','NULL'
-'10','val_10','10','val_10'
-5 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	4	val_4
+5	val_5	NULL	NULL
+10	val_10	10	val_10
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'4','val_4','4','val_4'
-'10','val_10','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-6 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a right outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+4	val_4	4	val_4
+10	val_10	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : PREHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_1
-ERROR : POSTHOOK: Input: smb_mapjoin_2@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'1','val_1','NULL','NULL'
-'3','val_3','NULL','NULL'
-'4','val_4','4','val_4'
-'5','val_5','NULL','NULL'
-'10','val_10','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','23','val_23'
-9 rows selected 
->>>  
->>>   
->>>  
->>>  
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 2 Data size: 208 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_1
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_1 a full outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_1
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+1	val_1	NULL	NULL
+3	val_3	NULL	NULL
+4	val_4	4	val_4
+5	val_5	NULL	NULL
+10	val_10	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	23	val_23
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_3.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_3.q.out
index 2feeff6..65154d0 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_3.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_3.q.out
@@ -1,967 +1,494 @@
->>>  >>>  
->>>  
->>>  
->>>  create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_3
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_3
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE; 
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_3
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_3
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_3
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_3
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_3
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
-No rows affected 
->>>  
->>>  load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_3.smb_bucket_1 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_3.smb_bucket_1 stats: [numFiles=1, numRows=0, totalSize=208, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_2
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_3.smb_bucket_2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_3.smb_bucket_2 stats: [numFiles=1, numRows=0, totalSize=206, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
-No rows affected 
->>>  load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_3@smb_bucket_3
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_3.smb_bucket_3 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_3.smb_bucket_3 stats: [numFiles=1, numRows=0, totalSize=222, rawDataSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_3@smb_bucket_3
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
-No rows affected 
->>>  
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>   
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+PREHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: create table smb_bucket_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: create table smb_bucket_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: create table smb_bucket_3(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 1 BUCKETS STORED AS RCFILE
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_1
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_1.rc' overwrite into table smb_bucket_1
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_1
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_2
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_2.rc' overwrite into table smb_bucket_2
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_2
+PREHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket_3
+POSTHOOK: query: load data local inpath '../../data/files/smbbucket_3.rc' overwrite into table smb_bucket_3
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket_3
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 111 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-2 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 111 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+20	val_20	20	val_20
+23	val_23	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-'25','val_25','NULL','NULL'
-'30','val_30','NULL','NULL'
-4 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+20	val_20	20	val_20
+23	val_23	23	val_23
+25	val_25	NULL	NULL
+30	val_30	NULL	NULL
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','4','val_4'
-'NULL','NULL','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-6 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+NULL	NULL	4	val_4
+NULL	NULL	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+20	val_20	20	val_20
+23	val_23	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','4','val_4'
-'NULL','NULL','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-'25','val_25','NULL','NULL'
-'30','val_30','NULL','NULL'
-8 rows selected 
->>>  
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 2 Data size: 222 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+NULL	NULL	4	val_4
+NULL	NULL	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+20	val_20	20	val_20
+23	val_23	23	val_23
+25	val_25	NULL	NULL
+30	val_30	NULL	NULL
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Filter Operator'
-'              predicate: key is not null (type: boolean)'
-'              Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'              Sorted Merge Bucket Map Join Operator'
-'                condition map:'
-'                     Inner Join 0 to 1'
-'                keys:'
-'                  0 key (type: int)'
-'                  1 key (type: int)'
-'                outputColumnNames: _col0, _col1, _col5, _col6'
-'                Select Operator'
-'                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                  outputColumnNames: _col0, _col1, _col2, _col3'
-'                  File Output Operator'
-'                    compressed: false'
-'                    table:'
-'                        input format: org.apache.hadoop.mapred.TextInputFormat'
-'                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-37 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-2 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Filter Operator
+              predicate: key is not null (type: boolean)
+              Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+              Sorted Merge Bucket Map Join Operator
+                condition map:
+                     Inner Join 0 to 1
+                keys:
+                  0 key (type: int)
+                  1 key (type: int)
+                outputColumnNames: _col0, _col1, _col5, _col6
+                Select Operator
+                  expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                  outputColumnNames: _col0, _col1, _col2, _col3
+                  File Output Operator
+                    compressed: false
+                    table:
+                        input format: org.apache.hadoop.mapred.TextInputFormat
+                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                        serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+20	val_20	20	val_20
+23	val_23	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Left Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-'25','val_25','NULL','NULL'
-'30','val_30','NULL','NULL'
-4 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Left Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a left outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+20	val_20	20	val_20
+23	val_23	23	val_23
+25	val_25	NULL	NULL
+30	val_30	NULL	NULL
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Right Outer Join0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','4','val_4'
-'NULL','NULL','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-6 rows selected 
->>>  
->>>  explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
-select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Right Outer Join0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a right outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+NULL	NULL	4	val_4
+NULL	NULL	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+20	val_20	20	val_20
+23	val_23	23	val_23
+PREHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-3:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-0 depends on stages: Stage-1'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: a'
-'            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-''
-'  Stage: Stage-0'
-'    Fetch Operator'
-'      limit: -1'
-'      Processor Tree:'
-'        ListSink'
-''
-34 rows selected 
->>>  select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : PREHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_2
-ERROR : POSTHOOK: Input: smb_mapjoin_3@smb_bucket_3
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
-'a.key','a.value','b.key','b.value'
-'NULL','NULL','4','val_4'
-'NULL','NULL','10','val_10'
-'NULL','NULL','17','val_17'
-'NULL','NULL','19','val_19'
-'20','val_20','20','val_20'
-'23','val_23','23','val_23'
-'25','val_25','NULL','NULL'
-'30','val_30','NULL','NULL'
-8 rows selected 
->>>  
->>>   
->>>  
->>>  
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: a
+            Statistics: Num rows: 1 Data size: 206 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: -1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket_2
+PREHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+POSTHOOK: query: select /*+mapjoin(b)*/ * from smb_bucket_2 a full outer join smb_bucket_3 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket_2
+POSTHOOK: Input: default@smb_bucket_3
+#### A masked pattern was here ####
+NULL	NULL	4	val_4
+NULL	NULL	10	val_10
+NULL	NULL	17	val_17
+NULL	NULL	19	val_19
+20	val_20	20	val_20
+23	val_23	23	val_23
+25	val_25	NULL	NULL
+30	val_30	NULL	NULL
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_7.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_7.q.out
index a8b7674..82f5804 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_7.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_7.q.out
@@ -1,1824 +1,1268 @@
->>>  set hive.enforce.bucketing = true;
-No rows affected 
->>>  set hive.enforce.sorting = true;
-No rows affected 
->>>  set hive.exec.reducers.max = 1;
-No rows affected 
->>>  
->>>  
->>>  CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_7
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_7
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-No rows affected 
->>>  
->>>  
->>>  CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_7
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_bucket4_2
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_7
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_bucket4_2
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
-No rows affected 
->>>  
->>>  
->>>  
->>>  
->>>  create table smb_join_results(k1 int, v1 string, k2 int, v2 string);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_7
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_join_results
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_7
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_join_results
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
-No rows affected 
->>>  create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_7
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_7
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string)
-No rows affected 
->>>  create table normal_join_results(k1 int, v1 string, k2 int, v2 string);
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): create table normal_join_results(k1 int, v1 string, k2 int, v2 string)
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): create table normal_join_results(k1 int, v1 string, k2 int, v2 string)
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Output: database:smb_mapjoin_7
-ERROR : PREHOOK: Output: smb_mapjoin_7@normal_join_results
-INFO  : Starting task [Stage-0:DDL] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Output: database:smb_mapjoin_7
-ERROR : POSTHOOK: Output: smb_mapjoin_7@normal_join_results
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query create table normal_join_results(k1 int, v1 string, k2 int, v2 string)
-No rows affected 
->>>  
->>>  load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_bucket4_1 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_bucket4_1 stats: [numFiles=1, totalSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1
-No rows affected 
->>>  load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:null, properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: file:/!!ELIDED!!
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_bucket4_1 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-1:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_bucket4_1 stats: [numFiles=2, totalSize=0]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: file:/!!ELIDED!!
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_bucket4_1
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1
-No rows affected 
->>>  
->>>  insert overwrite table smb_bucket4_2
-select * from default.src;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): insert overwrite table smb_bucket4_2
-select * from default.src
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:_col0, type:int, comment:null), FieldSchema(name:_col1, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): insert overwrite table smb_bucket4_2
-select * from default.src
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: default@src
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_bucket4_2
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_bucket4_2 from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_bucket4_2 stats: [numFiles=2, numRows=500, totalSize=5812, rawDataSize=5312]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: default@src
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_bucket4_2
-ERROR : POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
-ERROR : POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query insert overwrite table smb_bucket4_2
-select * from default.src
-No rows affected 
->>>  
->>>  set hive.optimize.bucketmapjoin = true;
-No rows affected 
->>>  set hive.optimize.bucketmapjoin.sortedmerge = true;
-No rows affected 
->>>  set hive.input.format = org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;
-No rows affected 
->>>  
->>>  insert overwrite table smb_join_results_empty_bigtable
-select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results_empty_bigtable
+PREHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket4_1
+POSTHOOK: query: CREATE TABLE smb_bucket4_1(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket4_1
+PREHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_bucket4_2
+POSTHOOK: query: CREATE TABLE smb_bucket4_2(key int, value string) CLUSTERED BY (key) SORTED BY (key) INTO 2 BUCKETS
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_bucket4_2
+PREHOOK: query: create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_join_results
+POSTHOOK: query: create table smb_join_results(k1 int, v1 string, k2 int, v2 string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_join_results
+PREHOOK: query: create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@smb_join_results_empty_bigtable
+POSTHOOK: query: create table smb_join_results_empty_bigtable(k1 int, v1 string, k2 int, v2 string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@smb_join_results_empty_bigtable
+PREHOOK: query: create table normal_join_results(k1 int, v1 string, k2 int, v2 string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@normal_join_results
+POSTHOOK: query: create table normal_join_results(k1 int, v1 string, k2 int, v2 string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@normal_join_results
+PREHOOK: query: load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket4_1
+POSTHOOK: query: load data local inpath '../../data/files/empty1.txt' into table smb_bucket4_1
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket4_1
+PREHOOK: query: load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@smb_bucket4_1
+POSTHOOK: query: load data local inpath '../../data/files/empty2.txt' into table smb_bucket4_1
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@smb_bucket4_1
+PREHOOK: query: insert overwrite table smb_bucket4_2
+select * from src
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+PREHOOK: Output: default@smb_bucket4_2
+POSTHOOK: query: insert overwrite table smb_bucket4_2
+select * from src
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+POSTHOOK: Output: default@smb_bucket4_2
+POSTHOOK: Lineage: smb_bucket4_2.key EXPRESSION [(src)src.FieldSchema(name:key, type:string, comment:default), ]
+POSTHOOK: Lineage: smb_bucket4_2.value SIMPLE [(src)src.FieldSchema(name:value, type:string, comment:default), ]
+PREHOOK: query: insert overwrite table smb_join_results_empty_bigtable
 select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results_empty_bigtable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket4_1
+PREHOOK: Input: default@smb_bucket4_2
+PREHOOK: Output: default@smb_join_results_empty_bigtable
+POSTHOOK: query: insert overwrite table smb_join_results_empty_bigtable
 select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 3
-INFO  : Launching Job 1 out of 3
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-7:CONDITIONAL] in serial mode
-INFO  : Stage-4 is filtered out by condition resolver.
-INFO  : Stage-3 is selected by condition resolver.
-INFO  : Stage-5 is filtered out by condition resolver.
-INFO  : Launching Job 3 out of 3
-INFO  : Starting task [Stage-3:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_join_results_empty_bigtable from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_join_results_empty_bigtable stats: [numFiles=1, numRows=500, totalSize=8812, rawDataSize=8312]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-3:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query insert overwrite table smb_join_results_empty_bigtable
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket4_1
+POSTHOOK: Input: default@smb_bucket4_2
+POSTHOOK: Output: default@smb_join_results_empty_bigtable
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: insert overwrite table smb_join_results_empty_bigtable
 select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-No rows affected 
->>>  
->>>  insert overwrite table smb_join_results_empty_bigtable
-select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results_empty_bigtable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket4_1
+PREHOOK: Input: default@smb_bucket4_2
+PREHOOK: Output: default@smb_join_results_empty_bigtable
+POSTHOOK: query: insert overwrite table smb_join_results_empty_bigtable
 select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results_empty_bigtable
-select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 3
-INFO  : Launching Job 1 out of 3
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-7:CONDITIONAL] in serial mode
-INFO  : Stage-4 is filtered out by condition resolver.
-INFO  : Stage-3 is selected by condition resolver.
-INFO  : Stage-5 is filtered out by condition resolver.
-INFO  : Launching Job 3 out of 3
-INFO  : Starting task [Stage-3:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_join_results_empty_bigtable from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_join_results_empty_bigtable stats: [numFiles=1, numRows=500, totalSize=8812, rawDataSize=8312]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-3:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query insert overwrite table smb_join_results_empty_bigtable
-select /*+mapjoin(b)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-No rows affected 
->>>  
->>>  select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:smb_join_results_empty_bigtable.k1, type:int, comment:null), FieldSchema(name:smb_join_results_empty_bigtable.v1, type:string, comment:null), FieldSchema(name:smb_join_results_empty_bigtable.k2, type:int, comment:null), FieldSchema(name:smb_join_results_empty_bigtable.v2, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
-'smb_join_results_empty_bigtable.k1','smb_join_results_empty_bigtable.v1','smb_join_results_empty_bigtable.k2','smb_join_results_empty_bigtable.v2'
-'NULL','NULL','0','val_0'
-'NULL','NULL','0','val_0'
-'NULL','NULL','0','val_0'
-'NULL','NULL','2','val_2'
-'NULL','NULL','4','val_4'
-'NULL','NULL','5','val_5'
-'NULL','NULL','5','val_5'
-'NULL','NULL','5','val_5'
-'NULL','NULL','8','val_8'
-'NULL','NULL','9','val_9'
-'NULL','NULL','10','val_10'
-'NULL','NULL','11','val_11'
-'NULL','NULL','12','val_12'
-'NULL','NULL','12','val_12'
-'NULL','NULL','15','val_15'
-'NULL','NULL','15','val_15'
-'NULL','NULL','17','val_17'
-'NULL','NULL','18','val_18'
-'NULL','NULL','18','val_18'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','24','val_24'
-'NULL','NULL','24','val_24'
-'NULL','NULL','26','val_26'
-'NULL','NULL','26','val_26'
-'NULL','NULL','27','val_27'
-'NULL','NULL','28','val_28'
-'NULL','NULL','30','val_30'
-'NULL','NULL','33','val_33'
-'NULL','NULL','34','val_34'
-'NULL','NULL','35','val_35'
-'NULL','NULL','35','val_35'
-'NULL','NULL','35','val_35'
-'NULL','NULL','37','val_37'
-'NULL','NULL','37','val_37'
-'NULL','NULL','41','val_41'
-'NULL','NULL','42','val_42'
-'NULL','NULL','42','val_42'
-'NULL','NULL','43','val_43'
-'NULL','NULL','44','val_44'
-'NULL','NULL','47','val_47'
-'NULL','NULL','51','val_51'
-'NULL','NULL','51','val_51'
-'NULL','NULL','53','val_53'
-'NULL','NULL','54','val_54'
-'NULL','NULL','57','val_57'
-'NULL','NULL','58','val_58'
-'NULL','NULL','58','val_58'
-'NULL','NULL','64','val_64'
-'NULL','NULL','65','val_65'
-'NULL','NULL','66','val_66'
-'NULL','NULL','67','val_67'
-'NULL','NULL','67','val_67'
-'NULL','NULL','69','val_69'
-'NULL','NULL','70','val_70'
-'NULL','NULL','70','val_70'
-'NULL','NULL','70','val_70'
-'NULL','NULL','72','val_72'
-'NULL','NULL','72','val_72'
-'NULL','NULL','74','val_74'
-'NULL','NULL','76','val_76'
-'NULL','NULL','76','val_76'
-'NULL','NULL','77','val_77'
-'NULL','NULL','78','val_78'
-'NULL','NULL','80','val_80'
-'NULL','NULL','82','val_82'
-'NULL','NULL','83','val_83'
-'NULL','NULL','83','val_83'
-'NULL','NULL','84','val_84'
-'NULL','NULL','84','val_84'
-'NULL','NULL','85','val_85'
-'NULL','NULL','86','val_86'
-'NULL','NULL','87','val_87'
-'NULL','NULL','90','val_90'
-'NULL','NULL','90','val_90'
-'NULL','NULL','90','val_90'
-'NULL','NULL','92','val_92'
-'NULL','NULL','95','val_95'
-'NULL','NULL','95','val_95'
-'NULL','NULL','96','val_96'
-'NULL','NULL','97','val_97'
-'NULL','NULL','97','val_97'
-'NULL','NULL','98','val_98'
-'NULL','NULL','98','val_98'
-'NULL','NULL','100','val_100'
-'NULL','NULL','100','val_100'
-'NULL','NULL','103','val_103'
-'NULL','NULL','103','val_103'
-'NULL','NULL','104','val_104'
-'NULL','NULL','104','val_104'
-'NULL','NULL','105','val_105'
-'NULL','NULL','111','val_111'
-'NULL','NULL','113','val_113'
-'NULL','NULL','113','val_113'
-'NULL','NULL','114','val_114'
-'NULL','NULL','116','val_116'
-'NULL','NULL','118','val_118'
-'NULL','NULL','118','val_118'
-'NULL','NULL','119','val_119'
-'NULL','NULL','119','val_119'
-'NULL','NULL','119','val_119'
-'NULL','NULL','120','val_120'
-'NULL','NULL','120','val_120'
-'NULL','NULL','125','val_125'
-'NULL','NULL','125','val_125'
-'NULL','NULL','126','val_126'
-'NULL','NULL','128','val_128'
-'NULL','NULL','128','val_128'
-'NULL','NULL','128','val_128'
-'NULL','NULL','129','val_129'
-'NULL','NULL','129','val_129'
-'NULL','NULL','131','val_131'
-'NULL','NULL','133','val_133'
-'NULL','NULL','134','val_134'
-'NULL','NULL','134','val_134'
-'NULL','NULL','136','val_136'
-'NULL','NULL','137','val_137'
-'NULL','NULL','137','val_137'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','143','val_143'
-'NULL','NULL','145','val_145'
-'NULL','NULL','146','val_146'
-'NULL','NULL','146','val_146'
-'NULL','NULL','149','val_149'
-'NULL','NULL','149','val_149'
-'NULL','NULL','150','val_150'
-'NULL','NULL','152','val_152'
-'NULL','NULL','152','val_152'
-'NULL','NULL','153','val_153'
-'NULL','NULL','155','val_155'
-'NULL','NULL','156','val_156'
-'NULL','NULL','157','val_157'
-'NULL','NULL','158','val_158'
-'NULL','NULL','160','val_160'
-'NULL','NULL','162','val_162'
-'NULL','NULL','163','val_163'
-'NULL','NULL','164','val_164'
-'NULL','NULL','164','val_164'
-'NULL','NULL','165','val_165'
-'NULL','NULL','165','val_165'
-'NULL','NULL','166','val_166'
-'NULL','NULL','167','val_167'
-'NULL','NULL','167','val_167'
-'NULL','NULL','167','val_167'
-'NULL','NULL','168','val_168'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','170','val_170'
-'NULL','NULL','172','val_172'
-'NULL','NULL','172','val_172'
-'NULL','NULL','174','val_174'
-'NULL','NULL','174','val_174'
-'NULL','NULL','175','val_175'
-'NULL','NULL','175','val_175'
-'NULL','NULL','176','val_176'
-'NULL','NULL','176','val_176'
-'NULL','NULL','177','val_177'
-'NULL','NULL','178','val_178'
-'NULL','NULL','179','val_179'
-'NULL','NULL','179','val_179'
-'NULL','NULL','180','val_180'
-'NULL','NULL','181','val_181'
-'NULL','NULL','183','val_183'
-'NULL','NULL','186','val_186'
-'NULL','NULL','187','val_187'
-'NULL','NULL','187','val_187'
-'NULL','NULL','187','val_187'
-'NULL','NULL','189','val_189'
-'NULL','NULL','190','val_190'
-'NULL','NULL','191','val_191'
-'NULL','NULL','191','val_191'
-'NULL','NULL','192','val_192'
-'NULL','NULL','193','val_193'
-'NULL','NULL','193','val_193'
-'NULL','NULL','193','val_193'
-'NULL','NULL','194','val_194'
-'NULL','NULL','195','val_195'
-'NULL','NULL','195','val_195'
-'NULL','NULL','196','val_196'
-'NULL','NULL','197','val_197'
-'NULL','NULL','197','val_197'
-'NULL','NULL','199','val_199'
-'NULL','NULL','199','val_199'
-'NULL','NULL','199','val_199'
-'NULL','NULL','200','val_200'
-'NULL','NULL','200','val_200'
-'NULL','NULL','201','val_201'
-'NULL','NULL','202','val_202'
-'NULL','NULL','203','val_203'
-'NULL','NULL','203','val_203'
-'NULL','NULL','205','val_205'
-'NULL','NULL','205','val_205'
-'NULL','NULL','207','val_207'
-'NULL','NULL','207','val_207'
-'NULL','NULL','208','val_208'
-'NULL','NULL','208','val_208'
-'NULL','NULL','208','val_208'
-'NULL','NULL','209','val_209'
-'NULL','NULL','209','val_209'
-'NULL','NULL','213','val_213'
-'NULL','NULL','213','val_213'
-'NULL','NULL','214','val_214'
-'NULL','NULL','216','val_216'
-'NULL','NULL','216','val_216'
-'NULL','NULL','217','val_217'
-'NULL','NULL','217','val_217'
-'NULL','NULL','218','val_218'
-'NULL','NULL','219','val_219'
-'NULL','NULL','219','val_219'
-'NULL','NULL','221','val_221'
-'NULL','NULL','221','val_221'
-'NULL','NULL','222','val_222'
-'NULL','NULL','223','val_223'
-'NULL','NULL','223','val_223'
-'NULL','NULL','224','val_224'
-'NULL','NULL','224','val_224'
-'NULL','NULL','226','val_226'
-'NULL','NULL','228','val_228'
-'NULL','NULL','229','val_229'
-'NULL','NULL','229','val_229'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','233','val_233'
-'NULL','NULL','233','val_233'
-'NULL','NULL','235','val_235'
-'NULL','NULL','237','val_237'
-'NULL','NULL','237','val_237'
-'NULL','NULL','238','val_238'
-'NULL','NULL','238','val_238'
-'NULL','NULL','239','val_239'
-'NULL','NULL','239','val_239'
-'NULL','NULL','241','val_241'
-'NULL','NULL','242','val_242'
-'NULL','NULL','242','val_242'
-'NULL','NULL','244','val_244'
-'NULL','NULL','247','val_247'
-'NULL','NULL','248','val_248'
-'NULL','NULL','249','val_249'
-'NULL','NULL','252','val_252'
-'NULL','NULL','255','val_255'
-'NULL','NULL','255','val_255'
-'NULL','NULL','256','val_256'
-'NULL','NULL','256','val_256'
-'NULL','NULL','257','val_257'
-'NULL','NULL','258','val_258'
-'NULL','NULL','260','val_260'
-'NULL','NULL','262','val_262'
-'NULL','NULL','263','val_263'
-'NULL','NULL','265','val_265'
-'NULL','NULL','265','val_265'
-'NULL','NULL','266','val_266'
-'NULL','NULL','272','val_272'
-'NULL','NULL','272','val_272'
-'NULL','NULL','273','val_273'
-'NULL','NULL','273','val_273'
-'NULL','NULL','273','val_273'
-'NULL','NULL','274','val_274'
-'NULL','NULL','275','val_275'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','278','val_278'
-'NULL','NULL','278','val_278'
-'NULL','NULL','280','val_280'
-'NULL','NULL','280','val_280'
-'NULL','NULL','281','val_281'
-'NULL','NULL','281','val_281'
-'NULL','NULL','282','val_282'
-'NULL','NULL','282','val_282'
-'NULL','NULL','283','val_283'
-'NULL','NULL','284','val_284'
-'NULL','NULL','285','val_285'
-'NULL','NULL','286','val_286'
-'NULL','NULL','287','val_287'
-'NULL','NULL','288','val_288'
-'NULL','NULL','288','val_288'
-'NULL','NULL','289','val_289'
-'NULL','NULL','291','val_291'
-'NULL','NULL','292','val_292'
-'NULL','NULL','296','val_296'
-'NULL','NULL','298','val_298'
-'NULL','NULL','298','val_298'
-'NULL','NULL','298','val_298'
-'NULL','NULL','302','val_302'
-'NULL','NULL','305','val_305'
-'NULL','NULL','306','val_306'
-'NULL','NULL','307','val_307'
-'NULL','NULL','307','val_307'
-'NULL','NULL','308','val_308'
-'NULL','NULL','309','val_309'
-'NULL','NULL','309','val_309'
-'NULL','NULL','310','val_310'
-'NULL','NULL','311','val_311'
-'NULL','NULL','311','val_311'
-'NULL','NULL','311','val_311'
-'NULL','NULL','315','val_315'
-'NULL','NULL','316','val_316'
-'NULL','NULL','316','val_316'
-'NULL','NULL','316','val_316'
-'NULL','NULL','317','val_317'
-'NULL','NULL','317','val_317'
-'NULL','NULL','318','val_318'
-'NULL','NULL','318','val_318'
-'NULL','NULL','318','val_318'
-'NULL','NULL','321','val_321'
-'NULL','NULL','321','val_321'
-'NULL','NULL','322','val_322'
-'NULL','NULL','322','val_322'
-'NULL','NULL','323','val_323'
-'NULL','NULL','325','val_325'
-'NULL','NULL','325','val_325'
-'NULL','NULL','327','val_327'
-'NULL','NULL','327','val_327'
-'NULL','NULL','327','val_327'
-'NULL','NULL','331','val_331'
-'NULL','NULL','331','val_331'
-'NULL','NULL','332','val_332'
-'NULL','NULL','333','val_333'
-'NULL','NULL','333','val_333'
-'NULL','NULL','335','val_335'
-'NULL','NULL','336','val_336'
-'NULL','NULL','338','val_338'
-'NULL','NULL','339','val_339'
-'NULL','NULL','341','val_341'
-'NULL','NULL','342','val_342'
-'NULL','NULL','342','val_342'
-'NULL','NULL','344','val_344'
-'NULL','NULL','344','val_344'
-'NULL','NULL','345','val_345'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','351','val_351'
-'NULL','NULL','353','val_353'
-'NULL','NULL','353','val_353'
-'NULL','NULL','356','val_356'
-'NULL','NULL','360','val_360'
-'NULL','NULL','362','val_362'
-'NULL','NULL','364','val_364'
-'NULL','NULL','365','val_365'
-'NULL','NULL','366','val_366'
-'NULL','NULL','367','val_367'
-'NULL','NULL','367','val_367'
-'NULL','NULL','368','val_368'
-'NULL','NULL','369','val_369'
-'NULL','NULL','369','val_369'
-'NULL','NULL','369','val_369'
-'NULL','NULL','373','val_373'
-'NULL','NULL','374','val_374'
-'NULL','NULL','375','val_375'
-'NULL','NULL','377','val_377'
-'NULL','NULL','378','val_378'
-'NULL','NULL','379','val_379'
-'NULL','NULL','382','val_382'
-'NULL','NULL','382','val_382'
-'NULL','NULL','384','val_384'
-'NULL','NULL','384','val_384'
-'NULL','NULL','384','val_384'
-'NULL','NULL','386','val_386'
-'NULL','NULL','389','val_389'
-'NULL','NULL','392','val_392'
-'NULL','NULL','393','val_393'
-'NULL','NULL','394','val_394'
-'NULL','NULL','395','val_395'
-'NULL','NULL','395','val_395'
-'NULL','NULL','396','val_396'
-'NULL','NULL','396','val_396'
-'NULL','NULL','396','val_396'
-'NULL','NULL','397','val_397'
-'NULL','NULL','397','val_397'
-'NULL','NULL','399','val_399'
-'NULL','NULL','399','val_399'
-'NULL','NULL','400','val_400'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','402','val_402'
-'NULL','NULL','403','val_403'
-'NULL','NULL','403','val_403'
-'NULL','NULL','403','val_403'
-'NULL','NULL','404','val_404'
-'NULL','NULL','404','val_404'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','407','val_407'
-'NULL','NULL','409','val_409'
-'NULL','NULL','409','val_409'
-'NULL','NULL','409','val_409'
-'NULL','NULL','411','val_411'
-'NULL','NULL','413','val_413'
-'NULL','NULL','413','val_413'
-'NULL','NULL','414','val_414'
-'NULL','NULL','414','val_414'
-'NULL','NULL','417','val_417'
-'NULL','NULL','417','val_417'
-'NULL','NULL','417','val_417'
-'NULL','NULL','418','val_418'
-'NULL','NULL','419','val_419'
-'NULL','NULL','421','val_421'
-'NULL','NULL','424','val_424'
-'NULL','NULL','424','val_424'
-'NULL','NULL','427','val_427'
-'NULL','NULL','429','val_429'
-'NULL','NULL','429','val_429'
-'NULL','NULL','430','val_430'
-'NULL','NULL','430','val_430'
-'NULL','NULL','430','val_430'
-'NULL','NULL','431','val_431'
-'NULL','NULL','431','val_431'
-'NULL','NULL','431','val_431'
-'NULL','NULL','432','val_432'
-'NULL','NULL','435','val_435'
-'NULL','NULL','436','val_436'
-'NULL','NULL','437','val_437'
-'NULL','NULL','438','val_438'
-'NULL','NULL','438','val_438'
-'NULL','NULL','438','val_438'
-'NULL','NULL','439','val_439'
-'NULL','NULL','439','val_439'
-'NULL','NULL','443','val_443'
-'NULL','NULL','444','val_444'
-'NULL','NULL','446','val_446'
-'NULL','NULL','448','val_448'
-'NULL','NULL','449','val_449'
-'NULL','NULL','452','val_452'
-'NULL','NULL','453','val_453'
-'NULL','NULL','454','val_454'
-'NULL','NULL','454','val_454'
-'NULL','NULL','454','val_454'
-'NULL','NULL','455','val_455'
-'NULL','NULL','457','val_457'
-'NULL','NULL','458','val_458'
-'NULL','NULL','458','val_458'
-'NULL','NULL','459','val_459'
-'NULL','NULL','459','val_459'
-'NULL','NULL','460','val_460'
-'NULL','NULL','462','val_462'
-'NULL','NULL','462','val_462'
-'NULL','NULL','463','val_463'
-'NULL','NULL','463','val_463'
-'NULL','NULL','466','val_466'
-'NULL','NULL','466','val_466'
-'NULL','NULL','466','val_466'
-'NULL','NULL','467','val_467'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','470','val_470'
-'NULL','NULL','472','val_472'
-'NULL','NULL','475','val_475'
-'NULL','NULL','477','val_477'
-'NULL','NULL','478','val_478'
-'NULL','NULL','478','val_478'
-'NULL','NULL','479','val_479'
-'NULL','NULL','480','val_480'
-'NULL','NULL','480','val_480'
-'NULL','NULL','480','val_480'
-'NULL','NULL','481','val_481'
-'NULL','NULL','482','val_482'
-'NULL','NULL','483','val_483'
-'NULL','NULL','484','val_484'
-'NULL','NULL','485','val_485'
-'NULL','NULL','487','val_487'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','490','val_490'
-'NULL','NULL','491','val_491'
-'NULL','NULL','492','val_492'
-'NULL','NULL','492','val_492'
-'NULL','NULL','493','val_493'
-'NULL','NULL','494','val_494'
-'NULL','NULL','495','val_495'
-'NULL','NULL','496','val_496'
-'NULL','NULL','497','val_497'
-'NULL','NULL','498','val_498'
-'NULL','NULL','498','val_498'
-'NULL','NULL','498','val_498'
-500 rows selected 
->>>  
->>>  explain
-insert overwrite table smb_join_results
-select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): explain
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket4_1
+POSTHOOK: Input: default@smb_bucket4_2
+POSTHOOK: Output: default@smb_join_results_empty_bigtable
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: smb_join_results_empty_bigtable.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_join_results_empty_bigtable
+#### A masked pattern was here ####
+POSTHOOK: query: select * from smb_join_results_empty_bigtable order by k1, v1, k2, v2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_join_results_empty_bigtable
+#### A masked pattern was here ####
+NULL	NULL	0	val_0
+NULL	NULL	0	val_0
+NULL	NULL	0	val_0
+NULL	NULL	2	val_2
+NULL	NULL	4	val_4
+NULL	NULL	5	val_5
+NULL	NULL	5	val_5
+NULL	NULL	5	val_5
+NULL	NULL	8	val_8
+NULL	NULL	9	val_9
+NULL	NULL	10	val_10
+NULL	NULL	11	val_11
+NULL	NULL	12	val_12
+NULL	NULL	12	val_12
+NULL	NULL	15	val_15
+NULL	NULL	15	val_15
+NULL	NULL	17	val_17
+NULL	NULL	18	val_18
+NULL	NULL	18	val_18
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	24	val_24
+NULL	NULL	24	val_24
+NULL	NULL	26	val_26
+NULL	NULL	26	val_26
+NULL	NULL	27	val_27
+NULL	NULL	28	val_28
+NULL	NULL	30	val_30
+NULL	NULL	33	val_33
+NULL	NULL	34	val_34
+NULL	NULL	35	val_35
+NULL	NULL	35	val_35
+NULL	NULL	35	val_35
+NULL	NULL	37	val_37
+NULL	NULL	37	val_37
+NULL	NULL	41	val_41
+NULL	NULL	42	val_42
+NULL	NULL	42	val_42
+NULL	NULL	43	val_43
+NULL	NULL	44	val_44
+NULL	NULL	47	val_47
+NULL	NULL	51	val_51
+NULL	NULL	51	val_51
+NULL	NULL	53	val_53
+NULL	NULL	54	val_54
+NULL	NULL	57	val_57
+NULL	NULL	58	val_58
+NULL	NULL	58	val_58
+NULL	NULL	64	val_64
+NULL	NULL	65	val_65
+NULL	NULL	66	val_66
+NULL	NULL	67	val_67
+NULL	NULL	67	val_67
+NULL	NULL	69	val_69
+NULL	NULL	70	val_70
+NULL	NULL	70	val_70
+NULL	NULL	70	val_70
+NULL	NULL	72	val_72
+NULL	NULL	72	val_72
+NULL	NULL	74	val_74
+NULL	NULL	76	val_76
+NULL	NULL	76	val_76
+NULL	NULL	77	val_77
+NULL	NULL	78	val_78
+NULL	NULL	80	val_80
+NULL	NULL	82	val_82
+NULL	NULL	83	val_83
+NULL	NULL	83	val_83
+NULL	NULL	84	val_84
+NULL	NULL	84	val_84
+NULL	NULL	85	val_85
+NULL	NULL	86	val_86
+NULL	NULL	87	val_87
+NULL	NULL	90	val_90
+NULL	NULL	90	val_90
+NULL	NULL	90	val_90
+NULL	NULL	92	val_92
+NULL	NULL	95	val_95
+NULL	NULL	95	val_95
+NULL	NULL	96	val_96
+NULL	NULL	97	val_97
+NULL	NULL	97	val_97
+NULL	NULL	98	val_98
+NULL	NULL	98	val_98
+NULL	NULL	100	val_100
+NULL	NULL	100	val_100
+NULL	NULL	103	val_103
+NULL	NULL	103	val_103
+NULL	NULL	104	val_104
+NULL	NULL	104	val_104
+NULL	NULL	105	val_105
+NULL	NULL	111	val_111
+NULL	NULL	113	val_113
+NULL	NULL	113	val_113
+NULL	NULL	114	val_114
+NULL	NULL	116	val_116
+NULL	NULL	118	val_118
+NULL	NULL	118	val_118
+NULL	NULL	119	val_119
+NULL	NULL	119	val_119
+NULL	NULL	119	val_119
+NULL	NULL	120	val_120
+NULL	NULL	120	val_120
+NULL	NULL	125	val_125
+NULL	NULL	125	val_125
+NULL	NULL	126	val_126
+NULL	NULL	128	val_128
+NULL	NULL	128	val_128
+NULL	NULL	128	val_128
+NULL	NULL	129	val_129
+NULL	NULL	129	val_129
+NULL	NULL	131	val_131
+NULL	NULL	133	val_133
+NULL	NULL	134	val_134
+NULL	NULL	134	val_134
+NULL	NULL	136	val_136
+NULL	NULL	137	val_137
+NULL	NULL	137	val_137
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	143	val_143
+NULL	NULL	145	val_145
+NULL	NULL	146	val_146
+NULL	NULL	146	val_146
+NULL	NULL	149	val_149
+NULL	NULL	149	val_149
+NULL	NULL	150	val_150
+NULL	NULL	152	val_152
+NULL	NULL	152	val_152
+NULL	NULL	153	val_153
+NULL	NULL	155	val_155
+NULL	NULL	156	val_156
+NULL	NULL	157	val_157
+NULL	NULL	158	val_158
+NULL	NULL	160	val_160
+NULL	NULL	162	val_162
+NULL	NULL	163	val_163
+NULL	NULL	164	val_164
+NULL	NULL	164	val_164
+NULL	NULL	165	val_165
+NULL	NULL	165	val_165
+NULL	NULL	166	val_166
+NULL	NULL	167	val_167
+NULL	NULL	167	val_167
+NULL	NULL	167	val_167
+NULL	NULL	168	val_168
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	170	val_170
+NULL	NULL	172	val_172
+NULL	NULL	172	val_172
+NULL	NULL	174	val_174
+NULL	NULL	174	val_174
+NULL	NULL	175	val_175
+NULL	NULL	175	val_175
+NULL	NULL	176	val_176
+NULL	NULL	176	val_176
+NULL	NULL	177	val_177
+NULL	NULL	178	val_178
+NULL	NULL	179	val_179
+NULL	NULL	179	val_179
+NULL	NULL	180	val_180
+NULL	NULL	181	val_181
+NULL	NULL	183	val_183
+NULL	NULL	186	val_186
+NULL	NULL	187	val_187
+NULL	NULL	187	val_187
+NULL	NULL	187	val_187
+NULL	NULL	189	val_189
+NULL	NULL	190	val_190
+NULL	NULL	191	val_191
+NULL	NULL	191	val_191
+NULL	NULL	192	val_192
+NULL	NULL	193	val_193
+NULL	NULL	193	val_193
+NULL	NULL	193	val_193
+NULL	NULL	194	val_194
+NULL	NULL	195	val_195
+NULL	NULL	195	val_195
+NULL	NULL	196	val_196
+NULL	NULL	197	val_197
+NULL	NULL	197	val_197
+NULL	NULL	199	val_199
+NULL	NULL	199	val_199
+NULL	NULL	199	val_199
+NULL	NULL	200	val_200
+NULL	NULL	200	val_200
+NULL	NULL	201	val_201
+NULL	NULL	202	val_202
+NULL	NULL	203	val_203
+NULL	NULL	203	val_203
+NULL	NULL	205	val_205
+NULL	NULL	205	val_205
+NULL	NULL	207	val_207
+NULL	NULL	207	val_207
+NULL	NULL	208	val_208
+NULL	NULL	208	val_208
+NULL	NULL	208	val_208
+NULL	NULL	209	val_209
+NULL	NULL	209	val_209
+NULL	NULL	213	val_213
+NULL	NULL	213	val_213
+NULL	NULL	214	val_214
+NULL	NULL	216	val_216
+NULL	NULL	216	val_216
+NULL	NULL	217	val_217
+NULL	NULL	217	val_217
+NULL	NULL	218	val_218
+NULL	NULL	219	val_219
+NULL	NULL	219	val_219
+NULL	NULL	221	val_221
+NULL	NULL	221	val_221
+NULL	NULL	222	val_222
+NULL	NULL	223	val_223
+NULL	NULL	223	val_223
+NULL	NULL	224	val_224
+NULL	NULL	224	val_224
+NULL	NULL	226	val_226
+NULL	NULL	228	val_228
+NULL	NULL	229	val_229
+NULL	NULL	229	val_229
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	233	val_233
+NULL	NULL	233	val_233
+NULL	NULL	235	val_235
+NULL	NULL	237	val_237
+NULL	NULL	237	val_237
+NULL	NULL	238	val_238
+NULL	NULL	238	val_238
+NULL	NULL	239	val_239
+NULL	NULL	239	val_239
+NULL	NULL	241	val_241
+NULL	NULL	242	val_242
+NULL	NULL	242	val_242
+NULL	NULL	244	val_244
+NULL	NULL	247	val_247
+NULL	NULL	248	val_248
+NULL	NULL	249	val_249
+NULL	NULL	252	val_252
+NULL	NULL	255	val_255
+NULL	NULL	255	val_255
+NULL	NULL	256	val_256
+NULL	NULL	256	val_256
+NULL	NULL	257	val_257
+NULL	NULL	258	val_258
+NULL	NULL	260	val_260
+NULL	NULL	262	val_262
+NULL	NULL	263	val_263
+NULL	NULL	265	val_265
+NULL	NULL	265	val_265
+NULL	NULL	266	val_266
+NULL	NULL	272	val_272
+NULL	NULL	272	val_272
+NULL	NULL	273	val_273
+NULL	NULL	273	val_273
+NULL	NULL	273	val_273
+NULL	NULL	274	val_274
+NULL	NULL	275	val_275
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	278	val_278
+NULL	NULL	278	val_278
+NULL	NULL	280	val_280
+NULL	NULL	280	val_280
+NULL	NULL	281	val_281
+NULL	NULL	281	val_281
+NULL	NULL	282	val_282
+NULL	NULL	282	val_282
+NULL	NULL	283	val_283
+NULL	NULL	284	val_284
+NULL	NULL	285	val_285
+NULL	NULL	286	val_286
+NULL	NULL	287	val_287
+NULL	NULL	288	val_288
+NULL	NULL	288	val_288
+NULL	NULL	289	val_289
+NULL	NULL	291	val_291
+NULL	NULL	292	val_292
+NULL	NULL	296	val_296
+NULL	NULL	298	val_298
+NULL	NULL	298	val_298
+NULL	NULL	298	val_298
+NULL	NULL	302	val_302
+NULL	NULL	305	val_305
+NULL	NULL	306	val_306
+NULL	NULL	307	val_307
+NULL	NULL	307	val_307
+NULL	NULL	308	val_308
+NULL	NULL	309	val_309
+NULL	NULL	309	val_309
+NULL	NULL	310	val_310
+NULL	NULL	311	val_311
+NULL	NULL	311	val_311
+NULL	NULL	311	val_311
+NULL	NULL	315	val_315
+NULL	NULL	316	val_316
+NULL	NULL	316	val_316
+NULL	NULL	316	val_316
+NULL	NULL	317	val_317
+NULL	NULL	317	val_317
+NULL	NULL	318	val_318
+NULL	NULL	318	val_318
+NULL	NULL	318	val_318
+NULL	NULL	321	val_321
+NULL	NULL	321	val_321
+NULL	NULL	322	val_322
+NULL	NULL	322	val_322
+NULL	NULL	323	val_323
+NULL	NULL	325	val_325
+NULL	NULL	325	val_325
+NULL	NULL	327	val_327
+NULL	NULL	327	val_327
+NULL	NULL	327	val_327
+NULL	NULL	331	val_331
+NULL	NULL	331	val_331
+NULL	NULL	332	val_332
+NULL	NULL	333	val_333
+NULL	NULL	333	val_333
+NULL	NULL	335	val_335
+NULL	NULL	336	val_336
+NULL	NULL	338	val_338
+NULL	NULL	339	val_339
+NULL	NULL	341	val_341
+NULL	NULL	342	val_342
+NULL	NULL	342	val_342
+NULL	NULL	344	val_344
+NULL	NULL	344	val_344
+NULL	NULL	345	val_345
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	351	val_351
+NULL	NULL	353	val_353
+NULL	NULL	353	val_353
+NULL	NULL	356	val_356
+NULL	NULL	360	val_360
+NULL	NULL	362	val_362
+NULL	NULL	364	val_364
+NULL	NULL	365	val_365
+NULL	NULL	366	val_366
+NULL	NULL	367	val_367
+NULL	NULL	367	val_367
+NULL	NULL	368	val_368
+NULL	NULL	369	val_369
+NULL	NULL	369	val_369
+NULL	NULL	369	val_369
+NULL	NULL	373	val_373
+NULL	NULL	374	val_374
+NULL	NULL	375	val_375
+NULL	NULL	377	val_377
+NULL	NULL	378	val_378
+NULL	NULL	379	val_379
+NULL	NULL	382	val_382
+NULL	NULL	382	val_382
+NULL	NULL	384	val_384
+NULL	NULL	384	val_384
+NULL	NULL	384	val_384
+NULL	NULL	386	val_386
+NULL	NULL	389	val_389
+NULL	NULL	392	val_392
+NULL	NULL	393	val_393
+NULL	NULL	394	val_394
+NULL	NULL	395	val_395
+NULL	NULL	395	val_395
+NULL	NULL	396	val_396
+NULL	NULL	396	val_396
+NULL	NULL	396	val_396
+NULL	NULL	397	val_397
+NULL	NULL	397	val_397
+NULL	NULL	399	val_399
+NULL	NULL	399	val_399
+NULL	NULL	400	val_400
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	402	val_402
+NULL	NULL	403	val_403
+NULL	NULL	403	val_403
+NULL	NULL	403	val_403
+NULL	NULL	404	val_404
+NULL	NULL	404	val_404
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	407	val_407
+NULL	NULL	409	val_409
+NULL	NULL	409	val_409
+NULL	NULL	409	val_409
+NULL	NULL	411	val_411
+NULL	NULL	413	val_413
+NULL	NULL	413	val_413
+NULL	NULL	414	val_414
+NULL	NULL	414	val_414
+NULL	NULL	417	val_417
+NULL	NULL	417	val_417
+NULL	NULL	417	val_417
+NULL	NULL	418	val_418
+NULL	NULL	419	val_419
+NULL	NULL	421	val_421
+NULL	NULL	424	val_424
+NULL	NULL	424	val_424
+NULL	NULL	427	val_427
+NULL	NULL	429	val_429
+NULL	NULL	429	val_429
+NULL	NULL	430	val_430
+NULL	NULL	430	val_430
+NULL	NULL	430	val_430
+NULL	NULL	431	val_431
+NULL	NULL	431	val_431
+NULL	NULL	431	val_431
+NULL	NULL	432	val_432
+NULL	NULL	435	val_435
+NULL	NULL	436	val_436
+NULL	NULL	437	val_437
+NULL	NULL	438	val_438
+NULL	NULL	438	val_438
+NULL	NULL	438	val_438
+NULL	NULL	439	val_439
+NULL	NULL	439	val_439
+NULL	NULL	443	val_443
+NULL	NULL	444	val_444
+NULL	NULL	446	val_446
+NULL	NULL	448	val_448
+NULL	NULL	449	val_449
+NULL	NULL	452	val_452
+NULL	NULL	453	val_453
+NULL	NULL	454	val_454
+NULL	NULL	454	val_454
+NULL	NULL	454	val_454
+NULL	NULL	455	val_455
+NULL	NULL	457	val_457
+NULL	NULL	458	val_458
+NULL	NULL	458	val_458
+NULL	NULL	459	val_459
+NULL	NULL	459	val_459
+NULL	NULL	460	val_460
+NULL	NULL	462	val_462
+NULL	NULL	462	val_462
+NULL	NULL	463	val_463
+NULL	NULL	463	val_463
+NULL	NULL	466	val_466
+NULL	NULL	466	val_466
+NULL	NULL	466	val_466
+NULL	NULL	467	val_467
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	470	val_470
+NULL	NULL	472	val_472
+NULL	NULL	475	val_475
+NULL	NULL	477	val_477
+NULL	NULL	478	val_478
+NULL	NULL	478	val_478
+NULL	NULL	479	val_479
+NULL	NULL	480	val_480
+NULL	NULL	480	val_480
+NULL	NULL	480	val_480
+NULL	NULL	481	val_481
+NULL	NULL	482	val_482
+NULL	NULL	483	val_483
+NULL	NULL	484	val_484
+NULL	NULL	485	val_485
+NULL	NULL	487	val_487
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	490	val_490
+NULL	NULL	491	val_491
+NULL	NULL	492	val_492
+NULL	NULL	492	val_492
+NULL	NULL	493	val_493
+NULL	NULL	494	val_494
+NULL	NULL	495	val_495
+NULL	NULL	496	val_496
+NULL	NULL	497	val_497
+NULL	NULL	498	val_498
+NULL	NULL	498	val_498
+NULL	NULL	498	val_498
+PREHOOK: query: explain
 insert overwrite table smb_join_results
 select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:Explain, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): explain
+PREHOOK: type: QUERY
+POSTHOOK: query: explain
 insert overwrite table smb_join_results
 select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-INFO  : Starting task [Stage-9:EXPLAIN] in serial mode
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query explain
-insert overwrite table smb_join_results
-select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-'Explain'
-'STAGE DEPENDENCIES:'
-'  Stage-1 is a root stage'
-'  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6'
-'  Stage-5'
-'  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7'
-'  Stage-3 depends on stages: Stage-0'
-'  Stage-4'
-'  Stage-6'
-'  Stage-7 depends on stages: Stage-6'
-''
-'STAGE PLANS:'
-'  Stage: Stage-1'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            alias: b'
-'            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE'
-'            Sorted Merge Bucket Map Join Operator'
-'              condition map:'
-'                   Outer Join 0 to 1'
-'              keys:'
-'                0 key (type: int)'
-'                1 key (type: int)'
-'              outputColumnNames: _col0, _col1, _col5, _col6'
-'              Select Operator'
-'                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)'
-'                outputColumnNames: _col0, _col1, _col2, _col3'
-'                File Output Operator'
-'                  compressed: false'
-'                  table:'
-'                      input format: org.apache.hadoop.mapred.TextInputFormat'
-'                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                      name: smb_mapjoin_7.smb_join_results'
-''
-'  Stage: Stage-8'
-'    Conditional Operator'
-''
-'  Stage: Stage-5'
-'    Move Operator'
-'      files:'
-'          hdfs directory: true'
-'          destination: file:/!!ELIDED!!
-''
-'  Stage: Stage-0'
-'    Move Operator'
-'      tables:'
-'          replace: true'
-'          table:'
-'              input format: org.apache.hadoop.mapred.TextInputFormat'
-'              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'              name: smb_mapjoin_7.smb_join_results'
-''
-'  Stage: Stage-3'
-'    Stats-Aggr Operator'
-''
-'  Stage: Stage-4'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            File Output Operator'
-'              compressed: false'
-'              table:'
-'                  input format: org.apache.hadoop.mapred.TextInputFormat'
-'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  name: smb_mapjoin_7.smb_join_results'
-''
-'  Stage: Stage-6'
-'    Map Reduce'
-'      Map Operator Tree:'
-'          TableScan'
-'            File Output Operator'
-'              compressed: false'
-'              table:'
-'                  input format: org.apache.hadoop.mapred.TextInputFormat'
-'                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
-'                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe'
-'                  name: smb_mapjoin_7.smb_join_results'
-''
-'  Stage: Stage-7'
-'    Move Operator'
-'      files:'
-'          hdfs directory: true'
-'          destination: file:/!!ELIDED!!
-''
-87 rows selected 
->>>  
->>>  insert overwrite table smb_join_results
-select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results
-select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): insert overwrite table smb_join_results
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-8 depends on stages: Stage-1 , consists of Stage-5, Stage-4, Stage-6
+  Stage-5
+  Stage-0 depends on stages: Stage-5, Stage-4, Stage-7
+  Stage-3 depends on stages: Stage-0
+  Stage-4
+  Stage-6
+  Stage-7 depends on stages: Stage-6
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: b
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+            Sorted Merge Bucket Map Join Operator
+              condition map:
+                   Outer Join 0 to 1
+              keys:
+                0 key (type: int)
+                1 key (type: int)
+              outputColumnNames: _col0, _col1, _col5, _col6
+              Select Operator
+                expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
+                outputColumnNames: _col0, _col1, _col2, _col3
+                File Output Operator
+                  compressed: false
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                      name: default.smb_join_results
+
+  Stage: Stage-8
+    Conditional Operator
+
+  Stage: Stage-5
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+  Stage: Stage-0
+    Move Operator
+      tables:
+          replace: true
+          table:
+              input format: org.apache.hadoop.mapred.TextInputFormat
+              output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+              serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+              name: default.smb_join_results
+
+  Stage: Stage-3
+    Stats-Aggr Operator
+
+  Stage: Stage-4
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.smb_join_results
+
+  Stage: Stage-6
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+                  name: default.smb_join_results
+
+  Stage: Stage-7
+    Move Operator
+      files:
+          hdfs directory: true
+#### A masked pattern was here ####
+
+PREHOOK: query: insert overwrite table smb_join_results
 select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : PREHOOK: Output: smb_mapjoin_7@smb_join_results
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 3
-INFO  : Launching Job 1 out of 3
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:2
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-8:CONDITIONAL] in serial mode
-INFO  : Stage-5 is filtered out by condition resolver.
-INFO  : Stage-4 is selected by condition resolver.
-INFO  : Stage-6 is filtered out by condition resolver.
-INFO  : Launching Job 3 out of 3
-INFO  : Starting task [Stage-4:MAPRED] in serial mode
-INFO  : Number of reduce tasks is set to 0 since there's no reduce operator
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.smb_join_results from file:/!!ELIDED!!
-INFO  : Starting task [Stage-3:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.smb_join_results stats: [numFiles=1, numRows=500, totalSize=8812, rawDataSize=8312]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : POSTHOOK: Output: smb_mapjoin_7@smb_join_results
-ERROR : POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-ERROR : POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Stage-Stage-4:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query insert overwrite table smb_join_results
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket4_1
+PREHOOK: Input: default@smb_bucket4_2
+PREHOOK: Output: default@smb_join_results
+POSTHOOK: query: insert overwrite table smb_join_results
 select /*+mapjoin(a)*/ * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-No rows affected 
->>>  
->>>  select * from smb_join_results order by k1, v1, k2, v2;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select * from smb_join_results order by k1, v1, k2, v2
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:smb_join_results.k1, type:int, comment:null), FieldSchema(name:smb_join_results.v1, type:string, comment:null), FieldSchema(name:smb_join_results.k2, type:int, comment:null), FieldSchema(name:smb_join_results.v2, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select * from smb_join_results order by k1, v1, k2, v2
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_join_results
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_join_results
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select * from smb_join_results order by k1, v1, k2, v2
-'smb_join_results.k1','smb_join_results.v1','smb_join_results.k2','smb_join_results.v2'
-'NULL','NULL','0','val_0'
-'NULL','NULL','0','val_0'
-'NULL','NULL','0','val_0'
-'NULL','NULL','2','val_2'
-'NULL','NULL','4','val_4'
-'NULL','NULL','5','val_5'
-'NULL','NULL','5','val_5'
-'NULL','NULL','5','val_5'
-'NULL','NULL','8','val_8'
-'NULL','NULL','9','val_9'
-'NULL','NULL','10','val_10'
-'NULL','NULL','11','val_11'
-'NULL','NULL','12','val_12'
-'NULL','NULL','12','val_12'
-'NULL','NULL','15','val_15'
-'NULL','NULL','15','val_15'
-'NULL','NULL','17','val_17'
-'NULL','NULL','18','val_18'
-'NULL','NULL','18','val_18'
-'NULL','NULL','19','val_19'
-'NULL','NULL','20','val_20'
-'NULL','NULL','24','val_24'
-'NULL','NULL','24','val_24'
-'NULL','NULL','26','val_26'
-'NULL','NULL','26','val_26'
-'NULL','NULL','27','val_27'
-'NULL','NULL','28','val_28'
-'NULL','NULL','30','val_30'
-'NULL','NULL','33','val_33'
-'NULL','NULL','34','val_34'
-'NULL','NULL','35','val_35'
-'NULL','NULL','35','val_35'
-'NULL','NULL','35','val_35'
-'NULL','NULL','37','val_37'
-'NULL','NULL','37','val_37'
-'NULL','NULL','41','val_41'
-'NULL','NULL','42','val_42'
-'NULL','NULL','42','val_42'
-'NULL','NULL','43','val_43'
-'NULL','NULL','44','val_44'
-'NULL','NULL','47','val_47'
-'NULL','NULL','51','val_51'
-'NULL','NULL','51','val_51'
-'NULL','NULL','53','val_53'
-'NULL','NULL','54','val_54'
-'NULL','NULL','57','val_57'
-'NULL','NULL','58','val_58'
-'NULL','NULL','58','val_58'
-'NULL','NULL','64','val_64'
-'NULL','NULL','65','val_65'
-'NULL','NULL','66','val_66'
-'NULL','NULL','67','val_67'
-'NULL','NULL','67','val_67'
-'NULL','NULL','69','val_69'
-'NULL','NULL','70','val_70'
-'NULL','NULL','70','val_70'
-'NULL','NULL','70','val_70'
-'NULL','NULL','72','val_72'
-'NULL','NULL','72','val_72'
-'NULL','NULL','74','val_74'
-'NULL','NULL','76','val_76'
-'NULL','NULL','76','val_76'
-'NULL','NULL','77','val_77'
-'NULL','NULL','78','val_78'
-'NULL','NULL','80','val_80'
-'NULL','NULL','82','val_82'
-'NULL','NULL','83','val_83'
-'NULL','NULL','83','val_83'
-'NULL','NULL','84','val_84'
-'NULL','NULL','84','val_84'
-'NULL','NULL','85','val_85'
-'NULL','NULL','86','val_86'
-'NULL','NULL','87','val_87'
-'NULL','NULL','90','val_90'
-'NULL','NULL','90','val_90'
-'NULL','NULL','90','val_90'
-'NULL','NULL','92','val_92'
-'NULL','NULL','95','val_95'
-'NULL','NULL','95','val_95'
-'NULL','NULL','96','val_96'
-'NULL','NULL','97','val_97'
-'NULL','NULL','97','val_97'
-'NULL','NULL','98','val_98'
-'NULL','NULL','98','val_98'
-'NULL','NULL','100','val_100'
-'NULL','NULL','100','val_100'
-'NULL','NULL','103','val_103'
-'NULL','NULL','103','val_103'
-'NULL','NULL','104','val_104'
-'NULL','NULL','104','val_104'
-'NULL','NULL','105','val_105'
-'NULL','NULL','111','val_111'
-'NULL','NULL','113','val_113'
-'NULL','NULL','113','val_113'
-'NULL','NULL','114','val_114'
-'NULL','NULL','116','val_116'
-'NULL','NULL','118','val_118'
-'NULL','NULL','118','val_118'
-'NULL','NULL','119','val_119'
-'NULL','NULL','119','val_119'
-'NULL','NULL','119','val_119'
-'NULL','NULL','120','val_120'
-'NULL','NULL','120','val_120'
-'NULL','NULL','125','val_125'
-'NULL','NULL','125','val_125'
-'NULL','NULL','126','val_126'
-'NULL','NULL','128','val_128'
-'NULL','NULL','128','val_128'
-'NULL','NULL','128','val_128'
-'NULL','NULL','129','val_129'
-'NULL','NULL','129','val_129'
-'NULL','NULL','131','val_131'
-'NULL','NULL','133','val_133'
-'NULL','NULL','134','val_134'
-'NULL','NULL','134','val_134'
-'NULL','NULL','136','val_136'
-'NULL','NULL','137','val_137'
-'NULL','NULL','137','val_137'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','138','val_138'
-'NULL','NULL','143','val_143'
-'NULL','NULL','145','val_145'
-'NULL','NULL','146','val_146'
-'NULL','NULL','146','val_146'
-'NULL','NULL','149','val_149'
-'NULL','NULL','149','val_149'
-'NULL','NULL','150','val_150'
-'NULL','NULL','152','val_152'
-'NULL','NULL','152','val_152'
-'NULL','NULL','153','val_153'
-'NULL','NULL','155','val_155'
-'NULL','NULL','156','val_156'
-'NULL','NULL','157','val_157'
-'NULL','NULL','158','val_158'
-'NULL','NULL','160','val_160'
-'NULL','NULL','162','val_162'
-'NULL','NULL','163','val_163'
-'NULL','NULL','164','val_164'
-'NULL','NULL','164','val_164'
-'NULL','NULL','165','val_165'
-'NULL','NULL','165','val_165'
-'NULL','NULL','166','val_166'
-'NULL','NULL','167','val_167'
-'NULL','NULL','167','val_167'
-'NULL','NULL','167','val_167'
-'NULL','NULL','168','val_168'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','169','val_169'
-'NULL','NULL','170','val_170'
-'NULL','NULL','172','val_172'
-'NULL','NULL','172','val_172'
-'NULL','NULL','174','val_174'
-'NULL','NULL','174','val_174'
-'NULL','NULL','175','val_175'
-'NULL','NULL','175','val_175'
-'NULL','NULL','176','val_176'
-'NULL','NULL','176','val_176'
-'NULL','NULL','177','val_177'
-'NULL','NULL','178','val_178'
-'NULL','NULL','179','val_179'
-'NULL','NULL','179','val_179'
-'NULL','NULL','180','val_180'
-'NULL','NULL','181','val_181'
-'NULL','NULL','183','val_183'
-'NULL','NULL','186','val_186'
-'NULL','NULL','187','val_187'
-'NULL','NULL','187','val_187'
-'NULL','NULL','187','val_187'
-'NULL','NULL','189','val_189'
-'NULL','NULL','190','val_190'
-'NULL','NULL','191','val_191'
-'NULL','NULL','191','val_191'
-'NULL','NULL','192','val_192'
-'NULL','NULL','193','val_193'
-'NULL','NULL','193','val_193'
-'NULL','NULL','193','val_193'
-'NULL','NULL','194','val_194'
-'NULL','NULL','195','val_195'
-'NULL','NULL','195','val_195'
-'NULL','NULL','196','val_196'
-'NULL','NULL','197','val_197'
-'NULL','NULL','197','val_197'
-'NULL','NULL','199','val_199'
-'NULL','NULL','199','val_199'
-'NULL','NULL','199','val_199'
-'NULL','NULL','200','val_200'
-'NULL','NULL','200','val_200'
-'NULL','NULL','201','val_201'
-'NULL','NULL','202','val_202'
-'NULL','NULL','203','val_203'
-'NULL','NULL','203','val_203'
-'NULL','NULL','205','val_205'
-'NULL','NULL','205','val_205'
-'NULL','NULL','207','val_207'
-'NULL','NULL','207','val_207'
-'NULL','NULL','208','val_208'
-'NULL','NULL','208','val_208'
-'NULL','NULL','208','val_208'
-'NULL','NULL','209','val_209'
-'NULL','NULL','209','val_209'
-'NULL','NULL','213','val_213'
-'NULL','NULL','213','val_213'
-'NULL','NULL','214','val_214'
-'NULL','NULL','216','val_216'
-'NULL','NULL','216','val_216'
-'NULL','NULL','217','val_217'
-'NULL','NULL','217','val_217'
-'NULL','NULL','218','val_218'
-'NULL','NULL','219','val_219'
-'NULL','NULL','219','val_219'
-'NULL','NULL','221','val_221'
-'NULL','NULL','221','val_221'
-'NULL','NULL','222','val_222'
-'NULL','NULL','223','val_223'
-'NULL','NULL','223','val_223'
-'NULL','NULL','224','val_224'
-'NULL','NULL','224','val_224'
-'NULL','NULL','226','val_226'
-'NULL','NULL','228','val_228'
-'NULL','NULL','229','val_229'
-'NULL','NULL','229','val_229'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','230','val_230'
-'NULL','NULL','233','val_233'
-'NULL','NULL','233','val_233'
-'NULL','NULL','235','val_235'
-'NULL','NULL','237','val_237'
-'NULL','NULL','237','val_237'
-'NULL','NULL','238','val_238'
-'NULL','NULL','238','val_238'
-'NULL','NULL','239','val_239'
-'NULL','NULL','239','val_239'
-'NULL','NULL','241','val_241'
-'NULL','NULL','242','val_242'
-'NULL','NULL','242','val_242'
-'NULL','NULL','244','val_244'
-'NULL','NULL','247','val_247'
-'NULL','NULL','248','val_248'
-'NULL','NULL','249','val_249'
-'NULL','NULL','252','val_252'
-'NULL','NULL','255','val_255'
-'NULL','NULL','255','val_255'
-'NULL','NULL','256','val_256'
-'NULL','NULL','256','val_256'
-'NULL','NULL','257','val_257'
-'NULL','NULL','258','val_258'
-'NULL','NULL','260','val_260'
-'NULL','NULL','262','val_262'
-'NULL','NULL','263','val_263'
-'NULL','NULL','265','val_265'
-'NULL','NULL','265','val_265'
-'NULL','NULL','266','val_266'
-'NULL','NULL','272','val_272'
-'NULL','NULL','272','val_272'
-'NULL','NULL','273','val_273'
-'NULL','NULL','273','val_273'
-'NULL','NULL','273','val_273'
-'NULL','NULL','274','val_274'
-'NULL','NULL','275','val_275'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','277','val_277'
-'NULL','NULL','278','val_278'
-'NULL','NULL','278','val_278'
-'NULL','NULL','280','val_280'
-'NULL','NULL','280','val_280'
-'NULL','NULL','281','val_281'
-'NULL','NULL','281','val_281'
-'NULL','NULL','282','val_282'
-'NULL','NULL','282','val_282'
-'NULL','NULL','283','val_283'
-'NULL','NULL','284','val_284'
-'NULL','NULL','285','val_285'
-'NULL','NULL','286','val_286'
-'NULL','NULL','287','val_287'
-'NULL','NULL','288','val_288'
-'NULL','NULL','288','val_288'
-'NULL','NULL','289','val_289'
-'NULL','NULL','291','val_291'
-'NULL','NULL','292','val_292'
-'NULL','NULL','296','val_296'
-'NULL','NULL','298','val_298'
-'NULL','NULL','298','val_298'
-'NULL','NULL','298','val_298'
-'NULL','NULL','302','val_302'
-'NULL','NULL','305','val_305'
-'NULL','NULL','306','val_306'
-'NULL','NULL','307','val_307'
-'NULL','NULL','307','val_307'
-'NULL','NULL','308','val_308'
-'NULL','NULL','309','val_309'
-'NULL','NULL','309','val_309'
-'NULL','NULL','310','val_310'
-'NULL','NULL','311','val_311'
-'NULL','NULL','311','val_311'
-'NULL','NULL','311','val_311'
-'NULL','NULL','315','val_315'
-'NULL','NULL','316','val_316'
-'NULL','NULL','316','val_316'
-'NULL','NULL','316','val_316'
-'NULL','NULL','317','val_317'
-'NULL','NULL','317','val_317'
-'NULL','NULL','318','val_318'
-'NULL','NULL','318','val_318'
-'NULL','NULL','318','val_318'
-'NULL','NULL','321','val_321'
-'NULL','NULL','321','val_321'
-'NULL','NULL','322','val_322'
-'NULL','NULL','322','val_322'
-'NULL','NULL','323','val_323'
-'NULL','NULL','325','val_325'
-'NULL','NULL','325','val_325'
-'NULL','NULL','327','val_327'
-'NULL','NULL','327','val_327'
-'NULL','NULL','327','val_327'
-'NULL','NULL','331','val_331'
-'NULL','NULL','331','val_331'
-'NULL','NULL','332','val_332'
-'NULL','NULL','333','val_333'
-'NULL','NULL','333','val_333'
-'NULL','NULL','335','val_335'
-'NULL','NULL','336','val_336'
-'NULL','NULL','338','val_338'
-'NULL','NULL','339','val_339'
-'NULL','NULL','341','val_341'
-'NULL','NULL','342','val_342'
-'NULL','NULL','342','val_342'
-'NULL','NULL','344','val_344'
-'NULL','NULL','344','val_344'
-'NULL','NULL','345','val_345'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','348','val_348'
-'NULL','NULL','351','val_351'
-'NULL','NULL','353','val_353'
-'NULL','NULL','353','val_353'
-'NULL','NULL','356','val_356'
-'NULL','NULL','360','val_360'
-'NULL','NULL','362','val_362'
-'NULL','NULL','364','val_364'
-'NULL','NULL','365','val_365'
-'NULL','NULL','366','val_366'
-'NULL','NULL','367','val_367'
-'NULL','NULL','367','val_367'
-'NULL','NULL','368','val_368'
-'NULL','NULL','369','val_369'
-'NULL','NULL','369','val_369'
-'NULL','NULL','369','val_369'
-'NULL','NULL','373','val_373'
-'NULL','NULL','374','val_374'
-'NULL','NULL','375','val_375'
-'NULL','NULL','377','val_377'
-'NULL','NULL','378','val_378'
-'NULL','NULL','379','val_379'
-'NULL','NULL','382','val_382'
-'NULL','NULL','382','val_382'
-'NULL','NULL','384','val_384'
-'NULL','NULL','384','val_384'
-'NULL','NULL','384','val_384'
-'NULL','NULL','386','val_386'
-'NULL','NULL','389','val_389'
-'NULL','NULL','392','val_392'
-'NULL','NULL','393','val_393'
-'NULL','NULL','394','val_394'
-'NULL','NULL','395','val_395'
-'NULL','NULL','395','val_395'
-'NULL','NULL','396','val_396'
-'NULL','NULL','396','val_396'
-'NULL','NULL','396','val_396'
-'NULL','NULL','397','val_397'
-'NULL','NULL','397','val_397'
-'NULL','NULL','399','val_399'
-'NULL','NULL','399','val_399'
-'NULL','NULL','400','val_400'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','401','val_401'
-'NULL','NULL','402','val_402'
-'NULL','NULL','403','val_403'
-'NULL','NULL','403','val_403'
-'NULL','NULL','403','val_403'
-'NULL','NULL','404','val_404'
-'NULL','NULL','404','val_404'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','406','val_406'
-'NULL','NULL','407','val_407'
-'NULL','NULL','409','val_409'
-'NULL','NULL','409','val_409'
-'NULL','NULL','409','val_409'
-'NULL','NULL','411','val_411'
-'NULL','NULL','413','val_413'
-'NULL','NULL','413','val_413'
-'NULL','NULL','414','val_414'
-'NULL','NULL','414','val_414'
-'NULL','NULL','417','val_417'
-'NULL','NULL','417','val_417'
-'NULL','NULL','417','val_417'
-'NULL','NULL','418','val_418'
-'NULL','NULL','419','val_419'
-'NULL','NULL','421','val_421'
-'NULL','NULL','424','val_424'
-'NULL','NULL','424','val_424'
-'NULL','NULL','427','val_427'
-'NULL','NULL','429','val_429'
-'NULL','NULL','429','val_429'
-'NULL','NULL','430','val_430'
-'NULL','NULL','430','val_430'
-'NULL','NULL','430','val_430'
-'NULL','NULL','431','val_431'
-'NULL','NULL','431','val_431'
-'NULL','NULL','431','val_431'
-'NULL','NULL','432','val_432'
-'NULL','NULL','435','val_435'
-'NULL','NULL','436','val_436'
-'NULL','NULL','437','val_437'
-'NULL','NULL','438','val_438'
-'NULL','NULL','438','val_438'
-'NULL','NULL','438','val_438'
-'NULL','NULL','439','val_439'
-'NULL','NULL','439','val_439'
-'NULL','NULL','443','val_443'
-'NULL','NULL','444','val_444'
-'NULL','NULL','446','val_446'
-'NULL','NULL','448','val_448'
-'NULL','NULL','449','val_449'
-'NULL','NULL','452','val_452'
-'NULL','NULL','453','val_453'
-'NULL','NULL','454','val_454'
-'NULL','NULL','454','val_454'
-'NULL','NULL','454','val_454'
-'NULL','NULL','455','val_455'
-'NULL','NULL','457','val_457'
-'NULL','NULL','458','val_458'
-'NULL','NULL','458','val_458'
-'NULL','NULL','459','val_459'
-'NULL','NULL','459','val_459'
-'NULL','NULL','460','val_460'
-'NULL','NULL','462','val_462'
-'NULL','NULL','462','val_462'
-'NULL','NULL','463','val_463'
-'NULL','NULL','463','val_463'
-'NULL','NULL','466','val_466'
-'NULL','NULL','466','val_466'
-'NULL','NULL','466','val_466'
-'NULL','NULL','467','val_467'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','468','val_468'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','469','val_469'
-'NULL','NULL','470','val_470'
-'NULL','NULL','472','val_472'
-'NULL','NULL','475','val_475'
-'NULL','NULL','477','val_477'
-'NULL','NULL','478','val_478'
-'NULL','NULL','478','val_478'
-'NULL','NULL','479','val_479'
-'NULL','NULL','480','val_480'
-'NULL','NULL','480','val_480'
-'NULL','NULL','480','val_480'
-'NULL','NULL','481','val_481'
-'NULL','NULL','482','val_482'
-'NULL','NULL','483','val_483'
-'NULL','NULL','484','val_484'
-'NULL','NULL','485','val_485'
-'NULL','NULL','487','val_487'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','489','val_489'
-'NULL','NULL','490','val_490'
-'NULL','NULL','491','val_491'
-'NULL','NULL','492','val_492'
-'NULL','NULL','492','val_492'
-'NULL','NULL','493','val_493'
-'NULL','NULL','494','val_494'
-'NULL','NULL','495','val_495'
-'NULL','NULL','496','val_496'
-'NULL','NULL','497','val_497'
-'NULL','NULL','498','val_498'
-'NULL','NULL','498','val_498'
-'NULL','NULL','498','val_498'
-500 rows selected 
->>>  
->>>  insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:a.key, type:int, comment:null), FieldSchema(name:a.value, type:string, comment:null), FieldSchema(name:b.key, type:int, comment:null), FieldSchema(name:b.value, type:string, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : PREHOOK: Output: smb_mapjoin_7@normal_join_results
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks not specified. Estimated from input data size: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:4
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-INFO  : Starting task [Stage-0:MOVE] in serial mode
-INFO  : Loading data to table smb_mapjoin_7.normal_join_results from file:/!!ELIDED!!
-INFO  : Starting task [Stage-2:STATS] in serial mode
-INFO  : Table smb_mapjoin_7.normal_join_results stats: [numFiles=1, numRows=500, totalSize=8812, rawDataSize=8312]
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_1
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_bucket4_2
-ERROR : POSTHOOK: Output: smb_mapjoin_7@normal_join_results
-ERROR : POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
-ERROR : POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
-ERROR : POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
-No rows affected 
->>>  
->>>  select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:k1, type:bigint, comment:null), FieldSchema(name:k2, type:bigint, comment:null), FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:v2, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@normal_join_results
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@normal_join_results
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
-'k1','k2','v1','v2'
-'0','130091','0','36210398070'
-1 row selected 
->>>  select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:k1, type:bigint, comment:null), FieldSchema(name:k2, type:bigint, comment:null), FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:v2, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_join_results
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_join_results
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
-'k1','k2','v1','v2'
-'0','130091','0','36210398070'
-1 row selected 
->>>  select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable;
-DEBUG : Acquire a monitor for compiling query
-INFO  : Compiling commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
-DEBUG : Encoding valid txns info 9223372036854775807:
-INFO  : Semantic Analysis Completed
-INFO  : Returning Hive schema: Schema(fieldSchemas:[FieldSchema(name:k1, type:bigint, comment:null), FieldSchema(name:k2, type:bigint, comment:null), FieldSchema(name:v1, type:bigint, comment:null), FieldSchema(name:v2, type:bigint, comment:null)], properties:null)
-INFO  : Completed compiling commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : Executing commandqueryId=(!!{queryId}!!): select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
-ERROR : PREHOOK: query: 
-ERROR : PREHOOK: type: SWITCHDATABASE
-ERROR : PREHOOK: Input: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : PREHOOK: Output: file:/!!ELIDED!!
-INFO  : Query ID = !!{queryId}!!
-INFO  : Total jobs = 1
-INFO  : Launching Job 1 out of 1
-INFO  : Starting task [Stage-1:MAPRED] in serial mode
-INFO  : Number of reduce tasks determined at compile time: 1
-INFO  : In order to change the average load for a reducer (in bytes):
-INFO  :   set hive.exec.reducers.bytes.per.reducer=<number>
-INFO  : In order to limit the maximum number of reducers:
-INFO  :   set hive.exec.reducers.max=<number>
-INFO  : In order to set a constant number of reducers:
-INFO  :   set mapreduce.job.reduces=<number>
-DEBUG : Configuring job !!{jobId}}!! with file:/!!ELIDED!! as the submit dir
-DEBUG : adding the following namenodes' delegation tokens:[file:///]
-DEBUG : Creating splits at file:/!!ELIDED!!
-INFO  : number of splits:1
-INFO  : Submitting tokens for job: !!{jobId}}!!
-INFO  : The url to track the job: http://localhost:8080/
-INFO  : Job running in-process (local Hadoop)
-INFO  : Ended Job = !!{jobId}!!
-ERROR : POSTHOOK: query: 
-ERROR : POSTHOOK: type: SWITCHDATABASE
-ERROR : POSTHOOK: Input: smb_mapjoin_7@smb_join_results_empty_bigtable
-ERROR : POSTHOOK: Output: file:/!!ELIDED!!
-INFO  : MapReduce Jobs Launched: 
-INFO  : Stage-Stage-1:  HDFS Read: 0 HDFS Write: 0 SUCCESS
-INFO  : Total MapReduce CPU Time Spent: 0 msec
-INFO  : Completed executing commandqueryId=(!!{queryId}!!); Time taken: !!ELIDED!! seconds
-INFO  : OK
-DEBUG : Shutting down query select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
-'k1','k2','v1','v2'
-'0','130091','0','36210398070'
-1 row selected 
->>>  
->>>  
->>>  
->>>  
->>>  
->>>  
->>>  !record
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket4_1
+POSTHOOK: Input: default@smb_bucket4_2
+POSTHOOK: Output: default@smb_join_results
+POSTHOOK: Lineage: smb_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: smb_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: smb_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: select * from smb_join_results order by k1, v1, k2, v2
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_join_results
+#### A masked pattern was here ####
+POSTHOOK: query: select * from smb_join_results order by k1, v1, k2, v2
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_join_results
+#### A masked pattern was here ####
+NULL	NULL	0	val_0
+NULL	NULL	0	val_0
+NULL	NULL	0	val_0
+NULL	NULL	2	val_2
+NULL	NULL	4	val_4
+NULL	NULL	5	val_5
+NULL	NULL	5	val_5
+NULL	NULL	5	val_5
+NULL	NULL	8	val_8
+NULL	NULL	9	val_9
+NULL	NULL	10	val_10
+NULL	NULL	11	val_11
+NULL	NULL	12	val_12
+NULL	NULL	12	val_12
+NULL	NULL	15	val_15
+NULL	NULL	15	val_15
+NULL	NULL	17	val_17
+NULL	NULL	18	val_18
+NULL	NULL	18	val_18
+NULL	NULL	19	val_19
+NULL	NULL	20	val_20
+NULL	NULL	24	val_24
+NULL	NULL	24	val_24
+NULL	NULL	26	val_26
+NULL	NULL	26	val_26
+NULL	NULL	27	val_27
+NULL	NULL	28	val_28
+NULL	NULL	30	val_30
+NULL	NULL	33	val_33
+NULL	NULL	34	val_34
+NULL	NULL	35	val_35
+NULL	NULL	35	val_35
+NULL	NULL	35	val_35
+NULL	NULL	37	val_37
+NULL	NULL	37	val_37
+NULL	NULL	41	val_41
+NULL	NULL	42	val_42
+NULL	NULL	42	val_42
+NULL	NULL	43	val_43
+NULL	NULL	44	val_44
+NULL	NULL	47	val_47
+NULL	NULL	51	val_51
+NULL	NULL	51	val_51
+NULL	NULL	53	val_53
+NULL	NULL	54	val_54
+NULL	NULL	57	val_57
+NULL	NULL	58	val_58
+NULL	NULL	58	val_58
+NULL	NULL	64	val_64
+NULL	NULL	65	val_65
+NULL	NULL	66	val_66
+NULL	NULL	67	val_67
+NULL	NULL	67	val_67
+NULL	NULL	69	val_69
+NULL	NULL	70	val_70
+NULL	NULL	70	val_70
+NULL	NULL	70	val_70
+NULL	NULL	72	val_72
+NULL	NULL	72	val_72
+NULL	NULL	74	val_74
+NULL	NULL	76	val_76
+NULL	NULL	76	val_76
+NULL	NULL	77	val_77
+NULL	NULL	78	val_78
+NULL	NULL	80	val_80
+NULL	NULL	82	val_82
+NULL	NULL	83	val_83
+NULL	NULL	83	val_83
+NULL	NULL	84	val_84
+NULL	NULL	84	val_84
+NULL	NULL	85	val_85
+NULL	NULL	86	val_86
+NULL	NULL	87	val_87
+NULL	NULL	90	val_90
+NULL	NULL	90	val_90
+NULL	NULL	90	val_90
+NULL	NULL	92	val_92
+NULL	NULL	95	val_95
+NULL	NULL	95	val_95
+NULL	NULL	96	val_96
+NULL	NULL	97	val_97
+NULL	NULL	97	val_97
+NULL	NULL	98	val_98
+NULL	NULL	98	val_98
+NULL	NULL	100	val_100
+NULL	NULL	100	val_100
+NULL	NULL	103	val_103
+NULL	NULL	103	val_103
+NULL	NULL	104	val_104
+NULL	NULL	104	val_104
+NULL	NULL	105	val_105
+NULL	NULL	111	val_111
+NULL	NULL	113	val_113
+NULL	NULL	113	val_113
+NULL	NULL	114	val_114
+NULL	NULL	116	val_116
+NULL	NULL	118	val_118
+NULL	NULL	118	val_118
+NULL	NULL	119	val_119
+NULL	NULL	119	val_119
+NULL	NULL	119	val_119
+NULL	NULL	120	val_120
+NULL	NULL	120	val_120
+NULL	NULL	125	val_125
+NULL	NULL	125	val_125
+NULL	NULL	126	val_126
+NULL	NULL	128	val_128
+NULL	NULL	128	val_128
+NULL	NULL	128	val_128
+NULL	NULL	129	val_129
+NULL	NULL	129	val_129
+NULL	NULL	131	val_131
+NULL	NULL	133	val_133
+NULL	NULL	134	val_134
+NULL	NULL	134	val_134
+NULL	NULL	136	val_136
+NULL	NULL	137	val_137
+NULL	NULL	137	val_137
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	138	val_138
+NULL	NULL	143	val_143
+NULL	NULL	145	val_145
+NULL	NULL	146	val_146
+NULL	NULL	146	val_146
+NULL	NULL	149	val_149
+NULL	NULL	149	val_149
+NULL	NULL	150	val_150
+NULL	NULL	152	val_152
+NULL	NULL	152	val_152
+NULL	NULL	153	val_153
+NULL	NULL	155	val_155
+NULL	NULL	156	val_156
+NULL	NULL	157	val_157
+NULL	NULL	158	val_158
+NULL	NULL	160	val_160
+NULL	NULL	162	val_162
+NULL	NULL	163	val_163
+NULL	NULL	164	val_164
+NULL	NULL	164	val_164
+NULL	NULL	165	val_165
+NULL	NULL	165	val_165
+NULL	NULL	166	val_166
+NULL	NULL	167	val_167
+NULL	NULL	167	val_167
+NULL	NULL	167	val_167
+NULL	NULL	168	val_168
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	169	val_169
+NULL	NULL	170	val_170
+NULL	NULL	172	val_172
+NULL	NULL	172	val_172
+NULL	NULL	174	val_174
+NULL	NULL	174	val_174
+NULL	NULL	175	val_175
+NULL	NULL	175	val_175
+NULL	NULL	176	val_176
+NULL	NULL	176	val_176
+NULL	NULL	177	val_177
+NULL	NULL	178	val_178
+NULL	NULL	179	val_179
+NULL	NULL	179	val_179
+NULL	NULL	180	val_180
+NULL	NULL	181	val_181
+NULL	NULL	183	val_183
+NULL	NULL	186	val_186
+NULL	NULL	187	val_187
+NULL	NULL	187	val_187
+NULL	NULL	187	val_187
+NULL	NULL	189	val_189
+NULL	NULL	190	val_190
+NULL	NULL	191	val_191
+NULL	NULL	191	val_191
+NULL	NULL	192	val_192
+NULL	NULL	193	val_193
+NULL	NULL	193	val_193
+NULL	NULL	193	val_193
+NULL	NULL	194	val_194
+NULL	NULL	195	val_195
+NULL	NULL	195	val_195
+NULL	NULL	196	val_196
+NULL	NULL	197	val_197
+NULL	NULL	197	val_197
+NULL	NULL	199	val_199
+NULL	NULL	199	val_199
+NULL	NULL	199	val_199
+NULL	NULL	200	val_200
+NULL	NULL	200	val_200
+NULL	NULL	201	val_201
+NULL	NULL	202	val_202
+NULL	NULL	203	val_203
+NULL	NULL	203	val_203
+NULL	NULL	205	val_205
+NULL	NULL	205	val_205
+NULL	NULL	207	val_207
+NULL	NULL	207	val_207
+NULL	NULL	208	val_208
+NULL	NULL	208	val_208
+NULL	NULL	208	val_208
+NULL	NULL	209	val_209
+NULL	NULL	209	val_209
+NULL	NULL	213	val_213
+NULL	NULL	213	val_213
+NULL	NULL	214	val_214
+NULL	NULL	216	val_216
+NULL	NULL	216	val_216
+NULL	NULL	217	val_217
+NULL	NULL	217	val_217
+NULL	NULL	218	val_218
+NULL	NULL	219	val_219
+NULL	NULL	219	val_219
+NULL	NULL	221	val_221
+NULL	NULL	221	val_221
+NULL	NULL	222	val_222
+NULL	NULL	223	val_223
+NULL	NULL	223	val_223
+NULL	NULL	224	val_224
+NULL	NULL	224	val_224
+NULL	NULL	226	val_226
+NULL	NULL	228	val_228
+NULL	NULL	229	val_229
+NULL	NULL	229	val_229
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	230	val_230
+NULL	NULL	233	val_233
+NULL	NULL	233	val_233
+NULL	NULL	235	val_235
+NULL	NULL	237	val_237
+NULL	NULL	237	val_237
+NULL	NULL	238	val_238
+NULL	NULL	238	val_238
+NULL	NULL	239	val_239
+NULL	NULL	239	val_239
+NULL	NULL	241	val_241
+NULL	NULL	242	val_242
+NULL	NULL	242	val_242
+NULL	NULL	244	val_244
+NULL	NULL	247	val_247
+NULL	NULL	248	val_248
+NULL	NULL	249	val_249
+NULL	NULL	252	val_252
+NULL	NULL	255	val_255
+NULL	NULL	255	val_255
+NULL	NULL	256	val_256
+NULL	NULL	256	val_256
+NULL	NULL	257	val_257
+NULL	NULL	258	val_258
+NULL	NULL	260	val_260
+NULL	NULL	262	val_262
+NULL	NULL	263	val_263
+NULL	NULL	265	val_265
+NULL	NULL	265	val_265
+NULL	NULL	266	val_266
+NULL	NULL	272	val_272
+NULL	NULL	272	val_272
+NULL	NULL	273	val_273
+NULL	NULL	273	val_273
+NULL	NULL	273	val_273
+NULL	NULL	274	val_274
+NULL	NULL	275	val_275
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	277	val_277
+NULL	NULL	278	val_278
+NULL	NULL	278	val_278
+NULL	NULL	280	val_280
+NULL	NULL	280	val_280
+NULL	NULL	281	val_281
+NULL	NULL	281	val_281
+NULL	NULL	282	val_282
+NULL	NULL	282	val_282
+NULL	NULL	283	val_283
+NULL	NULL	284	val_284
+NULL	NULL	285	val_285
+NULL	NULL	286	val_286
+NULL	NULL	287	val_287
+NULL	NULL	288	val_288
+NULL	NULL	288	val_288
+NULL	NULL	289	val_289
+NULL	NULL	291	val_291
+NULL	NULL	292	val_292
+NULL	NULL	296	val_296
+NULL	NULL	298	val_298
+NULL	NULL	298	val_298
+NULL	NULL	298	val_298
+NULL	NULL	302	val_302
+NULL	NULL	305	val_305
+NULL	NULL	306	val_306
+NULL	NULL	307	val_307
+NULL	NULL	307	val_307
+NULL	NULL	308	val_308
+NULL	NULL	309	val_309
+NULL	NULL	309	val_309
+NULL	NULL	310	val_310
+NULL	NULL	311	val_311
+NULL	NULL	311	val_311
+NULL	NULL	311	val_311
+NULL	NULL	315	val_315
+NULL	NULL	316	val_316
+NULL	NULL	316	val_316
+NULL	NULL	316	val_316
+NULL	NULL	317	val_317
+NULL	NULL	317	val_317
+NULL	NULL	318	val_318
+NULL	NULL	318	val_318
+NULL	NULL	318	val_318
+NULL	NULL	321	val_321
+NULL	NULL	321	val_321
+NULL	NULL	322	val_322
+NULL	NULL	322	val_322
+NULL	NULL	323	val_323
+NULL	NULL	325	val_325
+NULL	NULL	325	val_325
+NULL	NULL	327	val_327
+NULL	NULL	327	val_327
+NULL	NULL	327	val_327
+NULL	NULL	331	val_331
+NULL	NULL	331	val_331
+NULL	NULL	332	val_332
+NULL	NULL	333	val_333
+NULL	NULL	333	val_333
+NULL	NULL	335	val_335
+NULL	NULL	336	val_336
+NULL	NULL	338	val_338
+NULL	NULL	339	val_339
+NULL	NULL	341	val_341
+NULL	NULL	342	val_342
+NULL	NULL	342	val_342
+NULL	NULL	344	val_344
+NULL	NULL	344	val_344
+NULL	NULL	345	val_345
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	348	val_348
+NULL	NULL	351	val_351
+NULL	NULL	353	val_353
+NULL	NULL	353	val_353
+NULL	NULL	356	val_356
+NULL	NULL	360	val_360
+NULL	NULL	362	val_362
+NULL	NULL	364	val_364
+NULL	NULL	365	val_365
+NULL	NULL	366	val_366
+NULL	NULL	367	val_367
+NULL	NULL	367	val_367
+NULL	NULL	368	val_368
+NULL	NULL	369	val_369
+NULL	NULL	369	val_369
+NULL	NULL	369	val_369
+NULL	NULL	373	val_373
+NULL	NULL	374	val_374
+NULL	NULL	375	val_375
+NULL	NULL	377	val_377
+NULL	NULL	378	val_378
+NULL	NULL	379	val_379
+NULL	NULL	382	val_382
+NULL	NULL	382	val_382
+NULL	NULL	384	val_384
+NULL	NULL	384	val_384
+NULL	NULL	384	val_384
+NULL	NULL	386	val_386
+NULL	NULL	389	val_389
+NULL	NULL	392	val_392
+NULL	NULL	393	val_393
+NULL	NULL	394	val_394
+NULL	NULL	395	val_395
+NULL	NULL	395	val_395
+NULL	NULL	396	val_396
+NULL	NULL	396	val_396
+NULL	NULL	396	val_396
+NULL	NULL	397	val_397
+NULL	NULL	397	val_397
+NULL	NULL	399	val_399
+NULL	NULL	399	val_399
+NULL	NULL	400	val_400
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	401	val_401
+NULL	NULL	402	val_402
+NULL	NULL	403	val_403
+NULL	NULL	403	val_403
+NULL	NULL	403	val_403
+NULL	NULL	404	val_404
+NULL	NULL	404	val_404
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	406	val_406
+NULL	NULL	407	val_407
+NULL	NULL	409	val_409
+NULL	NULL	409	val_409
+NULL	NULL	409	val_409
+NULL	NULL	411	val_411
+NULL	NULL	413	val_413
+NULL	NULL	413	val_413
+NULL	NULL	414	val_414
+NULL	NULL	414	val_414
+NULL	NULL	417	val_417
+NULL	NULL	417	val_417
+NULL	NULL	417	val_417
+NULL	NULL	418	val_418
+NULL	NULL	419	val_419
+NULL	NULL	421	val_421
+NULL	NULL	424	val_424
+NULL	NULL	424	val_424
+NULL	NULL	427	val_427
+NULL	NULL	429	val_429
+NULL	NULL	429	val_429
+NULL	NULL	430	val_430
+NULL	NULL	430	val_430
+NULL	NULL	430	val_430
+NULL	NULL	431	val_431
+NULL	NULL	431	val_431
+NULL	NULL	431	val_431
+NULL	NULL	432	val_432
+NULL	NULL	435	val_435
+NULL	NULL	436	val_436
+NULL	NULL	437	val_437
+NULL	NULL	438	val_438
+NULL	NULL	438	val_438
+NULL	NULL	438	val_438
+NULL	NULL	439	val_439
+NULL	NULL	439	val_439
+NULL	NULL	443	val_443
+NULL	NULL	444	val_444
+NULL	NULL	446	val_446
+NULL	NULL	448	val_448
+NULL	NULL	449	val_449
+NULL	NULL	452	val_452
+NULL	NULL	453	val_453
+NULL	NULL	454	val_454
+NULL	NULL	454	val_454
+NULL	NULL	454	val_454
+NULL	NULL	455	val_455
+NULL	NULL	457	val_457
+NULL	NULL	458	val_458
+NULL	NULL	458	val_458
+NULL	NULL	459	val_459
+NULL	NULL	459	val_459
+NULL	NULL	460	val_460
+NULL	NULL	462	val_462
+NULL	NULL	462	val_462
+NULL	NULL	463	val_463
+NULL	NULL	463	val_463
+NULL	NULL	466	val_466
+NULL	NULL	466	val_466
+NULL	NULL	466	val_466
+NULL	NULL	467	val_467
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	468	val_468
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	469	val_469
+NULL	NULL	470	val_470
+NULL	NULL	472	val_472
+NULL	NULL	475	val_475
+NULL	NULL	477	val_477
+NULL	NULL	478	val_478
+NULL	NULL	478	val_478
+NULL	NULL	479	val_479
+NULL	NULL	480	val_480
+NULL	NULL	480	val_480
+NULL	NULL	480	val_480
+NULL	NULL	481	val_481
+NULL	NULL	482	val_482
+NULL	NULL	483	val_483
+NULL	NULL	484	val_484
+NULL	NULL	485	val_485
+NULL	NULL	487	val_487
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	489	val_489
+NULL	NULL	490	val_490
+NULL	NULL	491	val_491
+NULL	NULL	492	val_492
+NULL	NULL	492	val_492
+NULL	NULL	493	val_493
+NULL	NULL	494	val_494
+NULL	NULL	495	val_495
+NULL	NULL	496	val_496
+NULL	NULL	497	val_497
+NULL	NULL	498	val_498
+NULL	NULL	498	val_498
+NULL	NULL	498	val_498
+PREHOOK: query: insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_bucket4_1
+PREHOOK: Input: default@smb_bucket4_2
+PREHOOK: Output: default@normal_join_results
+POSTHOOK: query: insert overwrite table normal_join_results select * from smb_bucket4_1 a full outer join smb_bucket4_2 b on a.key = b.key
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_bucket4_1
+POSTHOOK: Input: default@smb_bucket4_2
+POSTHOOK: Output: default@normal_join_results
+POSTHOOK: Lineage: normal_join_results.k1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: normal_join_results.k2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:key, type:int, comment:null), ]
+POSTHOOK: Lineage: normal_join_results.v1 SIMPLE [(smb_bucket4_1)a.FieldSchema(name:value, type:string, comment:null), ]
+POSTHOOK: Lineage: normal_join_results.v2 SIMPLE [(smb_bucket4_2)b.FieldSchema(name:value, type:string, comment:null), ]
+PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
+PREHOOK: type: QUERY
+PREHOOK: Input: default@normal_join_results
+#### A masked pattern was here ####
+POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from normal_join_results
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@normal_join_results
+#### A masked pattern was here ####
+0	130091	0	36210398070
+PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_join_results
+#### A masked pattern was here ####
+POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_join_results
+#### A masked pattern was here ####
+0	130091	0	36210398070
+PREHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@smb_join_results_empty_bigtable
+#### A masked pattern was here ####
+POSTHOOK: query: select sum(hash(k1)) as k1, sum(hash(k2)) as k2, sum(hash(v1)) as v1, sum(hash(v2)) as v2 from smb_join_results_empty_bigtable
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@smb_join_results_empty_bigtable
+#### A masked pattern was here ####
+0	130091	0	36210398070
diff --git a/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppenderForTest.java b/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppenderForTest.java
new file mode 100644
index 0000000..358794e
--- /dev/null
+++ b/service/src/java/org/apache/hive/service/cli/operation/LogDivertAppenderForTest.java
@@ -0,0 +1,115 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.hive.service.cli.operation;
+
+import org.apache.hadoop.hive.ql.session.OperationLog;
+import org.apache.hive.service.cli.CLIServiceUtils;
+import org.apache.log4j.Appender;
+import org.apache.log4j.ConsoleAppender;
+import org.apache.log4j.Layout;
+import org.apache.log4j.Level;
+import org.apache.log4j.Logger;
+import org.apache.log4j.WriterAppender;
+import org.apache.log4j.spi.Filter;
+import org.apache.log4j.spi.LoggingEvent;
+
+import java.io.CharArrayWriter;
+import java.util.Enumeration;
+
+
+/**
+ * Divert appender to redirect and filter test operation logs to match the output of the original
+ * CLI qtest results.
+ */
+public class LogDivertAppenderForTest extends WriterAppender {
+  private static final Logger LOG = Logger.getLogger(LogDivertAppenderForTest.class.getName());
+  private final OperationManager operationManager;
+  /** This is where the log message will go to */
+  private final CharArrayWriter writer = new CharArrayWriter();
+
+  public LogDivertAppenderForTest(OperationManager operationManager) {
+    setWriter(writer);
+    setName("LogDivertAppenderForTest");
+    this.operationManager = operationManager;
+    addFilter(new LogDivertAppenderForTest.TestFilter());
+    initLayout(false);
+  }
+
+  private void setLayout (boolean isVerbose, Layout lo) {
+    if (isVerbose) {
+      if (lo == null) {
+        lo = CLIServiceUtils.verboseLayout;
+        LOG.info("Cannot find a Layout from a ConsoleAppender. Using default Layout pattern.");
+      }
+    } else {
+      lo = CLIServiceUtils.nonVerboseLayout;
+    }
+    setLayout(lo);
+  }
+
+  private void initLayout(boolean isVerbose) {
+    // There should be a ConsoleAppender. Copy its Layout.
+    Logger root = Logger.getRootLogger();
+    Layout layout = null;
+
+    Enumeration<?> appenders = root.getAllAppenders();
+    while (appenders.hasMoreElements()) {
+      Appender ap = (Appender) appenders.nextElement();
+      if (ap.getClass().equals(ConsoleAppender.class)) {
+        layout = ap.getLayout();
+        break;
+      }
+    }
+    setLayout(isVerbose, layout);
+  }
+
+  /**
+   * A log filter that filters test messages coming from the logger.
+   */
+  private static class TestFilter extends Filter {
+    @Override
+    public int decide(LoggingEvent event) {
+      if (event.getLevel().equals(Level.ERROR) && "SessionState".equals(event.getLoggerName())) {
+        if (event.getRenderedMessage().startsWith("PREHOOK:")
+            || event.getRenderedMessage().startsWith("POSTHOOK:")) {
+          return Filter.ACCEPT;
+        }
+      }
+      return Filter.DENY;
+    }
+  }
+
+  /**
+   * Overrides WriterAppender.subAppend(), which does the real logging. No need
+   * to worry about concurrency since log4j calls this synchronously.
+   */
+  @Override
+  protected void subAppend(LoggingEvent event) {
+    super.subAppend(event);
+    // That should've gone into our writer. Notify the LogContext.
+    String logOutput = writer.toString();
+    writer.reset();
+
+    OperationLog log = operationManager.getOperationLogByThread();
+    if (log == null) {
+      LOG.debug(" ---+++=== Dropped log event from thread " + event.getThreadName());
+      return;
+    }
+    log.writeOperationLogForTest(logOutput);
+  }
+}
diff --git a/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java b/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
index f80f70d..5ab36b0 100644
--- a/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
+++ b/service/src/java/org/apache/hive/service/cli/operation/OperationManager.java
@@ -80,6 +80,13 @@ public synchronized void init(HiveConf hiveConf) {
     } else {
       LOG.debug("Operation level logging is turned off");
     }
+    if (hiveConf.getBoolean(HiveConf.ConfVars.HIVE_IN_TEST.varname,
+        HiveConf.ConfVars.HIVE_IN_TEST.defaultBoolVal)) {
+      // If in test mode, then create the test appender
+      Appender ap = new LogDivertAppenderForTest(this);
+      Logger.getRootLogger().addAppender(ap);
+    }
+
     if (hiveConf.isWebUiQueryInfoCacheEnabled()) {
       historicSqlOperations = new SQLOperationDisplayCache(
         hiveConf.getIntVar(ConfVars.HIVE_SERVER2_WEBUI_MAX_HISTORIC_QUERIES));
diff --git a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index db49521..249224a 100644
--- a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -653,7 +653,8 @@ public void close() throws HiveSQLException {
   }
 
   private void cleanupSessionLogDir() {
-    if (isOperationLogEnabled) {
+    // In case of test, if we might not want to remove the log directory
+    if (isOperationLogEnabled && hiveConf.getBoolVar(ConfVars.HIVE_IN_TEST_REMOVE_LOGS)) {
       try {
         FileUtils.forceDelete(sessionLogDir);
         LOG.info("Operation log session directory is deleted: "
-- 
1.7.9.5


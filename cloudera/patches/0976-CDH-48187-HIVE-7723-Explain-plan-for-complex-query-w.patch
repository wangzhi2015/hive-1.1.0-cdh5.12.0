From 7d8912d3524b13c72fb89fcf7abc7143b47ad98a Mon Sep 17 00:00:00 2001
From: Hari Subramaniyan <harisankar@apache.org>
Date: Fri, 30 Oct 2015 22:51:26 -0700
Subject: [PATCH 0976/1164] CDH-48187 HIVE-7723 : Explain plan for complex
 query with lots of partitions is slow due to
 in-efficient collection used to find a matching
 ReadEntity (Hari Subramaniyan, reviewed by
 Ashutosh Chauhan)

(cherry picked from commit 96cbc9700a010b87028c3f9b6200e12f3d14ca35)

Change-Id: I3faf45cca06d85338b80bdf4ba94e9efd6da571a
---
 ql/src/java/org/apache/hadoop/hive/ql/Driver.java  |    3 +-
 .../org/apache/hadoop/hive/ql/exec/MoveTask.java   |    3 +-
 .../hive/ql/index/HiveIndexQueryContext.java       |   10 +++---
 .../hadoop/hive/ql/optimizer/GenMapRedUtils.java   |    9 ++---
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    1 +
 .../org/apache/hadoop/hive/ql/plan/PlanUtils.java  |   38 ++++++++++++++++++++
 6 files changed, 51 insertions(+), 13 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
index 515a053..c0cc2b7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/Driver.java
@@ -27,6 +27,7 @@
 import java.util.HashMap;
 import java.util.HashSet;
 import java.util.LinkedHashMap;
+import java.util.LinkedHashSet;
 import java.util.LinkedList;
 import java.util.List;
 import java.util.Map;
@@ -1783,7 +1784,7 @@ public int execute(boolean deferClose) throws CommandNeedRetryException {
       // remove incomplete outputs.
       // Some incomplete outputs may be added at the beginning, for eg: for dynamic partitions.
       // remove them
-      HashSet<WriteEntity> remOutputs = new HashSet<WriteEntity>();
+      HashSet<WriteEntity> remOutputs = new LinkedHashSet<WriteEntity>();
       for (WriteEntity output : plan.getOutputs()) {
         if (!output.isComplete()) {
           remOutputs.add(output);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
index 1508160..89704dc 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/MoveTask.java
@@ -69,6 +69,7 @@
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
@@ -426,7 +427,7 @@ public int execute(DriverContext driverContext) {
               // For DP, WriteEntity creation is deferred at this stage so we need to update
               // queryPlan here.
               if (queryPlan.getOutputs() == null) {
-                queryPlan.setOutputs(new HashSet<WriteEntity>());
+                queryPlan.setOutputs(new LinkedHashSet<WriteEntity>());
               }
               queryPlan.getOutputs().add(enty);
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexQueryContext.java b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexQueryContext.java
index 617723e..06e7547 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexQueryContext.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/HiveIndexQueryContext.java
@@ -18,7 +18,7 @@
 package org.apache.hadoop.hive.ql.index;
 
 import java.io.Serializable;
-import java.util.HashSet;
+import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Set;
 
@@ -34,7 +34,7 @@
  */
 public class HiveIndexQueryContext {
 
-  private HashSet<ReadEntity> additionalSemanticInputs; // additional inputs to add to the parse context when
+  private Set<ReadEntity> additionalSemanticInputs; // additional inputs to add to the parse context when
                                                         // merging the index query tasks
   private String indexInputFormat;        // input format to set on the TableScanOperator to activate indexing
   private String indexIntermediateFile;   // name of intermediate file written by the index query for the
@@ -52,12 +52,12 @@ public HiveIndexQueryContext() {
     this.queryTasks = null;
   }
 
-  public HashSet<ReadEntity> getAdditionalSemanticInputs() {
+  public Set<ReadEntity> getAdditionalSemanticInputs() {
     return additionalSemanticInputs;
   }
-  public void addAdditionalSemanticInputs(HashSet<ReadEntity> additionalParseInputs) {
+  public void addAdditionalSemanticInputs(Set<ReadEntity> additionalParseInputs) {
     if (this.additionalSemanticInputs == null) {
-      this.additionalSemanticInputs = new HashSet<ReadEntity>();
+      this.additionalSemanticInputs = new LinkedHashSet<ReadEntity>();
     }
     this.additionalSemanticInputs.addAll(additionalParseInputs);
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index c3c01c6..c29461b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -583,13 +583,9 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
     // this is being read because it is a dependency of a view).
     boolean isDirectRead = (parentViewInfo == null);
 
-    for (Partition part : parts) {
-      if (part.getTable().isPartitioned()) {
-        PlanUtils.addInput(inputs, new ReadEntity(part, parentViewInfo, isDirectRead));
-      } else {
-        PlanUtils.addInput(inputs, new ReadEntity(part.getTable(), parentViewInfo, isDirectRead));
-      }
+    PlanUtils.addPartitionInputs(parts, inputs, parentViewInfo, isDirectRead);
 
+    for (Partition part: parts) {
       // Later the properties have to come from the partition as opposed
       // to from the table in order to support versioning.
       Path[] paths = null;
@@ -693,6 +689,7 @@ public static void setMapWork(MapWork plan, ParseContext parseCtx, Set<ReadEntit
         }
       }
     }
+
     if (emptyInput) {
       parseCtx.getGlobalLimitCtx().disableOpt();
     }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index e17f95a..5a15cd4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -9715,6 +9715,7 @@ private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String
         if (partitions != null) {
           for (Partition partn : partitions) {
             // inputs.add(new ReadEntity(partn)); // is this needed at all?
+	      LOG.info("XXX: adding part: "+partn);
             outputs.add(new WriteEntity(partn, WriteEntity.WriteType.DDL_NO_LOCK));
           }
         }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
index 5f9ea3f..b87576d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/PlanUtils.java
@@ -22,6 +22,7 @@
 import java.util.Collection;
 import java.util.Collections;
 import java.util.Comparator;
+import java.util.HashSet;
 import java.util.LinkedHashMap;
 import java.util.List;
 import java.util.Map;
@@ -51,6 +52,7 @@
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.metadata.HiveStorageHandler;
 import org.apache.hadoop.hive.ql.metadata.HiveUtils;
+import org.apache.hadoop.hive.ql.metadata.Partition;
 import org.apache.hadoop.hive.ql.metadata.Table;
 import org.apache.hadoop.hive.ql.parse.ParseContext;
 import org.apache.hadoop.hive.ql.parse.SemanticAnalyzer;
@@ -950,6 +952,42 @@ public static void addExprToStringBuffer(ExprNodeDesc expr, StringBuffer sb) {
     sb.append(expr.getTypeString());
     sb.append(")");
   }
+  
+  public static void addPartitionInputs(Collection<Partition> parts, Collection<ReadEntity> inputs,
+      ReadEntity parentViewInfo, boolean isDirectRead) {
+    // Store the inputs in a HashMap since we can't get a ReadEntity from inputs since it is
+    // implemented as a set.ReadEntity is used as the key so that the HashMap has the same behavior
+    // of equals and hashCode
+    Map<ReadEntity, ReadEntity> readEntityMap =
+        new LinkedHashMap<ReadEntity, ReadEntity>(inputs.size());
+    for (ReadEntity input : inputs) {
+      readEntityMap.put(input, input);
+    }
+
+    for (Partition part : parts) {
+      ReadEntity newInput = null;
+      if (part.getTable().isPartitioned()) {
+        newInput = new ReadEntity(part, parentViewInfo, isDirectRead);
+      } else {
+        newInput = new ReadEntity(part.getTable(), parentViewInfo, isDirectRead);
+      }
+
+      if (readEntityMap.containsKey(newInput)) {
+        ReadEntity input = readEntityMap.get(newInput);
+        if ((newInput.getParents() != null) && (!newInput.getParents().isEmpty())) {
+          input.getParents().addAll(newInput.getParents());
+          input.setDirect(input.isDirect() || newInput.isDirect());
+        }
+      } else {
+        readEntityMap.put(newInput, newInput);
+      }
+    }
+
+    // Add the new ReadEntity that were added to readEntityMap in PlanUtils.addInput
+    if (inputs.size() != readEntityMap.size()) {
+      inputs.addAll(readEntityMap.keySet());
+    }
+  }
 
   public static void addInputsForView(ParseContext parseCtx) throws HiveException {
     Set<ReadEntity> inputs = parseCtx.getSemanticInputs();
-- 
1.7.9.5


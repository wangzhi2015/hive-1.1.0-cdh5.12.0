From 497dd5bb2f68e780eae02933633fbaaba1d5be40 Mon Sep 17 00:00:00 2001
From: Sergio Pena <sergio.pena@cloudera.com>
Date: Mon, 30 Mar 2015 14:27:10 -0500
Subject: [PATCH 0103/1164] CDH-26015: HIVE-10086: Parquet file using column
 index access is throwing error in Hive

---
 data/files/HiveGroup.parquet                       |  Bin 0 -> 693 bytes
 .../convert/DataWritableRecordConverter.java       |    3 +-
 .../io/parquet/read/DataWritableReadSupport.java   |  223 ++++++++++++++------
 .../parquet/read/ParquetRecordReaderWrapper.java   |    2 +-
 .../clientpositive/parquet_schema_evolution.q      |   27 +++
 .../clientpositive/parquet_table_with_subschema.q  |   13 ++
 .../clientpositive/parquet_schema_evolution.q.out  |  141 +++++++++++++
 .../parquet_table_with_subschema.q.out             |   47 +++++
 8 files changed, 391 insertions(+), 65 deletions(-)
 create mode 100644 data/files/HiveGroup.parquet
 create mode 100644 ql/src/test/queries/clientpositive/parquet_schema_evolution.q
 create mode 100644 ql/src/test/queries/clientpositive/parquet_table_with_subschema.q
 create mode 100644 ql/src/test/results/clientpositive/parquet_schema_evolution.q.out
 create mode 100644 ql/src/test/results/clientpositive/parquet_table_with_subschema.q.out

diff --git a/data/files/HiveGroup.parquet b/data/files/HiveGroup.parquet
new file mode 100644
index 0000000000000000000000000000000000000000..7d1c1aa2e49ae4c0fca2938ed5d0558f61b12ade
GIT binary patch
literal 693
zcmZXSv2MaJ5QZHmq>&06wh}c$>43%1A`pUhU_o6vA_l}zQKiH{h6L&WL)B;Kh^mf#
zf<8l^tY-(Pfg$$U{{H_iXZL%9CPAc3$~Oc<U=zo8EdwExj}#(=;WG`n2}FfdG_){S
z%(!s#NrMXjt0PcQLN$f;c`eQ5sCZMFV<gwWHb_HbOAm{5CU}+wy$!Fs$4gyZ0+S;a
ztO62fR)zb!R;T0UlP;B&sxyMU_s7e5yy4H;WX_Y2*ofdNc5xgbQiQ-t!-Q86YGd~z
zj3Qasp0U$)^1?;V+1W~Pz5-Io%(r+6F~y$mJu%0JL~X}Lo-`(~O^0;IobFlM+1Mw=
z9ja0*IHD07cXu7v8_#}?gLQ3R->xpD9MKUlkmuwE*cUz|`WNQ^poCk4ZVjNiCpU1i
zR9$61=DC^dfO4ABDF9IY*=^7di(*u}T883v%~!XH2sT_~@gfbH^-jGZCN1<0Pp}UU
F_7A9gc)tJu

literal 0
HcmV?d00001

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
index a43661e..e9d1131 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/convert/DataWritableRecordConverter.java
@@ -18,7 +18,6 @@
 import parquet.io.api.GroupConverter;
 import parquet.io.api.RecordMaterializer;
 import parquet.schema.GroupType;
-import parquet.schema.MessageType;
 import parquet.schema.MessageTypeParser;
 
 import java.util.Map;
@@ -34,7 +33,7 @@
 
   public DataWritableRecordConverter(final GroupType requestedSchema, final Map<String, String> metadata) {
     this.root = new HiveStructConverter(requestedSchema,
-      MessageTypeParser.parseMessageType(metadata.get(DataWritableReadSupport.HIVE_SCHEMA_KEY)), metadata);
+      MessageTypeParser.parseMessageType(metadata.get(DataWritableReadSupport.HIVE_TABLE_AS_PARQUET_SCHEMA)), metadata);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
index 57ae7a9..0e60f6e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/DataWritableReadSupport.java
@@ -16,6 +16,7 @@
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
+import java.util.ListIterator;
 import java.util.Map;
 
 import org.apache.hadoop.conf.Configuration;
@@ -24,17 +25,20 @@
 import org.apache.hadoop.hive.ql.io.parquet.convert.DataWritableRecordConverter;
 import org.apache.hadoop.hive.ql.metadata.VirtualColumn;
 import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
+import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
+import org.apache.hadoop.hive.serde2.typeinfo.StructTypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfo;
+import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoUtils;
 import org.apache.hadoop.io.ArrayWritable;
 import org.apache.hadoop.util.StringUtils;
 
-import parquet.column.ColumnDescriptor;
 import parquet.hadoop.api.ReadSupport;
 import parquet.io.api.RecordMaterializer;
+import parquet.schema.GroupType;
 import parquet.schema.MessageType;
-import parquet.schema.PrimitiveType;
-import parquet.schema.PrimitiveType.PrimitiveTypeName;
 import parquet.schema.Type;
-import parquet.schema.Type.Repetition;
+import parquet.schema.Types;
+import parquet.schema.PrimitiveType.PrimitiveTypeName;
 
 /**
  *
@@ -45,8 +49,7 @@
  */
 public class DataWritableReadSupport extends ReadSupport<ArrayWritable> {
 
-  private static final String TABLE_SCHEMA = "table_schema";
-  public static final String HIVE_SCHEMA_KEY = "HIVE_TABLE_SCHEMA";
+  public static final String HIVE_TABLE_AS_PARQUET_SCHEMA = "HIVE_TABLE_SCHEMA";
   public static final String PARQUET_COLUMN_INDEX_ACCESS = "parquet.column.index.access";
 
   /**
@@ -56,80 +59,176 @@
    * @param columns comma separated list of columns
    * @return list with virtual columns removed
    */
-  private static List<String> getColumns(final String columns) {
+  private static List<String> getColumnNames(final String columns) {
     return (List<String>) VirtualColumn.
         removeVirtualColumns(StringUtils.getStringCollection(columns));
   }
 
   /**
+   * Returns a list of TypeInfo objects from a string which contains column
+   * types strings.
    *
-   * It creates the readContext for Parquet side with the requested schema during the init phase.
+   * @param types Comma separated list of types
+   * @return A list of TypeInfo objects.
+   */
+  private static List<TypeInfo> getColumnTypes(final String types) {
+    return TypeInfoUtils.getTypeInfosFromTypeString(types);
+  }
+
+  /**
+   * Searchs for a fieldName into a parquet GroupType by ignoring string case.
+   * GroupType#getType(String fieldName) is case sensitive, so we use this method.
    *
-   * @param configuration needed to get the wanted columns
-   * @param keyValueMetaData // unused
-   * @param fileSchema parquet file schema
-   * @return the parquet ReadContext
+   * @param groupType Group of field types where to search for fieldName
+   * @param fieldName The field what we are searching
+   * @return The Type object of the field found; null otherwise.
    */
-  @Override
-  public parquet.hadoop.api.ReadSupport.ReadContext init(final Configuration configuration,
-      final Map<String, String> keyValueMetaData, final MessageType fileSchema) {
-    final String columns = configuration.get(IOConstants.COLUMNS);
-    final Map<String, String> contextMetadata = new HashMap<String, String>();
-    final boolean indexAccess = configuration.getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false);
-    if (columns != null) {
-      final List<String> listColumns = getColumns(columns);
-      final Map<String, String> lowerCaseFileSchemaColumns = new HashMap<String,String>();
-      for (ColumnDescriptor c : fileSchema.getColumns()) {
-        lowerCaseFileSchemaColumns.put(c.getPath()[0].toLowerCase(), c.getPath()[0]);
+  private static Type getFieldTypeIgnoreCase(GroupType groupType, String fieldName) {
+    for (Type type : groupType.getFields()) {
+      if (type.getName().equalsIgnoreCase(fieldName)) {
+        return type;
       }
-      final List<Type> typeListTable = new ArrayList<Type>();
-      if(indexAccess) {
-        for (int index = 0; index < listColumns.size(); index++) {
-          //Take columns based on index or pad the field
-          if(index < fileSchema.getFieldCount()) {
-            typeListTable.add(fileSchema.getType(index));
-          } else {
-            //prefixing with '_mask_' to ensure no conflict with named
-            //columns in the file schema
-            typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, "_mask_"+listColumns.get(index)));
+    }
+
+    return null;
+  }
+
+  /**
+   * Searchs column names by name on a given Parquet schema, and returns its corresponded
+   * Parquet schema types.
+   *
+   * @param schema Group schema where to search for column names.
+   * @param colNames List of column names.
+   * @param colTypes List of column types.
+   * @return List of GroupType objects of projected columns.
+   */
+  private static List<Type> getProjectedGroupFields(GroupType schema, List<String> colNames, List<TypeInfo> colTypes) {
+    List<Type> schemaTypes = new ArrayList<Type>();
+
+    ListIterator columnIterator = colNames.listIterator();
+    while (columnIterator.hasNext()) {
+      TypeInfo colType = colTypes.get(columnIterator.nextIndex());
+      String colName = (String) columnIterator.next();
+
+      Type fieldType = getFieldTypeIgnoreCase(schema, colName);
+      if (fieldType != null) {
+        if (colType.getCategory() == ObjectInspector.Category.STRUCT) {
+          if (fieldType.isPrimitive()) {
+            throw new IllegalStateException("Invalid schema data type, found: PRIMITIVE, expected: STRUCT");
           }
+
+          GroupType groupFieldType = fieldType.asGroupType();
+
+          List<Type> groupFields = getProjectedGroupFields(
+              groupFieldType,
+              ((StructTypeInfo) colType).getAllStructFieldNames(),
+              ((StructTypeInfo) colType).getAllStructFieldTypeInfos()
+          );
+
+          Type[] typesArray = groupFields.toArray(new Type[0]);
+          schemaTypes.add(Types.buildGroup(groupFieldType.getRepetition())
+              .addFields(typesArray)
+              .named(fieldType.getName())
+          );
+        } else {
+          schemaTypes.add(fieldType);
         }
       } else {
-        for (String col : listColumns) {
-          col = col.toLowerCase();
-          // listColumns contains partition columns which are metadata only
-          if (lowerCaseFileSchemaColumns.containsKey(col)) {
-            typeListTable.add(fileSchema.getType(lowerCaseFileSchemaColumns.get(col)));
-          } else {
-            // below allows schema evolution
-            typeListTable.add(new PrimitiveType(Repetition.OPTIONAL, PrimitiveTypeName.BINARY, col));
-          }
+        // Add type for schema evolution
+        schemaTypes.add(Types.optional(PrimitiveTypeName.BINARY).named(colName));
+      }
+    }
+
+    return schemaTypes;
+  }
+
+  /**
+   * Searchs column names by name on a given Parquet message schema, and returns its projected
+   * Parquet schema types.
+   *
+   * @param schema Message type schema where to search for column names.
+   * @param colNames List of column names.
+   * @param colTypes List of column types.
+   * @return A MessageType object of projected columns.
+   */
+  private static MessageType getSchemaByName(MessageType schema, List<String> colNames, List<TypeInfo> colTypes) {
+    List<Type> projectedFields = getProjectedGroupFields(schema, colNames, colTypes);
+    Type[] typesArray = projectedFields.toArray(new Type[0]);
+
+    return Types.buildMessage()
+        .addFields(typesArray)
+        .named(schema.getName());
+  }
+
+  /**
+   * Searchs column names by index on a given Parquet file schema, and returns its corresponded
+   * Parquet schema types.
+   *
+   * @param schema Message schema where to search for column names.
+   * @param colNames List of column names.
+   * @param colIndexes List of column indexes.
+   * @return A MessageType object of the column names found.
+   */
+  private static MessageType getSchemaByIndex(MessageType schema, List<String> colNames, List<Integer> colIndexes) {
+    List<Type> schemaTypes = new ArrayList<Type>();
+
+    for (Integer i : colIndexes) {
+      if (i < colNames.size()) {
+        if (i < schema.getFieldCount()) {
+          schemaTypes.add(schema.getType(i));
+        } else {
+          //prefixing with '_mask_' to ensure no conflict with named
+          //columns in the file schema
+          schemaTypes.add(Types.optional(PrimitiveTypeName.BINARY).named("_mask_" + colNames.get(i)));
         }
       }
-      MessageType tableSchema = new MessageType(TABLE_SCHEMA, typeListTable);
-      contextMetadata.put(HIVE_SCHEMA_KEY, tableSchema.toString());
-
-      final List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);
-
-      final List<Type> typeListWanted = new ArrayList<Type>();
-
-      for (final Integer idx : indexColumnsWanted) {
-        if (idx < listColumns.size()) {
-          String col = listColumns.get(idx);
-          if (indexAccess) {
-              typeListWanted.add(fileSchema.getFields().get(idx));
-          } else {
-            col = col.toLowerCase();
-            if (lowerCaseFileSchemaColumns.containsKey(col)) {
-              typeListWanted.add(tableSchema.getType(lowerCaseFileSchemaColumns.get(col)));
-            }
-          }
+    }
+
+    return new MessageType(schema.getName(), schemaTypes);
+  }
+
+  /**
+   * It creates the readContext for Parquet side with the requested schema during the init phase.
+   *
+   * @param configuration    needed to get the wanted columns
+   * @param keyValueMetaData // unused
+   * @param fileSchema       parquet file schema
+   * @return the parquet ReadContext
+   */
+  @Override
+  public ReadContext init(final Configuration configuration, final Map<String, String> keyValueMetaData, final MessageType fileSchema) {
+    String columnNames = configuration.get(IOConstants.COLUMNS);
+    Map<String, String> contextMetadata = new HashMap<String, String>();
+    boolean indexAccess = configuration.getBoolean(PARQUET_COLUMN_INDEX_ACCESS, false);
+
+    if (columnNames != null) {
+      List<String> columnNamesList = getColumnNames(columnNames);
+
+      MessageType tableSchema;
+      if (indexAccess) {
+        List<Integer> indexSequence = new ArrayList<Integer>();
+
+        // Generates a sequence list of indexes
+        for(int i = 0; i < columnNamesList.size(); i++) {
+          indexSequence.add(i);
         }
+
+        tableSchema = getSchemaByIndex(fileSchema, columnNamesList, indexSequence);
+      } else {
+        String columnTypes = configuration.get(IOConstants.COLUMNS_TYPES);
+        List<TypeInfo> columnTypesList = getColumnTypes(columnTypes);
+
+        tableSchema = getSchemaByName(fileSchema, columnNamesList, columnTypesList);
       }
-      MessageType requestedSchemaByUser = new MessageType(fileSchema.getName(), typeListWanted);
+
+      contextMetadata.put(HIVE_TABLE_AS_PARQUET_SCHEMA, tableSchema.toString());
+
+      List<Integer> indexColumnsWanted = ColumnProjectionUtils.getReadColumnIDs(configuration);
+      MessageType requestedSchemaByUser = getSchemaByIndex(tableSchema, columnNamesList, indexColumnsWanted);
+
       return new ReadContext(requestedSchemaByUser, contextMetadata);
     } else {
-      contextMetadata.put(HIVE_SCHEMA_KEY, fileSchema.toString());
+      contextMetadata.put(HIVE_TABLE_AS_PARQUET_SCHEMA, fileSchema.toString());
       return new ReadContext(fileSchema, contextMetadata);
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
index a261996..b99fd56 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/parquet/read/ParquetRecordReaderWrapper.java
@@ -246,7 +246,7 @@ protected ParquetInputSplit getSplit(
       final ReadContext readContext = new DataWritableReadSupport()
           .init(jobConf, fileMetaData.getKeyValueMetaData(), fileMetaData.getSchema());
       schemaSize = MessageTypeParser.parseMessageType(readContext.getReadSupportMetadata()
-          .get(DataWritableReadSupport.HIVE_SCHEMA_KEY)).getFieldCount();
+          .get(DataWritableReadSupport.HIVE_TABLE_AS_PARQUET_SCHEMA)).getFieldCount();
       final List<BlockMetaData> splitGroup = new ArrayList<BlockMetaData>();
       final long splitStart = ((FileSplit) oldSplit).getStart();
       final long splitLength = ((FileSplit) oldSplit).getLength();
diff --git a/ql/src/test/queries/clientpositive/parquet_schema_evolution.q b/ql/src/test/queries/clientpositive/parquet_schema_evolution.q
new file mode 100644
index 0000000..af0cf99
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/parquet_schema_evolution.q
@@ -0,0 +1,27 @@
+-- Some tables might have extra columns and struct elements on the schema than the on Parquet schema;
+-- This is called 'schema evolution' as the Parquet file is not ready yet for such new columns;
+-- Hive should support this schema, and return NULL values instead;
+
+DROP TABLE NewStructField;
+DROP TABLE NewStructFieldTable;
+
+CREATE TABLE NewStructField(a struct<a1:map<string,string>, a2:struct<e1:int>>) STORED AS PARQUET;
+
+INSERT OVERWRITE TABLE NewStructField SELECT named_struct('a1', map('k1','v1'), 'a2', named_struct('e1',5)) FROM srcpart LIMIT 5;
+
+DESCRIBE NewStructField;
+SELECT * FROM NewStructField;
+
+-- Adds new fields to the struct types
+ALTER TABLE NewStructField REPLACE COLUMNS (a struct<a1:map<string,string>, a2:struct<e1:int,e2:string>, a3:int>, b int);
+
+DESCRIBE NewStructField;
+SELECT * FROM NewStructField;
+
+-- Makes sure that new parquet tables contain the new struct field
+CREATE TABLE NewStructFieldTable STORED AS PARQUET AS SELECT * FROM NewStructField;
+DESCRIBE NewStructFieldTable;
+SELECT * FROM NewStructFieldTable;
+
+DROP TABLE NewStructField;
+DROP TABLE NewStructFieldTable;
\ No newline at end of file
diff --git a/ql/src/test/queries/clientpositive/parquet_table_with_subschema.q b/ql/src/test/queries/clientpositive/parquet_table_with_subschema.q
new file mode 100644
index 0000000..7c1b7b5
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/parquet_table_with_subschema.q
@@ -0,0 +1,13 @@
+-- Sometimes, the user wants to create a table from just a portion of the file schema;
+-- This test makes sure that this scenario works;
+
+DROP TABLE test;
+
+-- Current file schema is: (id int, name string, address struct<number:int,street:string,zip:string>);
+-- Creates a table from just a portion of the file schema, including struct elements (test lower/upper case as well)
+CREATE TABLE test (Name string, address struct<Zip:string,Street:string>) STORED AS PARQUET;
+
+LOAD DATA LOCAL INPATH '../../data/files/HiveGroup.parquet' OVERWRITE INTO TABLE test;
+SELECT * FROM test;
+
+DROP TABLE test;
\ No newline at end of file
diff --git a/ql/src/test/results/clientpositive/parquet_schema_evolution.q.out b/ql/src/test/results/clientpositive/parquet_schema_evolution.q.out
new file mode 100644
index 0000000..4b0711e
--- /dev/null
+++ b/ql/src/test/results/clientpositive/parquet_schema_evolution.q.out
@@ -0,0 +1,141 @@
+PREHOOK: query: -- Some tables might have extra columns and struct elements on the schema than the on Parquet schema;
+-- This is called 'schema evolution' as the Parquet file is not ready yet for such new columns;
+-- Hive should support this schema, and return NULL values instead;
+
+DROP TABLE NewStructField
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: -- Some tables might have extra columns and struct elements on the schema than the on Parquet schema;
+-- This is called 'schema evolution' as the Parquet file is not ready yet for such new columns;
+-- Hive should support this schema, and return NULL values instead;
+
+DROP TABLE NewStructField
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: DROP TABLE NewStructFieldTable
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE NewStructFieldTable
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: CREATE TABLE NewStructField(a struct<a1:map<string,string>, a2:struct<e1:int>>) STORED AS PARQUET
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@NewStructField
+POSTHOOK: query: CREATE TABLE NewStructField(a struct<a1:map<string,string>, a2:struct<e1:int>>) STORED AS PARQUET
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@NewStructField
+PREHOOK: query: INSERT OVERWRITE TABLE NewStructField SELECT named_struct('a1', map('k1','v1'), 'a2', named_struct('e1',5)) FROM srcpart LIMIT 5
+PREHOOK: type: QUERY
+PREHOOK: Input: default@srcpart
+PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
+PREHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
+PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
+PREHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
+PREHOOK: Output: default@newstructfield
+POSTHOOK: query: INSERT OVERWRITE TABLE NewStructField SELECT named_struct('a1', map('k1','v1'), 'a2', named_struct('e1',5)) FROM srcpart LIMIT 5
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@srcpart
+POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=11
+POSTHOOK: Input: default@srcpart@ds=2008-04-08/hr=12
+POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=11
+POSTHOOK: Input: default@srcpart@ds=2008-04-09/hr=12
+POSTHOOK: Output: default@newstructfield
+POSTHOOK: Lineage: newstructfield.a EXPRESSION []
+PREHOOK: query: DESCRIBE NewStructField
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@newstructfield
+POSTHOOK: query: DESCRIBE NewStructField
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@newstructfield
+a                   	struct<a1:map<string,string>,a2:struct<e1:int>>	                    
+PREHOOK: query: SELECT * FROM NewStructField
+PREHOOK: type: QUERY
+PREHOOK: Input: default@newstructfield
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM NewStructField
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@newstructfield
+#### A masked pattern was here ####
+{"a1":{"k1":"v1"},"a2":{"e1":5}}
+{"a1":{"k1":"v1"},"a2":{"e1":5}}
+{"a1":{"k1":"v1"},"a2":{"e1":5}}
+{"a1":{"k1":"v1"},"a2":{"e1":5}}
+{"a1":{"k1":"v1"},"a2":{"e1":5}}
+PREHOOK: query: -- Adds new fields to the struct types
+ALTER TABLE NewStructField REPLACE COLUMNS (a struct<a1:map<string,string>, a2:struct<e1:int,e2:string>, a3:int>, b int)
+PREHOOK: type: ALTERTABLE_REPLACECOLS
+PREHOOK: Input: default@newstructfield
+PREHOOK: Output: default@newstructfield
+POSTHOOK: query: -- Adds new fields to the struct types
+ALTER TABLE NewStructField REPLACE COLUMNS (a struct<a1:map<string,string>, a2:struct<e1:int,e2:string>, a3:int>, b int)
+POSTHOOK: type: ALTERTABLE_REPLACECOLS
+POSTHOOK: Input: default@newstructfield
+POSTHOOK: Output: default@newstructfield
+PREHOOK: query: DESCRIBE NewStructField
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@newstructfield
+POSTHOOK: query: DESCRIBE NewStructField
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@newstructfield
+a                   	struct<a1:map<string,string>,a2:struct<e1:int,e2:string>,a3:int>	                    
+b                   	int                 	                    
+PREHOOK: query: SELECT * FROM NewStructField
+PREHOOK: type: QUERY
+PREHOOK: Input: default@newstructfield
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM NewStructField
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@newstructfield
+#### A masked pattern was here ####
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+PREHOOK: query: -- Makes sure that new parquet tables contain the new struct field
+CREATE TABLE NewStructFieldTable STORED AS PARQUET AS SELECT * FROM NewStructField
+PREHOOK: type: CREATETABLE_AS_SELECT
+PREHOOK: Input: default@newstructfield
+PREHOOK: Output: database:default
+PREHOOK: Output: default@NewStructFieldTable
+POSTHOOK: query: -- Makes sure that new parquet tables contain the new struct field
+CREATE TABLE NewStructFieldTable STORED AS PARQUET AS SELECT * FROM NewStructField
+POSTHOOK: type: CREATETABLE_AS_SELECT
+POSTHOOK: Input: default@newstructfield
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@NewStructFieldTable
+PREHOOK: query: DESCRIBE NewStructFieldTable
+PREHOOK: type: DESCTABLE
+PREHOOK: Input: default@newstructfieldtable
+POSTHOOK: query: DESCRIBE NewStructFieldTable
+POSTHOOK: type: DESCTABLE
+POSTHOOK: Input: default@newstructfieldtable
+a                   	struct<a1:map<string,string>,a2:struct<e1:int,e2:string>,a3:int>	                    
+b                   	int                 	                    
+PREHOOK: query: SELECT * FROM NewStructFieldTable
+PREHOOK: type: QUERY
+PREHOOK: Input: default@newstructfieldtable
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM NewStructFieldTable
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@newstructfieldtable
+#### A masked pattern was here ####
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+{"a1":{"k1":"v1"},"a2":{"e1":5,"e2":null},"a3":null}	NULL
+PREHOOK: query: DROP TABLE NewStructField
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@newstructfield
+PREHOOK: Output: default@newstructfield
+POSTHOOK: query: DROP TABLE NewStructField
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@newstructfield
+POSTHOOK: Output: default@newstructfield
+PREHOOK: query: DROP TABLE NewStructFieldTable
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@newstructfieldtable
+PREHOOK: Output: default@newstructfieldtable
+POSTHOOK: query: DROP TABLE NewStructFieldTable
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@newstructfieldtable
+POSTHOOK: Output: default@newstructfieldtable
diff --git a/ql/src/test/results/clientpositive/parquet_table_with_subschema.q.out b/ql/src/test/results/clientpositive/parquet_table_with_subschema.q.out
new file mode 100644
index 0000000..c6b57f4
--- /dev/null
+++ b/ql/src/test/results/clientpositive/parquet_table_with_subschema.q.out
@@ -0,0 +1,47 @@
+PREHOOK: query: -- Sometimes, the user wants to create a table from just a portion of the file schema;
+-- This test makes sure that this scenario works;
+
+DROP TABLE test
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: -- Sometimes, the user wants to create a table from just a portion of the file schema;
+-- This test makes sure that this scenario works;
+
+DROP TABLE test
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: -- Current file schema is: (id int, name string, address struct<number:int,street:string,zip:string>);
+-- Creates a table from just a portion of the file schema, including struct elements (test lower/upper case as well)
+CREATE TABLE test (Name string, address struct<Zip:string,Street:string>) STORED AS PARQUET
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test
+POSTHOOK: query: -- Current file schema is: (id int, name string, address struct<number:int,street:string,zip:string>);
+-- Creates a table from just a portion of the file schema, including struct elements (test lower/upper case as well)
+CREATE TABLE test (Name string, address struct<Zip:string,Street:string>) STORED AS PARQUET
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test
+PREHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/HiveGroup.parquet' OVERWRITE INTO TABLE test
+PREHOOK: type: LOAD
+#### A masked pattern was here ####
+PREHOOK: Output: default@test
+POSTHOOK: query: LOAD DATA LOCAL INPATH '../../data/files/HiveGroup.parquet' OVERWRITE INTO TABLE test
+POSTHOOK: type: LOAD
+#### A masked pattern was here ####
+POSTHOOK: Output: default@test
+PREHOOK: query: SELECT * FROM test
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT * FROM test
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+Roger	{"Zip":"87366","Street":"Congress Ave."}
+PREHOOK: query: DROP TABLE test
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@test
+PREHOOK: Output: default@test
+POSTHOOK: query: DROP TABLE test
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@test
+POSTHOOK: Output: default@test
-- 
1.7.9.5


From 30ce1cfadd929fab17e7358ab71e6c037bd0df16 Mon Sep 17 00:00:00 2001
From: Ashutosh Chauhan <hashutosh@apache.org>
Date: Wed, 7 Oct 2015 17:45:50 -0700
Subject: [PATCH 1092/1164] CDH-52465: HIVE-12065 : FS stats collection may
 generate incorrect stats for multi-insert query
 (Ashutosh Chauhan via Pengcheng Xiong)

Signed-off-by: Ashutosh Chauhan <hashutosh@apache.org>
(cherry picked from commit 9b4826e765423c4766dbac052127546213ac0752)

Change-Id: I57ead9bb77cd4060be0e390a9bb7e22bd3435bd5
---
 .../hadoop/hive/hbase/HBaseStatsAggregator.java    |    7 +-
 .../hadoop/hive/hbase/HBaseStatsPublisher.java     |   11 ++--
 .../hadoop/hive/ql/stats/DummyStatsAggregator.java |   12 ++--
 .../hadoop/hive/ql/stats/DummyStatsPublisher.java  |   15 +++--
 .../hive/ql/stats/KeyVerifyingStatsAggregator.java |   10 +--
 .../hadoop/hive/ql/exec/FileSinkOperator.java      |    7 +-
 .../org/apache/hadoop/hive/ql/exec/StatsTask.java  |   31 ++++++---
 .../hadoop/hive/ql/exec/TableScanOperator.java     |    7 +-
 .../org/apache/hadoop/hive/ql/exec/Utilities.java  |   30 +++++++++
 .../apache/hadoop/hive/ql/exec/mr/ExecDriver.java  |   14 +++-
 .../hive/ql/exec/spark/SparkPlanGenerator.java     |   16 +++--
 .../apache/hadoop/hive/ql/exec/tez/DagUtils.java   |    5 +-
 .../hive/ql/index/AggregateIndexHandler.java       |    1 -
 .../hive/ql/index/TableBasedIndexHandler.java      |    7 --
 .../hive/ql/index/bitmap/BitmapIndexHandler.java   |    1 -
 .../hive/ql/index/compact/CompactIndexHandler.java |    1 -
 .../hive/ql/io/rcfile/stats/PartialScanMapper.java |    7 +-
 .../hive/ql/io/rcfile/stats/PartialScanTask.java   |   11 +++-
 .../hive/ql/io/rcfile/stats/PartialScanWork.java   |   14 ++++
 .../hadoop/hive/ql/optimizer/GenMRTableScan1.java  |    3 +
 .../hadoop/hive/ql/optimizer/GenMapRedUtils.java   |    2 +-
 .../hadoop/hive/ql/parse/ProcessAnalyzeTable.java  |    4 +-
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    6 +-
 .../ql/parse/spark/SparkProcessAnalyzeTable.java   |    2 +
 .../apache/hadoop/hive/ql/plan/FileSinkDesc.java   |   14 +++-
 .../org/apache/hadoop/hive/ql/plan/StatsWork.java  |   15 +++--
 .../apache/hadoop/hive/ql/plan/TableScanDesc.java  |   11 +++-
 .../hive/ql/stats/CounterStatsAggregator.java      |    8 +--
 .../hive/ql/stats/CounterStatsAggregatorSpark.java |    6 +-
 .../hive/ql/stats/CounterStatsAggregatorTez.java   |   10 ++-
 .../hive/ql/stats/CounterStatsPublisher.java       |    7 +-
 .../hadoop/hive/ql/stats/StatsAggregator.java      |    7 +-
 .../hive/ql/stats/StatsCollectionContext.java      |   63 ++++++++++++++++++
 .../hadoop/hive/ql/stats/StatsPublisher.java       |    8 +--
 .../hadoop/hive/ql/stats/fs/FSStatsAggregator.java |   23 ++++---
 .../hadoop/hive/ql/stats/fs/FSStatsPublisher.java  |   32 +++++----
 .../hive/ql/stats/jdbc/JDBCStatsAggregator.java    |   18 +++---
 .../hive/ql/stats/jdbc/JDBCStatsPublisher.java     |   22 ++++---
 .../hadoop/hive/ql/exec/TestFileSinkOperator.java  |   13 ++--
 .../hive/ql/exec/TestStatsPublisherEnhanced.java   |   61 ++++++++++--------
 .../infer_bucket_sort_multi_insert.q               |    1 +
 ql/src/test/queries/clientpositive/multi_insert.q  |    2 +-
 .../queries/clientpositive/multi_insert_gby2.q     |    2 +-
 .../queries/clientpositive/multi_insert_gby3.q     |    2 +-
 .../clientpositive/multi_insert_lateral_view.q     |    1 +
 .../queries/clientpositive/multi_insert_mixed.q    |    2 +-
 .../multi_insert_move_tasks_share_dependencies.q   |    2 +-
 .../clientpositive/multi_insert_union_src.q        |    2 +-
 .../clientpositive/beeline/smb_mapjoin_11.q.out    |   20 +++---
 .../clientpositive/beeline/smb_mapjoin_12.q.out    |    8 +--
 .../clientpositive/beeline/smb_mapjoin_13.q.out    |   40 ++++++------
 .../clientpositive/beeline/smb_mapjoin_16.q.out    |    4 +-
 .../clientpositive/beeline/smb_mapjoin_2.q.out     |    5 +-
 .../clientpositive/spark/column_access_stats.q.out |   46 ++++++-------
 ql/src/test/results/clientpositive/spark/pcr.q.out |   16 ++---
 .../results/clientpositive/spark/ppd_join5.q.out   |   58 ++++++++---------
 .../clientpositive/spark/smb_mapjoin_12.q.out      |    6 +-
 .../clientpositive/spark/smb_mapjoin_13.q.out      |   36 +++++------
 .../clientpositive/spark/smb_mapjoin_15.q.out      |   12 ++--
 .../clientpositive/spark/smb_mapjoin_16.q.out      |    2 +-
 .../results/clientpositive/spark/union34.q.out     |   68 ++++++++++----------
 61 files changed, 533 insertions(+), 344 deletions(-)
 create mode 100644 ql/src/java/org/apache/hadoop/hive/ql/stats/StatsCollectionContext.java

diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsAggregator.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsAggregator.java
index 685afaa..bf4036c 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsAggregator.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsAggregator.java
@@ -35,6 +35,7 @@
 import org.apache.hadoop.hbase.util.Bytes;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 
 
 /**
@@ -48,10 +49,10 @@
   /**
    * Does the necessary HBase initializations.
    */
-  public boolean connect(Configuration hiveconf, Task sourceTask) {
+  public boolean connect(StatsCollectionContext context) {
 
     try {
-      htable = new HTable(HBaseConfiguration.create(hiveconf),
+      htable = new HTable(HBaseConfiguration.create(context.getHiveConf()),
         HBaseStatsSetupConstants.PART_STAT_TABLE_NAME);
 
       return true;
@@ -100,7 +101,7 @@ public String aggregateStats(String rowID, String key) {
     }
   }
 
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext context) {
     if (htable != null) {
       IOUtils.closeQuietly(htable);
       htable = null;
diff --git a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsPublisher.java b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsPublisher.java
index 408cae0..3b05703 100644
--- a/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsPublisher.java
+++ b/hbase-handler/src/java/org/apache/hadoop/hive/hbase/HBaseStatsPublisher.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hbase.client.Put;
 import org.apache.hadoop.hbase.client.Result;
 import org.apache.hadoop.hbase.util.Bytes;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 
 /**
@@ -47,10 +48,10 @@
   /**
    * Does the necessary HBase initializations.
    */
-  public boolean connect(Configuration hiveconf) {
+  public boolean connect(StatsCollectionContext context) {
 
     try {
-      htable = new HTable(HBaseConfiguration.create(hiveconf),
+      htable = new HTable(HBaseConfiguration.create(context.getHiveConf()),
         HBaseStatsSetupConstants.PART_STAT_TABLE_NAME);
       // for performance reason, defer update until the closeConnection
       htable.setAutoFlush(false);
@@ -119,7 +120,7 @@ public boolean publishStat(String rowID, Map<String, String> stats) {
     }
   }
 
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext context) {
     // batch update
     try {
       htable.flushCommits();
@@ -139,9 +140,9 @@ public boolean closeConnection() {
   /**
    * Does the necessary HBase initializations.
    */
-  public boolean init(Configuration hiveconf) {
+  public boolean init(StatsCollectionContext context) {
     try {
-      HBaseAdmin hbase = new HBaseAdmin(HBaseConfiguration.create(hiveconf));
+      HBaseAdmin hbase = new HBaseAdmin(HBaseConfiguration.create(context.getHiveConf()));
 
       // Creating table if not exists
       if (!hbase.tableExists(HBaseStatsSetupConstants.PART_STAT_TABLE_NAME)) {
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
index 327eabc..eb3f6eb 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsAggregator.java
@@ -18,9 +18,7 @@
 
 package org.apache.hadoop.hive.ql.stats;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.Task;
 
 /**
  * An test implementation for StatsAggregator.
@@ -34,8 +32,9 @@
 
   // This is a test. The parameter hive.test.dummystats.aggregator's value
   // denotes the method which needs to throw an error.
-  public boolean connect(Configuration hconf, Task sourceTask) {
-    errorMethod = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVETESTMODEDUMMYSTATAGGR);
+  @Override
+  public boolean connect(StatsCollectionContext scc) {
+    errorMethod = HiveConf.getVar(scc.getHiveConf(), HiveConf.ConfVars.HIVETESTMODEDUMMYSTATAGGR);
     if (errorMethod.equalsIgnoreCase("connect")) {
       return false;
     }
@@ -43,17 +42,20 @@ public boolean connect(Configuration hconf, Task sourceTask) {
     return true;
   }
 
+  @Override
   public String aggregateStats(String keyPrefix, String statType) {
     return null;
   }
 
-  public boolean closeConnection() {
+  @Override
+  public boolean closeConnection(StatsCollectionContext scc) {
     if (errorMethod.equalsIgnoreCase("closeConnection")) {
       return false;
     }
     return true;
   }
 
+  @Override
   public boolean cleanUp(String keyPrefix) {
     if (errorMethod.equalsIgnoreCase("cleanUp")) {
       return false;
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
index 1f6e80f..9f1fdb4 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/DummyStatsPublisher.java
@@ -20,7 +20,6 @@
 
 import java.util.Map;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.conf.HiveConf;
 
 /**
@@ -36,8 +35,9 @@
 
   // This is a test. The parameter hive.test.dummystats.publisher's value
   // denotes the method which needs to throw an error.
-  public boolean init(Configuration hconf) {
-    errorMethod = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVETESTMODEDUMMYSTATPUB);
+  @Override
+  public boolean init(StatsCollectionContext context) {
+    errorMethod = HiveConf.getVar(context.getHiveConf(), HiveConf.ConfVars.HIVETESTMODEDUMMYSTATPUB);
     if (errorMethod.equalsIgnoreCase("init")) {
       return false;
     }
@@ -45,8 +45,9 @@ public boolean init(Configuration hconf) {
     return true;
   }
 
-  public boolean connect(Configuration hconf) {
-    errorMethod = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVETESTMODEDUMMYSTATPUB);
+  @Override
+  public boolean connect(StatsCollectionContext context) {
+    errorMethod = HiveConf.getVar(context.getHiveConf(), HiveConf.ConfVars.HIVETESTMODEDUMMYSTATPUB);
     if (errorMethod.equalsIgnoreCase("connect")) {
       return false;
     }
@@ -54,6 +55,7 @@ public boolean connect(Configuration hconf) {
     return true;
   }
 
+  @Override
   public boolean publishStat(String fileID, Map<String, String> stats) {
     if (errorMethod.equalsIgnoreCase("publishStat")) {
       return false;
@@ -61,7 +63,8 @@ public boolean publishStat(String fileID, Map<String, String> stats) {
     return true;
   }
 
-  public boolean closeConnection() {
+  @Override
+  public boolean closeConnection(StatsCollectionContext context) {
     if (errorMethod.equalsIgnoreCase("closeConnection")) {
       return false;
     }
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
index cb0b584..4e00316 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/ql/stats/KeyVerifyingStatsAggregator.java
@@ -18,8 +18,6 @@
 
 package org.apache.hadoop.hive.ql.stats;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.session.SessionState;
 
 /**
@@ -30,10 +28,12 @@
 
 public class KeyVerifyingStatsAggregator implements StatsAggregator {
 
-  public boolean connect(Configuration hconf, Task sourceTask) {
+  @Override
+  public boolean connect(StatsCollectionContext scc) {
     return true;
   }
 
+  @Override
   public String aggregateStats(String keyPrefix, String statType) {
     SessionState ss = SessionState.get();
     // Have to use the length instead of the actual prefix because the prefix is location dependent
@@ -43,10 +43,12 @@ public String aggregateStats(String keyPrefix, String statType) {
     return null;
   }
 
-  public boolean closeConnection() {
+  @Override
+  public boolean closeConnection(StatsCollectionContext scc) {
     return true;
   }
 
+  @Override
   public boolean cleanUp(String keyPrefix) {
     return true;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
index 750ceae..0bb595d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/FileSinkOperator.java
@@ -57,6 +57,7 @@
 import org.apache.hadoop.hive.ql.plan.SkewedColumnPositionPair;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.serde2.SerDeException;
 import org.apache.hadoop.hive.serde2.SerDeStats;
@@ -1107,7 +1108,9 @@ private void publishStats() throws HiveException {
       return;
     }
 
-    if (!statsPublisher.connect(hconf)) {
+    StatsCollectionContext sContext = new StatsCollectionContext(hconf);
+    sContext.setStatsTmpDir(conf.getStatsTmpDir());
+    if (!statsPublisher.connect(sContext)) {
       // just return, stats gathering should not block the main query
       LOG.error("StatsPublishing error: cannot connect to database");
       if (isStatsReliable) {
@@ -1174,7 +1177,7 @@ private void publishStats() throws HiveException {
         }
       }
     }
-    if (!statsPublisher.closeConnection()) {
+    if (!statsPublisher.closeConnection(sContext)) {
       // The original exception is lost.
       // Not changing the interface to maintain backward compatibility
       if (isStatsReliable) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
index 4576da5..06461de 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/StatsTask.java
@@ -51,6 +51,7 @@
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.util.StringUtils;
@@ -141,13 +142,14 @@ private int aggregateStats() {
 
     StatsAggregator statsAggregator = null;
     int ret = 0;
-
+    StatsCollectionContext scc = null;
     try {
       // Stats setup:
       final Warehouse wh = new Warehouse(conf);
       if (!getWork().getNoStatsAggregator() && !getWork().isNoScanAnalyzeCommand()) {
         try {
-          statsAggregator = createStatsAggregator(conf);
+          scc = getContext();
+          statsAggregator = createStatsAggregator(scc);
         } catch (HiveException e) {
           if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
             throw e;
@@ -294,7 +296,7 @@ public Void call() throws Exception {
       }
     } finally {
       if (statsAggregator != null) {
-        statsAggregator.closeConnection();
+        statsAggregator.closeConnection(scc);
       }
     }
     // The return value of 0 indicates success,
@@ -321,7 +323,7 @@ private String getAggregationPrefix(boolean counter, Table table, Partition part
     return prefix.toString();
   }
 
-  private StatsAggregator createStatsAggregator(HiveConf conf) throws HiveException {
+  private StatsAggregator createStatsAggregator(StatsCollectionContext scc) throws HiveException {
     String statsImpl = HiveConf.getVar(conf, HiveConf.ConfVars.HIVESTATSDBCLASS);
     StatsFactory factory = StatsFactory.newFactory(statsImpl, conf);
     if (factory == null) {
@@ -330,21 +332,30 @@ private StatsAggregator createStatsAggregator(HiveConf conf) throws HiveExceptio
     // initialize stats publishing table for noscan which has only stats task
     // the rest of MR task following stats task initializes it in ExecDriver.java
     StatsPublisher statsPublisher = factory.getStatsPublisher();
-    if (!statsPublisher.init(conf)) { // creating stats table if not exists
+    if (!statsPublisher.init(scc)) { // creating stats table if not exists
       throw new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
     }
-    Task sourceTask = getWork().getSourceTask();
-    if (sourceTask == null) {
-      throw new HiveException(ErrorMsg.STATSAGGREGATOR_SOURCETASK_NULL.getErrorCodedMsg());
-    }
+
     // manufacture a StatsAggregator
     StatsAggregator statsAggregator = factory.getStatsAggregator();
-    if (!statsAggregator.connect(conf, sourceTask)) {
+    if (!statsAggregator.connect(scc)) {
       throw new HiveException(ErrorMsg.STATSAGGREGATOR_CONNECTION_ERROR.getErrorCodedMsg(statsImpl));
     }
     return statsAggregator;
   }
 
+  private StatsCollectionContext getContext() throws HiveException {
+
+    StatsCollectionContext scc = new StatsCollectionContext(conf);
+    Task sourceTask = getWork().getSourceTask();
+    if (sourceTask == null) {
+      throw new HiveException(ErrorMsg.STATSAGGREGATOR_SOURCETASK_NULL.getErrorCodedMsg());
+    }
+    scc.setTask(sourceTask);
+    scc.setStatsTmpDir(this.getWork().getStatsTmpDir());
+    return scc;
+  }
+
   private boolean existStats(Map<String, String> parameters) {
     return parameters.containsKey(StatsSetupConst.ROW_COUNT)
         || parameters.containsKey(StatsSetupConst.NUM_FILES)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
index c55b9e1..e242384 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/TableScanOperator.java
@@ -37,6 +37,7 @@
 import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.ql.plan.api.OperatorType;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
@@ -279,7 +280,9 @@ private void publishStats() throws HiveException {
 
     // Initializing a stats publisher
     StatsPublisher statsPublisher = Utilities.getStatsPublisher(jc);
-    if (!statsPublisher.connect(jc)) {
+    StatsCollectionContext sc = new StatsCollectionContext(jc);
+    sc.setStatsTmpDir(conf.getTmpStatsDir());
+    if (!statsPublisher.connect(sc)) {
       // just return, stats gathering should not block the main query.
       LOG.info("StatsPublishing error: cannot connect to database.");
       if (isStatsReliable) {
@@ -311,7 +314,7 @@ private void publishStats() throws HiveException {
       }
       LOG.info("publishing : " + key + " : " + statsToPublish.toString());
     }
-    if (!statsPublisher.closeConnection()) {
+    if (!statsPublisher.closeConnection(sc)) {
       if (isStatsReliable) {
         throw new HiveException(ErrorMsg.STATSPUBLISHER_CLOSING_ERROR.getErrorCodedMsg());
       }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 5684142..c64e33c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -105,6 +105,7 @@
 import org.apache.hadoop.hive.common.HiveStatsUtils;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.common.StringInternUtils;
+import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.metastore.Warehouse;
@@ -163,6 +164,7 @@
 import org.apache.hadoop.hive.ql.plan.SparkEdgeProperty;
 import org.apache.hadoop.hive.ql.plan.SparkWork;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
+import org.apache.hadoop.hive.ql.plan.TableScanDesc;
 import org.apache.hadoop.hive.ql.plan.api.Adjacency;
 import org.apache.hadoop.hive.ql.plan.api.Graph;
 import org.apache.hadoop.hive.ql.session.SessionState;
@@ -3922,4 +3924,32 @@ public static String getQualifiedPath(HiveConf conf, Path path) throws HiveExcep
   public static boolean isDefaultNameNode(HiveConf conf) {
     return !conf.getChangedProperties().containsKey(HiveConf.ConfVars.HADOOPFS.varname);
   }
+
+  public static List<String> getStatsTmpDirs(BaseWork work, Configuration conf) {
+
+    List<String> statsTmpDirs = new ArrayList<>();
+    if (!StatsSetupConst.StatDB.fs.name().equalsIgnoreCase(HiveConf.getVar(conf, ConfVars.HIVESTATSDBCLASS))) {
+      // no-op for non-fs stats collection
+      return statsTmpDirs;
+    }
+    // if its auto-stats gather for inserts or CTAS, stats dir will be in FileSink
+    Set<Operator<? extends OperatorDesc>> ops = work.getAllLeafOperators();
+    if (work instanceof MapWork) {
+      // if its an anlayze statement, stats dir will be in TableScan
+      ops.addAll(work.getAllRootOperators());
+    }
+    for (Operator<? extends OperatorDesc> op : ops) {
+      OperatorDesc desc = op.getConf();
+      String statsTmpDir = null;
+      if (desc instanceof FileSinkDesc) {
+         statsTmpDir = ((FileSinkDesc)desc).getStatsTmpDir();
+      } else if (desc instanceof TableScanDesc) {
+        statsTmpDir = ((TableScanDesc) desc).getTmpStatsDir();
+      }
+      if (statsTmpDir != null && !statsTmpDir.isEmpty()) {
+        statsTmpDirs.add(statsTmpDir);
+      }
+    }
+    return statsTmpDirs;
+  }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
index b9319cb..e8a6de6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
@@ -30,6 +30,7 @@
 import java.util.Enumeration;
 import java.util.List;
 import java.util.Properties;
+import java.util.Set;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
@@ -76,6 +77,7 @@
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
@@ -259,7 +261,7 @@ public int execute(DriverContext driverContext) {
 
     try {
       String partitioner = HiveConf.getVar(job, ConfVars.HIVEPARTITIONER);
-      job.setPartitionerClass((Class<? extends Partitioner>) JavaUtils.loadClass(partitioner));
+      job.setPartitionerClass(JavaUtils.loadClass(partitioner));
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e.getMessage(), e);
     }
@@ -305,7 +307,7 @@ public int execute(DriverContext driverContext) {
     LOG.info("Using " + inpFormat);
 
     try {
-      job.setInputFormat((Class<? extends InputFormat>) JavaUtils.loadClass(inpFormat));
+      job.setInputFormat(JavaUtils.loadClass(inpFormat));
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e.getMessage(), e);
     }
@@ -400,7 +402,13 @@ public int execute(DriverContext driverContext) {
         StatsFactory factory = StatsFactory.newFactory(job);
         if (factory != null) {
           statsPublisher = factory.getStatsPublisher();
-          if (!statsPublisher.init(job)) { // creating stats table if not exists
+          List<String> statsTmpDir = Utilities.getStatsTmpDirs(mWork, job);
+          if (rWork != null) {
+            statsTmpDir.addAll(Utilities.getStatsTmpDirs(rWork, job));
+          }
+          StatsCollectionContext sc = new StatsCollectionContext(job);
+          sc.setStatsTmpDirs(statsTmpDir);
+          if (!statsPublisher.init(sc)) { // creating stats table if not exists
             if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
               throw
                 new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
index 9f5b1e1..510f9b6 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkPlanGenerator.java
@@ -53,6 +53,7 @@
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
 import org.apache.hadoop.hive.ql.plan.SparkEdgeProperty;
 import org.apache.hadoop.hive.ql.plan.SparkWork;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.io.Writable;
@@ -67,11 +68,11 @@
   private final PerfLogger perfLogger = SessionState.getPerfLogger();
   private static final Log LOG = LogFactory.getLog(SparkPlanGenerator.class);
 
-  private JavaSparkContext sc;
+  private final JavaSparkContext sc;
   private final JobConf jobConf;
-  private Context context;
-  private Path scratchDir;
-  private SparkReporter sparkReporter;
+  private final Context context;
+  private final Path scratchDir;
+  private final SparkReporter sparkReporter;
   private Map<BaseWork, BaseWork> cloneToWork;
   private final Map<BaseWork, SparkTran> workToTranMap;
   private final Map<BaseWork, SparkTran> workToParentWorkTranMap;
@@ -262,8 +263,7 @@ private JobConf cloneJobConf(BaseWork work) throws Exception {
     // Make sure we'll use a different plan path from the original one
     HiveConf.setVar(cloned, HiveConf.ConfVars.PLAN, "");
     try {
-      cloned.setPartitionerClass((Class<? extends Partitioner>)
-          JavaUtils.loadClass(HiveConf.getVar(cloned, HiveConf.ConfVars.HIVEPARTITIONER)));
+      cloned.setPartitionerClass(JavaUtils.loadClass(HiveConf.getVar(cloned, HiveConf.ConfVars.HIVEPARTITIONER)));
     } catch (ClassNotFoundException e) {
       String msg = "Could not find partitioner class: " + e.getMessage()
         + " which is specified by: " + HiveConf.ConfVars.HIVEPARTITIONER.varname;
@@ -303,7 +303,9 @@ private void initStatsPublisher(BaseWork work) throws HiveException {
       StatsFactory factory = StatsFactory.newFactory(jobConf);
       if (factory != null) {
         statsPublisher = factory.getStatsPublisher();
-        if (!statsPublisher.init(jobConf)) { // creating stats table if not exists
+        StatsCollectionContext sc = new StatsCollectionContext(jobConf);
+        sc.setStatsTmpDirs(Utilities.getStatsTmpDirs(work, jobConf));
+        if (!statsPublisher.init(sc)) { // creating stats table if not exists
           if (HiveConf.getBoolVar(jobConf, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
             throw new HiveException(
                 ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
index f80df70..68fcabe 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/tez/DagUtils.java
@@ -73,6 +73,7 @@
 import org.apache.hadoop.hive.ql.plan.TezWork;
 import org.apache.hadoop.hive.ql.plan.TezWork.VertexType;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.shims.Utils;
@@ -1089,8 +1090,10 @@ public Vertex createVertex(JobConf conf, BaseWork work,
       StatsPublisher statsPublisher;
       StatsFactory factory = StatsFactory.newFactory(conf);
       if (factory != null) {
+        StatsCollectionContext sCntxt = new StatsCollectionContext(conf);
+        sCntxt.setStatsTmpDirs(Utilities.getStatsTmpDirs(work, conf));
         statsPublisher = factory.getStatsPublisher();
-        if (!statsPublisher.init(conf)) { // creating stats table if not exists
+        if (!statsPublisher.init(sCntxt)) { // creating stats table if not exists
           if (HiveConf.getBoolVar(conf, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
             throw
               new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
index e67996d..68709b4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/AggregateIndexHandler.java
@@ -153,7 +153,6 @@ private void createAggregationFunction(List<FieldSchema> indexTblCols, String pr
       builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
       Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
           command, (LinkedHashMap<String, String>) partSpec, indexTableName, dbName);
-      super.setStatsDir(builderConf);
       return rootTask;
     }
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/TableBasedIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/TableBasedIndexHandler.java
index a019350..807959e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/TableBasedIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/TableBasedIndexHandler.java
@@ -116,13 +116,6 @@
     return null;
   }
 
-  protected void setStatsDir(HiveConf builderConf) {
-    String statsDir;
-    if ((statsDir = builderConf.get(StatsSetupConst.STATS_TMP_LOC)) != null) {
-      getConf().set(StatsSetupConst.STATS_TMP_LOC, statsDir);
-    }
-   }
-
   protected List<String> getPartKVPairStringArray(
       LinkedHashMap<String, String> partSpec) {
     List<String> ret = new ArrayList<String>(partSpec.size());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/bitmap/BitmapIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/bitmap/BitmapIndexHandler.java
index b076933..cb191ac 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/bitmap/BitmapIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/bitmap/BitmapIndexHandler.java
@@ -289,7 +289,6 @@ public void analyzeIndexDefinition(Table baseTable, Index index,
 
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
-    super.setStatsDir(builderConf);
     return rootTask;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java b/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
index 1dbe230..586e16d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/index/compact/CompactIndexHandler.java
@@ -150,7 +150,6 @@ public void analyzeIndexDefinition(Table baseTable, Index index,
     builderConf.setBoolVar(HiveConf.ConfVars.HIVEMERGETEZFILES, false);
     Task<?> rootTask = IndexUtils.createRootTask(builderConf, inputs, outputs,
         command, partSpec, indexTableName, dbName);
-    super.setStatsDir(builderConf);
     return rootTask;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
index 3e1ef0a..be3a671 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanMapper.java
@@ -34,6 +34,7 @@
 import org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileValueBufferWrapper;
 import org.apache.hadoop.hive.ql.metadata.HiveException;
 import org.apache.hadoop.hive.ql.stats.CounterStatsPublisher;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.shims.CombineHiveKey;
@@ -145,7 +146,9 @@ private void publishStats() throws HiveException {
       throw new HiveException(ErrorMsg.STATSPUBLISHER_NOT_OBTAINED.getErrorCodedMsg());
     }
 
-    if (!statsPublisher.connect(jc)) {
+    StatsCollectionContext sc = new StatsCollectionContext(jc);
+    sc.setStatsTmpDir(jc.get(StatsSetupConst.STATS_TMP_LOC, ""));
+    if (!statsPublisher.connect(sc)) {
       // should fail since stats gathering is main purpose of the job
       LOG.error("StatsPublishing error: cannot connect to database");
       throw new HiveException(ErrorMsg.STATSPUBLISHER_CONNECTION_ERROR.getErrorCodedMsg());
@@ -170,7 +173,7 @@ private void publishStats() throws HiveException {
       throw new HiveException(ErrorMsg.STATSPUBLISHER_PUBLISHING_ERROR.getErrorCodedMsg());
     }
 
-    if (!statsPublisher.closeConnection()) {
+    if (!statsPublisher.closeConnection(sc)) {
       // The original exception is lost.
       // Not changing the interface to maintain backward compatibility
       throw new HiveException(ErrorMsg.STATSPUBLISHER_CLOSING_ERROR.getErrorCodedMsg());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
index f4ffdd6..77966d0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanTask.java
@@ -32,6 +32,8 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.common.JavaUtils;
+import org.apache.hadoop.hive.common.StatsSetupConst;
+import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
 import org.apache.hadoop.hive.ql.DriverContext;
@@ -50,6 +52,7 @@
 import org.apache.hadoop.hive.ql.plan.api.StageType;
 import org.apache.hadoop.hive.ql.session.SessionState;
 import org.apache.hadoop.hive.ql.session.SessionState.LogHelper;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.io.NullWritable;
@@ -145,7 +148,7 @@ public int execute(DriverContext driverContext) {
     LOG.info("Using " + inpFormat);
 
     try {
-      job.setInputFormat((Class<? extends InputFormat>) JavaUtils.loadClass(inpFormat));
+      job.setInputFormat(JavaUtils.loadClass(inpFormat));
     } catch (ClassNotFoundException e) {
       throw new RuntimeException(e.getMessage(), e);
     }
@@ -175,7 +178,7 @@ public int execute(DriverContext driverContext) {
     HiveConf.setVar(job,
         HiveConf.ConfVars.HIVE_STATS_KEY_PREFIX,
         work.getAggKey());
-
+      job.set(StatsSetupConst.STATS_TMP_LOC, work.getStatsTmpDir());
     try {
       addInputPaths(job, work);
 
@@ -205,7 +208,9 @@ public int execute(DriverContext driverContext) {
         StatsFactory factory = StatsFactory.newFactory(job);
         if (factory != null) {
           statsPublisher = factory.getStatsPublisher();
-          if (!statsPublisher.init(job)) { // creating stats table if not exists
+          StatsCollectionContext sc = new StatsCollectionContext(job);
+          sc.setStatsTmpDir(work.getStatsTmpDir());
+          if (!statsPublisher.init(sc)) { // creating stats table if not exists
             if (HiveConf.getBoolVar(job, HiveConf.ConfVars.HIVE_STATS_RELIABLE)) {
               throw
                 new HiveException(ErrorMsg.STATSPUBLISHER_INITIALIZATION_ERROR.getErrorCodedMsg());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanWork.java b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanWork.java
index 095afd4..2a34d49 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/io/rcfile/stats/PartialScanWork.java
@@ -18,11 +18,15 @@
 
 package org.apache.hadoop.hive.ql.io.rcfile.stats;
 
+import static org.apache.hadoop.hive.conf.HiveConf.ConfVars.HIVESTATSDBCLASS;
+
 import java.io.Serializable;
 import java.util.LinkedHashMap;
 import java.util.List;
 
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.StatsSetupConst.StatDB;
+import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
 import org.apache.hadoop.hive.ql.io.rcfile.merge.RCFileBlockMergeInputFormat;
 import org.apache.hadoop.hive.ql.plan.Explain;
@@ -41,6 +45,7 @@
 
   private transient List<Path> inputPaths;
   private String aggKey;
+  private String statsTmpDir;
 
   public PartialScanWork() {
   }
@@ -100,4 +105,13 @@ public void setAggKey(String aggKey) {
     this.aggKey = aggKey;
   }
 
+  public String getStatsTmpDir() {
+    return statsTmpDir;
+  }
+
+  public void setStatsTmpDir(String statsTmpDir, HiveConf conf) {
+    this.statsTmpDir = HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())
+      ?  statsTmpDir : "";
+  }
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
index 0b45e25..641465b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMRTableScan1.java
@@ -65,6 +65,7 @@ public GenMRTableScan1() {
    * @param opProcCtx
    *          context
    */
+  @Override
   public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
       Object... nodeOutputs) throws SemanticException {
     TableScanOperator op = (TableScanOperator) nd;
@@ -112,6 +113,7 @@ public Object process(Node nd, Stack<Node> stack, NodeProcessorCtx opProcCtx,
 
             StatsWork statsWork = new StatsWork(parseCtx.getQB().getParseInfo().getTableSpec());
             statsWork.setAggKey(op.getConf().getStatsAggPrefix());
+            statsWork.setStatsTmpDir(op.getConf().getTmpStatsDir());
             statsWork.setSourceTask(currTask);
             statsWork.setStatsReliable(parseCtx.getConf().getBoolVar(
                 HiveConf.ConfVars.HIVE_STATS_RELIABLE));
@@ -187,6 +189,7 @@ private void handlePartialScanCommand(TableScanOperator op, GenMRProcContext ctx
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(op.getConf().getTmpStatsDir(), parseCtx.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
index 5ac8a87..04cec95 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/optimizer/GenMapRedUtils.java
@@ -1434,7 +1434,7 @@ public static void addStatsTask(FileSinkOperator nd, MoveTask mvTask,
 
     statsWork.setSourceTask(currTask);
     statsWork.setStatsReliable(hconf.getBoolVar(ConfVars.HIVE_STATS_RELIABLE));
-
+    statsWork.setStatsTmpDir(nd.getConf().getStatsTmpDir());
     if (currTask.getWork() instanceof MapredWork) {
       MapredWork mrWork = (MapredWork) currTask.getWork();
       mrWork.getMapWork().setGatheringStats(true);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
index 2ecf8a5..6182a9d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ProcessAnalyzeTable.java
@@ -71,7 +71,7 @@ public Object process(Node nd, Stack<Node> stack,
       throws SemanticException {
 
     GenTezProcContext context = (GenTezProcContext) procContext;
-    
+
     TableScanOperator tableScan = (TableScanOperator) nd;
 
     ParseContext parseContext = context.parseContext;
@@ -118,6 +118,7 @@ public Object process(Node nd, Stack<Node> stack,
       
       StatsWork statsWork = new StatsWork(parseInfo.getTableSpec());
       statsWork.setAggKey(tableScan.getConf().getStatsAggPrefix());
+      statsWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir());
       statsWork.setSourceTask(context.currentTask);
       statsWork.setStatsReliable(parseContext.getConf().getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));
       Task<StatsWork> statsTask = TaskFactory.get(statsWork, parseContext.getConf());
@@ -175,6 +176,7 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index 833f61b..8321f36 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -6658,8 +6658,8 @@ private Operator genFileSinkPlan(String dest, QB qb, Operator input)
       } else {
         statsTmpLoc = ctx.getTempDirForPath(dest_path).toString();
       }
-      LOG.info("Set stats collection dir : " + statsTmpLoc);
-      conf.set(StatsSetupConst.STATS_TMP_LOC, statsTmpLoc);
+      fileSinkDesc.setStatsTmpDir(statsTmpLoc);
+      LOG.debug("Set stats collection dir : " + statsTmpLoc);
     }
 
     if (dest_part != null) {
@@ -9671,7 +9671,7 @@ private void setupStats(TableScanDesc tsDesc, QBParseInfo qbp, Table tab, String
       if (HiveConf.getVar(conf, HIVESTATSDBCLASS).equalsIgnoreCase(StatDB.fs.name())) {
         String statsTmpLoc = ctx.getTempDirForPath(tab.getPath()).toString();
         LOG.info("Set stats collection dir : " + statsTmpLoc);
-        conf.set(StatsSetupConst.STATS_TMP_LOC, statsTmpLoc);
+        tsDesc.setTmpStatsDir(statsTmpLoc);
       }
       tsDesc.setGatherStats(true);
       tsDesc.setStatsReliable(conf.getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
index 0a03760..04d8454 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/spark/SparkProcessAnalyzeTable.java
@@ -125,6 +125,7 @@ public Object process(Node nd, Stack<Node> stack,
 
         StatsWork statsWork = new StatsWork(parseInfo.getTableSpec());
         statsWork.setAggKey(tableScan.getConf().getStatsAggPrefix());
+        statsWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir());
         statsWork.setSourceTask(context.currentTask);
         statsWork.setStatsReliable(parseContext.getConf().getBoolVar(HiveConf.ConfVars.HIVE_STATS_RELIABLE));
         Task<StatsWork> statsTask = TaskFactory.get(statsWork, parseContext.getConf());
@@ -180,6 +181,7 @@ private void handlePartialScanCommand(TableScanOperator tableScan, ParseContext
     PartialScanWork scanWork = new PartialScanWork(inputPaths);
     scanWork.setMapperCannotSpanPartns(true);
     scanWork.setAggKey(aggregationKey);
+    scanWork.setStatsTmpDir(tableScan.getConf().getTmpStatsDir(), parseContext.getConf());
 
     // stats work
     statsWork.setPartialScanAnalyzeCommand(true);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/FileSinkDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/FileSinkDesc.java
index 8bc35e4..be74d11 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/FileSinkDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/FileSinkDesc.java
@@ -83,6 +83,7 @@
   private boolean statsReliable;
   private ListBucketingCtx lbCtx;
   private int maxStatsKeyPrefixLength = -1;
+  private String statsTmpDir;
 
   private boolean statsCollectRawDataSize;
 
@@ -149,7 +150,8 @@ public Object clone() throws CloneNotSupportedException {
     ret.setDpSortState(dpSortState);
     ret.setWriteType(writeType);
     ret.setTransactionId(txnId);
-    return (Object) ret;
+    ret.setStatsTmpDir(statsTmpDir);
+    return ret;
   }
 
   @Explain(displayName = "directory", normalExplain = false)
@@ -438,4 +440,14 @@ public Table getTable() {
   public void setTable(Table table) {
     this.table = table;
   }
+
+
+  public String getStatsTmpDir() {
+    return statsTmpDir;
+  }
+
+  public void setStatsTmpDir(String statsCollectionTempDir) {
+    this.statsTmpDir = statsCollectionTempDir;
+  }
+
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
index 66d4d4a..dd0fb75 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/StatsWork.java
@@ -55,6 +55,9 @@
   // so this is set by DriverContext in runtime
   private transient Task sourceTask;
 
+  // used by FS based stats collector
+  private String statsTmpDir;
+
   public StatsWork() {
   }
 
@@ -70,10 +73,6 @@ public StatsWork(LoadFileDesc loadFileDesc) {
     this.loadFileDesc = loadFileDesc;
   }
 
-  public StatsWork(boolean statsReliable) {
-    this.statsReliable = statsReliable;
-  }
-
   public tableSpec getTableSpecs() {
     return tableSpecs;
   }
@@ -95,6 +94,14 @@ public String getAggKey() {
     return aggKey;
   }
 
+  public String getStatsTmpDir() {
+    return statsTmpDir;
+  }
+
+  public void setStatsTmpDir(String statsTmpDir) {
+    this.statsTmpDir = statsTmpDir;
+  }
+
   public boolean getNoStatsAggregator() {
     return noStatsAggregator;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
index be26e38..3a109e0 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/plan/TableScanDesc.java
@@ -67,6 +67,7 @@
   private boolean gatherStats;
   private boolean statsReliable;
   private int maxStatsKeyPrefixLength = -1;
+  private String tmpStatsDir;
 
   private ExprNodeGenericFuncDesc filterExpr;
   private transient Serializable filterObject;
@@ -196,6 +197,14 @@ public boolean isGatherStats() {
     return gatherStats;
   }
 
+  public String getTmpStatsDir() {
+    return tmpStatsDir;
+  }
+
+  public void setTmpStatsDir(String tmpStatsDir) {
+    this.tmpStatsDir = tmpStatsDir;
+  }
+
   public List<VirtualColumn> getVirtualCols() {
     return virtualCols;
   }
@@ -257,7 +266,7 @@ public Integer getRowLimitExplain() {
   public void setBucketFileNameMapping(Map<String, Integer> bucketFileNameMapping) {
     this.bucketFileNameMapping = bucketFileNameMapping;
   }
-  
+
   public void setIsMetadataOnly(boolean metadata_only) {
     isMetadataOnly = metadata_only;
   }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregator.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregator.java
index 16b4460..b9863d9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregator.java
@@ -25,7 +25,6 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.mr.ExecDriver;
-import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.mr.MapRedTask;
 import org.apache.hadoop.mapred.Counters;
 import org.apache.hadoop.mapred.JobClient;
@@ -40,10 +39,11 @@
   private JobClient jc;
 
   @Override
-  public boolean connect(Configuration hconf, Task sourceTask) {
+  public boolean connect(StatsCollectionContext scc) {
+    Task<?> sourceTask = scc.getTask();
     if (sourceTask instanceof MapRedTask) {
       try {
-        jc = new JobClient(toJobConf(hconf));
+        jc = new JobClient(toJobConf(scc.getHiveConf()));
         RunningJob job = jc.getJob(((MapRedTask)sourceTask).getJobID());
         if (job != null) {
           counters = job.getCounters();
@@ -71,7 +71,7 @@ public String aggregateStats(String counterGrpName, String statType) {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext scc) {
     try {
       jc.close();
     } catch (IOException e) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java
index 13f6024..4c01b25 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorSpark.java
@@ -33,8 +33,8 @@
 
   @SuppressWarnings("rawtypes")
   @Override
-  public boolean connect(Configuration hconf, Task sourceTask) {
-    SparkTask task = (SparkTask) sourceTask;
+  public boolean connect(StatsCollectionContext scc) {
+    SparkTask task = (SparkTask) scc.getTask();
     sparkCounters = task.getSparkCounters();
     if (sparkCounters == null) {
       return false;
@@ -52,7 +52,7 @@ public String aggregateStats(String keyPrefix, String statType) {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext scc) {
     return true;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorTez.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorTez.java
index 02e8c0b..662c106 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorTez.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsAggregatorTez.java
@@ -18,11 +18,8 @@
 
 package org.apache.hadoop.hive.ql.stats;
 
-import java.io.IOException;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.tez.TezTask;
 import org.apache.tez.common.counters.TezCounters;
@@ -46,10 +43,11 @@ public CounterStatsAggregatorTez() {
   }
 
   @Override
-  public boolean connect(Configuration hconf, Task sourceTask) {
+  public boolean connect(StatsCollectionContext scc) {
+    Task sourceTask = scc.getTask();
     if (!(sourceTask instanceof TezTask)) {
       delegate = true;
-      return mrAggregator.connect(hconf, sourceTask);
+      return mrAggregator.connect(scc);
     }
     counters = ((TezTask) sourceTask).getTezCounters();
     return counters != null;
@@ -75,7 +73,7 @@ public String aggregateStats(String keyPrefix, String statType) {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext scc) {
     return true;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsPublisher.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsPublisher.java
index bf7d027..e5f1400 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsPublisher.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/CounterStatsPublisher.java
@@ -22,7 +22,6 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.exec.MapredContext;
 import org.apache.hadoop.mapred.Reporter;
 
@@ -33,12 +32,12 @@
   private Reporter reporter;
 
   @Override
-  public boolean init(Configuration hconf) {
+  public boolean init(StatsCollectionContext context) {
     return true;
   }
 
   @Override
-  public boolean connect(Configuration hconf) {
+  public boolean connect(StatsCollectionContext statsContext) {
     MapredContext context = MapredContext.get();
     if (context == null || context.getReporter() == null) {
       return false;
@@ -61,7 +60,7 @@ public boolean publishStat(String fileID, Map<String, String> stats) {
     return true;
   }
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext context) {
     return true;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsAggregator.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsAggregator.java
index 0ae0489..b115daf 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsAggregator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsAggregator.java
@@ -18,9 +18,6 @@
 
 package org.apache.hadoop.hive.ql.stats;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.ql.exec.Task;
-
 /**
  * An interface for any possible implementation for gathering statistics.
  */
@@ -35,7 +32,7 @@
    * @param sourceTask
    * @return true if connection is successful, false otherwise.
    */
-  public boolean connect(Configuration hconf, Task sourceTask);
+  public boolean connect(StatsCollectionContext scc);
 
   /**
    * This method aggregates a given statistic from all tasks (partial stats).
@@ -65,7 +62,7 @@
    *
    * @return true if close connection is successful, false otherwise.
    */
-  public boolean closeConnection();
+  public boolean closeConnection(StatsCollectionContext scc);
 
   /**
    * This method is called after all statistics have been aggregated. Since we support multiple
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsCollectionContext.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsCollectionContext.java
new file mode 100644
index 0000000..ae6f2ac
--- /dev/null
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsCollectionContext.java
@@ -0,0 +1,63 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.ql.stats;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.ql.exec.Task;
+
+public class StatsCollectionContext {
+
+  private final Configuration hiveConf;
+  private Task task;
+  private List<String> statsTmpDirs;
+
+  public List<String> getStatsTmpDirs() {
+    return statsTmpDirs;
+  }
+
+  public void setStatsTmpDirs(List<String> statsTmpDirs) {
+    this.statsTmpDirs = statsTmpDirs;
+  }
+
+  public void setStatsTmpDir(String statsTmpDir) {
+    this.statsTmpDirs = statsTmpDir == null ? new ArrayList<String>() :
+        Arrays.asList(new String[]{statsTmpDir});
+  }
+
+  public StatsCollectionContext(Configuration hiveConf) {
+    super();
+    this.hiveConf = hiveConf;
+  }
+
+  public Configuration getHiveConf() {
+    return hiveConf;
+  }
+
+  public Task getTask() {
+    return task;
+  }
+
+  public void setTask(Task task) {
+    this.task = task;
+  }
+}
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsPublisher.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsPublisher.java
index 845ec6a..3631b83 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsPublisher.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/StatsPublisher.java
@@ -20,8 +20,6 @@
 
 import java.util.Map;
 
-import org.apache.hadoop.conf.Configuration;
-
 /**
  * An interface for any possible implementation for publishing statics.
  */
@@ -37,14 +35,14 @@
    * intermediate stats database.
    * @return true if initialization is successful, false otherwise.
    */
-  public boolean init(Configuration hconf);
+  public boolean init(StatsCollectionContext context);
 
   /**
    * This method connects to the intermediate statistics database.
    * @param hconf HiveConf that contains the connection parameters.
    * @return true if connection is successful, false otherwise.
    */
-  public boolean connect(Configuration hconf);
+  public boolean connect(StatsCollectionContext context);
 
   /**
    * This method publishes a given statistic into a disk storage, possibly HBase or MySQL.
@@ -66,6 +64,6 @@
   /**
    * This method closes the connection to the temporary storage.
    */
-  public boolean closeConnection();
+  public boolean closeConnection(StatsCollectionContext context);
 
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java
index be025fb..6dfc178 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsAggregator.java
@@ -26,15 +26,14 @@
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
 import org.apache.hadoop.hive.common.StatsSetupConst;
-import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
 
 import com.esotericsoftware.kryo.io.Input;
@@ -44,17 +43,17 @@
   private List<Map<String,Map<String,String>>> statsList;
   private Map<String, Map<String,String>> statsMap;
   private FileSystem fs;
-  private Configuration conf;
 
   @Override
-  public boolean connect(Configuration hconf, Task sourceTask) {
-    conf = hconf;
-    Path statsDir = new Path(hconf.get(StatsSetupConst.STATS_TMP_LOC));
+  public boolean connect(StatsCollectionContext scc) {
+    List<String> statsDirs = scc.getStatsTmpDirs();
+    assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;
+    Path statsDir = new Path(statsDirs.get(0));
     LOG.debug("About to read stats from : " + statsDir);
     statsMap  = new HashMap<String, Map<String,String>>();
 
     try {
-      fs = statsDir.getFileSystem(hconf);
+      fs = statsDir.getFileSystem(scc.getHiveConf());
       statsList = new ArrayList<Map<String,Map<String,String>>>();
       FileStatus[] status = fs.listStatus(statsDir, new PathFilter() {
         @Override
@@ -98,11 +97,15 @@ public String aggregateStats(String partID, String statType) {
   }
 
   @Override
-  public boolean closeConnection() {
-    LOG.debug("About to delete stats tmp dir");
+  public boolean closeConnection(StatsCollectionContext scc) {
+    List<String> statsDirs = scc.getStatsTmpDirs();
+    assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;
+    Path statsDir = new Path(statsDirs.get(0));
+
+    LOG.debug("About to delete stats tmp dir :" + statsDir);
 
     try {
-      fs.delete(new Path(conf.get(StatsSetupConst.STATS_TMP_LOC)),true);
+      fs.delete(statsDir,true);
       return true;
     } catch (IOException e) {
       LOG.error(e);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
index ce96064..aa2bf62 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/fs/FSStatsPublisher.java
@@ -20,6 +20,7 @@
 
 import java.io.IOException;
 import java.util.HashMap;
+import java.util.List;
 import java.util.Map;
 import java.util.Map.Entry;
 
@@ -30,6 +31,7 @@
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.stats.StatsCollectionTaskIndependent;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 
 import com.esotericsoftware.kryo.io.Output;
@@ -41,12 +43,14 @@
   private Map<String, Map<String,String>> statsMap; // map from partID -> (statType->value)
 
   @Override
-  public boolean init(Configuration hconf) {
-    Path statsDir = new Path(hconf.get(StatsSetupConst.STATS_TMP_LOC));
-    LOG.debug("Initing FSStatsPublisher with : " + statsDir);
+  public boolean init(StatsCollectionContext context) {
     try {
-      statsDir.getFileSystem(hconf).mkdirs(statsDir);
-      LOG.info("created : " + statsDir);
+      for (String tmpDir : context.getStatsTmpDirs()) {
+        Path statsDir = new Path(tmpDir);
+        LOG.debug("Initing FSStatsPublisher with : " + statsDir);
+        statsDir.getFileSystem(context.getHiveConf()).mkdirs(statsDir);
+        LOG.info("created : " + statsDir);
+      }
       return true;
     } catch (IOException e) {
       LOG.error(e);
@@ -55,9 +59,11 @@ public boolean init(Configuration hconf) {
   }
 
   @Override
-  public boolean connect(Configuration hconf) {
-    conf = hconf;
-    Path statsDir = new Path(hconf.get(StatsSetupConst.STATS_TMP_LOC));
+  public boolean connect(StatsCollectionContext context) {
+    conf = context.getHiveConf();
+    List<String> statsDirs = context.getStatsTmpDirs();
+    assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;
+    Path statsDir = new Path(statsDirs.get(0));
     LOG.debug("Connecting to : " + statsDir);
     statsMap = new HashMap<String, Map<String,String>>();
     try {
@@ -85,14 +91,16 @@ public boolean publishStat(String partKV, Map<String, String> stats) {
   }
 
   @Override
-  public boolean closeConnection() {
-    Path statsDir = new Path(conf.get(StatsSetupConst.STATS_TMP_LOC));
+  public boolean closeConnection(StatsCollectionContext context) {
+    List<String> statsDirs = context.getStatsTmpDirs();
+    assert statsDirs.size() == 1 : "Found multiple stats dirs: " + statsDirs;
+    Path statsDir = new Path(statsDirs.get(0));
     try {
       Path statsFile = new Path(statsDir,StatsSetupConst.STATS_FILE_PREFIX +conf.getInt("mapred.task.partition",0));
       LOG.debug("About to create stats file for this task : " + statsFile);
       Output output = new Output(statsFile.getFileSystem(conf).create(statsFile,true));
-      LOG.info("Created file : " + statsFile);
-      LOG.info("Writing stats in it : " + statsMap);
+      LOG.debug("Created file : " + statsFile);
+      LOG.debug("Writing stats in it : " + statsMap);
       Utilities.runtimeSerializationKryo.get().writeObject(output, statsMap);
       output.close();
       return true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
index c17ce23..290692e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsAggregator.java
@@ -34,16 +34,15 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.Task;
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 
 public class JDBCStatsAggregator implements StatsAggregator {
 
   private Connection conn;
   private String connectionString;
   private Configuration hiveconf;
-  private Task<?> sourceTask;
   private final Map<String, PreparedStatement> columnMapping;
   private final Log LOG = LogFactory.getLog(this.getClass().getName());
   private int timeout = 30;
@@ -58,8 +57,8 @@ public JDBCStatsAggregator() {
   }
 
   @Override
-  public boolean connect(Configuration hiveconf, Task sourceTask) {
-    this.hiveconf = hiveconf;
+  public boolean connect(StatsCollectionContext scc) {
+    this.hiveconf = scc.getHiveConf();
     timeout = (int) HiveConf.getTimeVar(
         hiveconf, HiveConf.ConfVars.HIVE_STATS_JDBC_TIMEOUT, TimeUnit.SECONDS);
     connectionString = HiveConf.getVar(hiveconf, HiveConf.ConfVars.HIVESTATSDBCONNECTIONSTRING);
@@ -67,7 +66,6 @@ public boolean connect(Configuration hiveconf, Task sourceTask) {
     maxRetries = HiveConf.getIntVar(hiveconf, HiveConf.ConfVars.HIVE_STATS_RETRIES_MAX);
     waitWindow = HiveConf.getTimeVar(
         hiveconf, HiveConf.ConfVars.HIVE_STATS_RETRIES_WAIT, TimeUnit.MILLISECONDS);
-    this.sourceTask = sourceTask;
 
     try {
       JavaUtils.loadClass(driver).newInstance();
@@ -159,14 +157,14 @@ public ResultSet run(PreparedStatement stmt) throws SQLException {
           return null;
         }
         // close the current connection
-        closeConnection();
+        closeConnection(null);
         long waitTime = Utilities.getRandomWaitTime(waitWindow, failures, r);
         try {
           Thread.sleep(waitTime);
         } catch (InterruptedException iex) {
         }
         // getting a new connection
-        if (!connect(hiveconf, sourceTask)) {
+        if (!connect(new StatsCollectionContext(hiveconf))) {
           // if cannot reconnect, just fail because connect() already handles retries.
           LOG.error("Error during publishing aggregation. " + e);
           return null;
@@ -181,7 +179,7 @@ public ResultSet run(PreparedStatement stmt) throws SQLException {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext scc) {
 
     if (conn == null) {
       return true;
@@ -238,14 +236,14 @@ public Void run(PreparedStatement stmt) throws SQLException {
             return false;
           }
           // close the current connection
-          closeConnection();
+          closeConnection(null);
           long waitTime = Utilities.getRandomWaitTime(waitWindow, failures, r);
           try {
             Thread.sleep(waitTime);
           } catch (InterruptedException iex) {
           }
           // getting a new connection
-          if (!connect(hiveconf, sourceTask)) {
+          if (!connect(new StatsCollectionContext(hiveconf))) {
             LOG.error("Error during clean-up. " + e);
             return false;
           }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
index afeed9c..efdc326 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/stats/jdbc/JDBCStatsPublisher.java
@@ -38,6 +38,7 @@
 import org.apache.hadoop.hive.common.JavaUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.exec.Utilities;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 
 public class JDBCStatsPublisher implements StatsPublisher {
@@ -59,8 +60,8 @@ public JDBCStatsPublisher() {
   }
 
   @Override
-  public boolean connect(Configuration hiveconf) {
-    this.hiveconf = hiveconf;
+  public boolean connect(StatsCollectionContext context) {
+    this.hiveconf = context.getHiveConf();
     maxRetries = HiveConf.getIntVar(hiveconf, HiveConf.ConfVars.HIVE_STATS_RETRIES_MAX);
     waitWindow = HiveConf.getTimeVar(
         hiveconf, HiveConf.ConfVars.HIVE_STATS_RETRIES_WAIT, TimeUnit.MILLISECONDS);
@@ -209,15 +210,16 @@ private boolean handleSQLRecoverableException(Exception e, int failures) {
     if (failures >= maxRetries) {
       return false;
     }
+    StatsCollectionContext sCntxt = new StatsCollectionContext(hiveconf);
     // close the current connection
-    closeConnection();
+    closeConnection(sCntxt);
     long waitTime = Utilities.getRandomWaitTime(waitWindow, failures, r);
     try {
       Thread.sleep(waitTime);
     } catch (InterruptedException iex) {
     }
     // get a new connection
-    if (!connect(hiveconf)) {
+    if (!connect(sCntxt)) {
       // if cannot reconnect, just fail because connect() already handles retries.
       LOG.error("Error during publishing aggregation. " + e);
       return false;
@@ -226,7 +228,7 @@ private boolean handleSQLRecoverableException(Exception e, int failures) {
   }
 
   @Override
-  public boolean closeConnection() {
+  public boolean closeConnection(StatsCollectionContext context) {
     if (conn == null) {
       return true;
     }
@@ -266,13 +268,13 @@ public boolean closeConnection() {
    * creating tables.).
    */
   @Override
-  public boolean init(Configuration hconf) {
+  public boolean init(StatsCollectionContext context) {
     Statement stmt = null;
     ResultSet rs = null;
     try {
-      this.hiveconf = hconf;
-      connectionString = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVESTATSDBCONNECTIONSTRING);
-      String driver = HiveConf.getVar(hconf, HiveConf.ConfVars.HIVESTATSJDBCDRIVER);
+      this.hiveconf = context.getHiveConf();
+      connectionString = HiveConf.getVar(hiveconf, HiveConf.ConfVars.HIVESTATSDBCONNECTIONSTRING);
+      String driver = HiveConf.getVar(hiveconf, HiveConf.ConfVars.HIVESTATSJDBCDRIVER);
       JavaUtils.loadClass(driver).newInstance();
       synchronized(DriverManager.class) {
         DriverManager.setLoginTimeout(timeout);
@@ -330,7 +332,7 @@ public boolean init(Configuration hconf) {
           // do nothing
         }
       }
-      closeConnection();
+      closeConnection(context);
     }
     return true;
   }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFileSinkOperator.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFileSinkOperator.java
index 5a1a867..f3ea0cb 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFileSinkOperator.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestFileSinkOperator.java
@@ -43,6 +43,7 @@
 import org.apache.hadoop.hive.ql.plan.FileSinkDesc;
 import org.apache.hadoop.hive.ql.plan.TableDesc;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.hive.serde.serdeConstants;
 import org.apache.hadoop.hive.serde2.SerDe;
@@ -203,8 +204,6 @@ public void testDeleteDynamicPartitioning() throws Exception {
   @Before
   public void setup() throws Exception {
     jc = new JobConf();
-    jc.set(StatsSetupConst.STATS_TMP_LOC, File.createTempFile("TestFileSinkOperator",
-        "stats").getPath());
     jc.set(HiveConf.ConfVars.HIVE_STATS_DEFAULT_PUBLISHER.varname,
         TFSOStatsPublisher.class.getName());
     jc.set(HiveConf.ConfVars.HIVE_STATS_DEFAULT_AGGREGATOR.varname,
@@ -728,12 +727,12 @@ public SerDeStats getSerDeStats() {
     static Map<String, String> stats;
 
     @Override
-    public boolean init(Configuration hconf) {
+    public boolean init(StatsCollectionContext context) {
       return true;
     }
 
     @Override
-    public boolean connect(Configuration hconf) {
+    public boolean connect(StatsCollectionContext context) {
       return true;
     }
 
@@ -744,7 +743,7 @@ public boolean publishStat(String fileID, Map<String, String> stats) {
     }
 
     @Override
-    public boolean closeConnection() {
+    public boolean closeConnection(StatsCollectionContext context) {
       return true;
     }
   }
@@ -752,7 +751,7 @@ public boolean closeConnection() {
   public static class TFSOStatsAggregator implements StatsAggregator {
 
     @Override
-    public boolean connect(Configuration hconf, Task sourceTask) {
+    public boolean connect(StatsCollectionContext scc) {
       return true;
     }
 
@@ -762,7 +761,7 @@ public String aggregateStats(String keyPrefix, String statType) {
     }
 
     @Override
-    public boolean closeConnection() {
+    public boolean closeConnection(StatsCollectionContext scc) {
       return true;
     }
 
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java
index 887716e..c257797 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestStatsPublisherEnhanced.java
@@ -27,6 +27,7 @@
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.stats.StatsAggregator;
+import org.apache.hadoop.hive.ql.stats.StatsCollectionContext;
 import org.apache.hadoop.hive.ql.stats.StatsFactory;
 import org.apache.hadoop.hive.ql.stats.StatsPublisher;
 import org.apache.hadoop.mapred.JobConf;
@@ -60,9 +61,10 @@ protected void setUp() {
   protected void tearDown() {
     StatsAggregator sa = factory.getStatsAggregator();
     assertNotNull(sa);
-    assertTrue(sa.connect(conf, null));
+    StatsCollectionContext sc = new StatsCollectionContext(conf);
+    assertTrue(sa.connect(sc));
     assertTrue(sa.cleanUp("file_0"));
-    assertTrue(sa.closeConnection());
+    assertTrue(sa.closeConnection(sc));
   }
 
   private void fillStatMap(String numRows, String rawDataSize) {
@@ -80,13 +82,14 @@ public void testStatsPublisherOneStat() throws Throwable {
       // instantiate stats publisher
       StatsPublisher statsPublisher = Utilities.getStatsPublisher((JobConf) conf);
       assertNotNull(statsPublisher);
-      assertTrue(statsPublisher.init(conf));
-      assertTrue(statsPublisher.connect(conf));
+      StatsCollectionContext sc = new StatsCollectionContext(conf);
+      assertTrue(statsPublisher.init(sc));
+      assertTrue(statsPublisher.connect(sc));
 
       // instantiate stats aggregator
       StatsAggregator statsAggregator = factory.getStatsAggregator();
       assertNotNull(statsAggregator);
-      assertTrue(statsAggregator.connect(conf, null));
+      assertTrue(statsAggregator.connect(sc));
 
       // publish stats
       fillStatMap("200", "1000");
@@ -109,8 +112,8 @@ public void testStatsPublisherOneStat() throws Throwable {
       assertEquals("3000", usize1);
 
       // close connections
-      assertTrue(statsPublisher.closeConnection());
-      assertTrue(statsAggregator.closeConnection());
+      assertTrue(statsPublisher.closeConnection(sc));
+      assertTrue(statsAggregator.closeConnection(sc));
 
       System.out
           .println("StatsPublisher - one stat published per key - aggregating matching key - OK");
@@ -128,13 +131,14 @@ public void testStatsPublisher() throws Throwable {
       StatsPublisher statsPublisher = Utilities.getStatsPublisher(
           (JobConf) conf);
       assertNotNull(statsPublisher);
-      assertTrue(statsPublisher.init(conf));
-      assertTrue(statsPublisher.connect(conf));
+      StatsCollectionContext sc = new StatsCollectionContext(conf);
+      assertTrue(statsPublisher.init(sc));
+      assertTrue(statsPublisher.connect(sc));
 
       // instantiate stats aggregator
       StatsAggregator statsAggregator = factory.getStatsAggregator();
       assertNotNull(statsAggregator);
-      assertTrue(statsAggregator.connect(conf, null));
+      assertTrue(statsAggregator.connect(sc));
       // statsAggregator.cleanUp("file_0000");
       // assertTrue(statsAggregator.connect(conf));
 
@@ -172,8 +176,8 @@ public void testStatsPublisher() throws Throwable {
       assertTrue(statsAggregator.cleanUp("file_0000"));
 
       // close connections
-      assertTrue(statsPublisher.closeConnection());
-      assertTrue(statsAggregator.closeConnection());
+      assertTrue(statsPublisher.closeConnection(sc));
+      assertTrue(statsAggregator.closeConnection(sc));
 
       System.out.println("StatsPublisher - basic functionality - OK");
     } catch (Throwable e) {
@@ -189,13 +193,14 @@ public void testStatsPublisherMultipleUpdates() throws Throwable {
       // instantiate stats publisher
       StatsPublisher statsPublisher = Utilities.getStatsPublisher((JobConf) conf);
       assertNotNull(statsPublisher);
-      assertTrue(statsPublisher.init(conf));
-      assertTrue(statsPublisher.connect(conf));
+      StatsCollectionContext sc = new StatsCollectionContext(conf);
+      assertTrue(statsPublisher.init(sc));
+      assertTrue(statsPublisher.connect(sc));
 
       // instantiate stats aggregator
       StatsAggregator statsAggregator = factory.getStatsAggregator();
       assertNotNull(statsAggregator);
-      assertTrue(statsAggregator.connect(conf, null));
+      assertTrue(statsAggregator.connect(sc));
 
       // publish stats
       fillStatMap("200", "1000");
@@ -236,8 +241,8 @@ public void testStatsPublisherMultipleUpdates() throws Throwable {
       assertTrue(statsAggregator.cleanUp("file_0000"));
 
       // close connections
-      assertTrue(statsPublisher.closeConnection());
-      assertTrue(statsAggregator.closeConnection());
+      assertTrue(statsPublisher.closeConnection(sc));
+      assertTrue(statsAggregator.closeConnection(sc));
 
       System.out.println("StatsPublisher - multiple updates - OK");
     } catch (Throwable e) {
@@ -254,13 +259,14 @@ public void testStatsPublisherMultipleUpdatesSubsetStatistics() throws Throwable
       // instantiate stats publisher
       StatsPublisher statsPublisher = Utilities.getStatsPublisher((JobConf) conf);
       assertNotNull(statsPublisher);
-      assertTrue(statsPublisher.init(conf));
-      assertTrue(statsPublisher.connect(conf));
+      StatsCollectionContext sc = new StatsCollectionContext(conf);
+      assertTrue(statsPublisher.init(sc));
+      assertTrue(statsPublisher.connect(sc));
 
       // instantiate stats aggregator
       StatsAggregator statsAggregator = factory.getStatsAggregator();
       assertNotNull(statsAggregator);
-      assertTrue(statsAggregator.connect(conf, null));
+      assertTrue(statsAggregator.connect(sc));
 
       // publish stats
       fillStatMap("200", "");
@@ -305,8 +311,8 @@ public void testStatsPublisherMultipleUpdatesSubsetStatistics() throws Throwable
       assertTrue(statsAggregator.cleanUp("file_0000"));
 
       // close connections
-      assertTrue(statsPublisher.closeConnection());
-      assertTrue(statsAggregator.closeConnection());
+      assertTrue(statsPublisher.closeConnection(sc));
+      assertTrue(statsAggregator.closeConnection(sc));
 
       System.out
           .println("StatsPublisher - (multiple updates + publishing subset of supported statistics) - OK");
@@ -325,13 +331,14 @@ public void testStatsAggregatorCleanUp() throws Throwable {
       // instantiate stats publisher
       StatsPublisher statsPublisher = Utilities.getStatsPublisher((JobConf) conf);
       assertNotNull(statsPublisher);
-      assertTrue(statsPublisher.init(conf));
-      assertTrue(statsPublisher.connect(conf));
+      StatsCollectionContext sc = new StatsCollectionContext(conf);
+      assertTrue(statsPublisher.init(sc));
+      assertTrue(statsPublisher.connect(sc));
 
       // instantiate stats aggregator
       StatsAggregator statsAggregator = factory.getStatsAggregator();
       assertNotNull(statsAggregator);
-      assertTrue(statsAggregator.connect(conf, null));
+      assertTrue(statsAggregator.connect(sc));
 
       // publish stats
       fillStatMap("200", "1000");
@@ -364,8 +371,8 @@ public void testStatsAggregatorCleanUp() throws Throwable {
       assertTrue(statsAggregator.cleanUp("file_0000"));
 
       // close connections
-      assertTrue(statsPublisher.closeConnection());
-      assertTrue(statsAggregator.closeConnection());
+      assertTrue(statsPublisher.closeConnection(sc));
+      assertTrue(statsAggregator.closeConnection(sc));
 
       System.out.println("StatsAggregator - clean-up - OK");
     } catch (Throwable e) {
diff --git a/ql/src/test/queries/clientpositive/infer_bucket_sort_multi_insert.q b/ql/src/test/queries/clientpositive/infer_bucket_sort_multi_insert.q
index e3992b8..3341df0 100644
--- a/ql/src/test/queries/clientpositive/infer_bucket_sort_multi_insert.q
+++ b/ql/src/test/queries/clientpositive/infer_bucket_sort_multi_insert.q
@@ -1,5 +1,6 @@
 set hive.exec.infer.bucket.sort=true;
 set hive.exec.infer.bucket.sort.num.buckets.power.two=true;
+set hive.stats.dbclass=fs;
 
 -- This tests inferring how data is bucketed/sorted from the operators in the reducer
 -- and populating that information in partitions' metadata.  In particular, those cases
diff --git a/ql/src/test/queries/clientpositive/multi_insert.q b/ql/src/test/queries/clientpositive/multi_insert.q
index 5947985..1fdfa59 100644
--- a/ql/src/test/queries/clientpositive/multi_insert.q
+++ b/ql/src/test/queries/clientpositive/multi_insert.q
@@ -5,7 +5,7 @@ create table src_multi2 like src;
 
 set hive.merge.mapfiles=false;
 set hive.merge.mapredfiles=false;
-
+set hive.stats.dbclass=fs;
 explain
 from src
 insert overwrite table src_multi1 select * where key < 10
diff --git a/ql/src/test/queries/clientpositive/multi_insert_gby2.q b/ql/src/test/queries/clientpositive/multi_insert_gby2.q
index 46e2b19..fa29261 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_gby2.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_gby2.q
@@ -1,7 +1,7 @@
 --HIVE-3699 Multiple insert overwrite into multiple tables query stores same results in all tables
 create table e1 (count int);
 create table e2 (percentile double);
-
+set hive.stats.dbclass=fs;
 explain
 FROM (select key, cast(key as double) as value from src order by key) a
 INSERT OVERWRITE TABLE e1
diff --git a/ql/src/test/queries/clientpositive/multi_insert_gby3.q b/ql/src/test/queries/clientpositive/multi_insert_gby3.q
index 1221af4..d85ff9a 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_gby3.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_gby3.q
@@ -2,7 +2,7 @@
 create table e1 (key string, keyD double);
 create table e2 (key string, keyD double, value string);
 create table e3 (key string, keyD double);
-
+set hive.stats.dbclass=fs;
 explain
 FROM (select key, cast(key as double) as keyD, value from src order by key) a
 INSERT OVERWRITE TABLE e1
diff --git a/ql/src/test/queries/clientpositive/multi_insert_lateral_view.q b/ql/src/test/queries/clientpositive/multi_insert_lateral_view.q
index acf905f..d80717f 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_lateral_view.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_lateral_view.q
@@ -1,3 +1,4 @@
+set hive.stats.dbclass=fs;
 -- SORT_QUERY_RESULTS
 
 create table src_10 as select * from src limit 10;
diff --git a/ql/src/test/queries/clientpositive/multi_insert_mixed.q b/ql/src/test/queries/clientpositive/multi_insert_mixed.q
index 6d91973..8fb577a 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_mixed.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_mixed.q
@@ -1,7 +1,7 @@
 create table src_multi1 like src;
 create table src_multi2 like src;
 create table src_multi3 like src;
-
+set hive.stats.dbclass=fs;
 -- Testing the case where a map work contains both shuffling (ReduceSinkOperator)
 -- and inserting to output table (FileSinkOperator).
 
diff --git a/ql/src/test/queries/clientpositive/multi_insert_move_tasks_share_dependencies.q b/ql/src/test/queries/clientpositive/multi_insert_move_tasks_share_dependencies.q
index 3117713..3ddaa47 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_move_tasks_share_dependencies.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_move_tasks_share_dependencies.q
@@ -1,5 +1,5 @@
 set hive.multi.insert.move.tasks.share.dependencies=true;
-
+set hive.stats.dbclass=fs;
 -- SORT_QUERY_RESULTS
 
 create table src_multi1 like src;
diff --git a/ql/src/test/queries/clientpositive/multi_insert_union_src.q b/ql/src/test/queries/clientpositive/multi_insert_union_src.q
index 088d756..f9b6f87 100644
--- a/ql/src/test/queries/clientpositive/multi_insert_union_src.q
+++ b/ql/src/test/queries/clientpositive/multi_insert_union_src.q
@@ -1,7 +1,7 @@
 drop table if exists src2;
 drop table if exists src_multi1;
 drop table if exists src_multi1;
-
+set hive.stats.dbclass=fs;
 CREATE TABLE src2 as SELECT * FROM src;
 
 create table src_multi1 like src;
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
index 1e14877..132ea4e 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_11.q.out
@@ -120,12 +120,12 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: a
-            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: false
               predicate: key is not null (type: boolean)
-              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Sorted Merge Bucket Map Join Operator
                 condition map:
                      Inner Join 0 to 1
@@ -186,10 +186,10 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numRows 0
+              numRows 500
               partition_columns ds
               partition_columns.types string
-              rawDataSize 0
+              rawDataSize 5312
               serialization.ddl struct test_table1 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -1848,23 +1848,23 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: test_table1
-            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: true
               predicate: (((hash(key) & 2147483647) % 16) = 1) (type: boolean)
               sampleDesc: BUCKET 2 OUT OF 16
-              Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Select Operator
                 expressions: key (type: int), value (type: string), ds (type: string)
                 outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
                   GlobalTableId: 0
 #### A masked pattern was here ####
                   NumFilesPerFileSink: 1
-                  Statistics: Num rows: 27 Data size: 2853 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
@@ -1900,10 +1900,10 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numRows 0
+              numRows 500
               partition_columns ds
               partition_columns.types string
-              rawDataSize 0
+              rawDataSize 5312
               serialization.ddl struct test_table1 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
index d6663c0..8701cd8 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_12.q.out
@@ -132,12 +132,12 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: a
-            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: false
               predicate: key is not null (type: boolean)
-              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Sorted Merge Bucket Map Join Operator
                 condition map:
                      Inner Join 0 to 1
@@ -199,10 +199,10 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numRows 0
+              numRows 500
               partition_columns ds
               partition_columns.types string
-              rawDataSize 0
+              rawDataSize 5312
               serialization.ddl struct test_table1 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
index 6833e26..e75d534 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_13.q.out
@@ -120,12 +120,12 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: a
-            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: false
               predicate: key is not null (type: boolean)
-              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Sorted Merge Bucket Map Join Operator
                 condition map:
                      Inner Join 0 to 1
@@ -162,8 +162,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table1
               numFiles 16
-              numRows 0
-              rawDataSize 0
+              numRows 500
+              rawDataSize 5312
               serialization.ddl struct test_table1 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -184,8 +184,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                 name default.test_table1
                 numFiles 16
-                numRows 0
-                rawDataSize 0
+                numRows 500
+                rawDataSize 5312
                 serialization.ddl struct test_table1 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -316,12 +316,12 @@ STAGE PLANS:
         b 
           TableScan
             alias: b
-            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: false
               predicate: UDFToDouble(value) is not null (type: boolean)
-              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               HashTable Sink Operator
                 keys:
                   0 UDFToDouble(key) (type: double)
@@ -333,12 +333,12 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: a
-            Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             GatherStats: false
             Filter Operator
               isSamplingPred: false
               predicate: UDFToDouble(key) is not null (type: boolean)
-              Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Map Join Operator
                 condition map:
                      Inner Join 0 to 1
@@ -347,15 +347,15 @@ STAGE PLANS:
                   1 UDFToDouble(value) (type: double)
                 outputColumnNames: _col0, _col1, _col5, _col6
                 Position of Big Table: 0
-                Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
                   outputColumnNames: _col0, _col1, _col2, _col3
-                  Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                   Reduce Output Operator
                     key expressions: _col0 (type: int)
                     sort order: +
-                    Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                     tag: -1
                     value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)
                     auto parallelism: false
@@ -380,8 +380,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
               name default.test_table3
               numFiles 16
-              numRows 0
-              rawDataSize 0
+              numRows 500
+              rawDataSize 5312
               serialization.ddl struct test_table3 { i32 key, string value}
               serialization.format 1
               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -402,8 +402,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                 name default.test_table3
                 numFiles 16
-                numRows 0
-                rawDataSize 0
+                numRows 500
+                rawDataSize 5312
                 serialization.ddl struct test_table3 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -419,16 +419,16 @@ STAGE PLANS:
         Select Operator
           expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)
           outputColumnNames: _col0, _col1, _col2, _col3
-          Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+          Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
           Limit
             Number of rows: 10
-            Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
             File Output Operator
               compressed: false
               GlobalTableId: 0
 #### A masked pattern was here ####
               NumFilesPerFileSink: 1
-              Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
               table:
                   input format: org.apache.hadoop.mapred.TextInputFormat
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
index db00dde..880a006 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_16.q.out
@@ -48,10 +48,10 @@ STAGE PLANS:
       Map Operator Tree:
           TableScan
             alias: a
-            Statistics: Num rows: 1453 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
             Filter Operator
               predicate: key is not null (type: boolean)
-              Statistics: Num rows: 727 Data size: 2908 Basic stats: COMPLETE Column stats: NONE
+              Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
               Sorted Merge Bucket Map Join Operator
                 condition map:
                      Inner Join 0 to 1
diff --git a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
index aa04760..0ba94c6 100644
--- a/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
+++ b/ql/src/test/results/clientpositive/beeline/smb_mapjoin_2.q.out
@@ -89,9 +89,6 @@ STAGE PLANS:
       Processor Tree:
         ListSink
 
-Warning: org.apache.hive.jdbc.ClosedOrCancelledStatementException: Method getQueryLog() failed. The statement has been closed or cancelled. (state=,code=0)
-4	val_4	4	val_4
-10	val_10	10	val_10
 PREHOOK: query: select /*+mapjoin(a)*/ * from smb_bucket_1 a join smb_bucket_3 b on a.key = b.key
 PREHOOK: type: QUERY
 PREHOOK: Input: default@smb_bucket_1
@@ -102,6 +99,8 @@ POSTHOOK: type: QUERY
 POSTHOOK: Input: default@smb_bucket_1
 POSTHOOK: Input: default@smb_bucket_3
 #### A masked pattern was here ####
+4	val_4	4	val_4
+10	val_10	10	val_10
 PREHOOK: query: explain
 select /*+mapjoin(a)*/ * from smb_bucket_1 a left outer join smb_bucket_3 b on a.key = b.key
 PREHOOK: type: QUERY
diff --git a/ql/src/test/results/clientpositive/spark/column_access_stats.q.out b/ql/src/test/results/clientpositive/spark/column_access_stats.q.out
index 951514b..b14a0f9 100644
--- a/ql/src/test/results/clientpositive/spark/column_access_stats.q.out
+++ b/ql/src/test/results/clientpositive/spark/column_access_stats.q.out
@@ -383,15 +383,15 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: t2
-                  Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 5 Data size: 15 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: key (type: string)
                       sort order: +
                       Map-reduce partition columns: key (type: string)
-                      Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
         Reducer 2 
             Reduce Operator Tree:
               Join Operator
@@ -401,10 +401,10 @@ STAGE PLANS:
                   0 key (type: string)
                   1 key (type: string)
                 outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -500,15 +500,15 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: t2
-                  Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 5 Data size: 15 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: ((val = 3) and key is not null) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: key (type: string)
                       sort order: +
                       Map-reduce partition columns: key (type: string)
-                      Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
         Reducer 2 
             Reduce Operator Tree:
               Join Operator
@@ -594,19 +594,19 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: t2
-                  Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 5 Data size: 15 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: ((key = 6) and val is not null) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: val (type: string)
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: string)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: string)
-                        Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
         Reducer 2 
             Reduce Operator Tree:
               Join Operator
@@ -702,32 +702,32 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: t2
-                  Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 5 Data size: 15 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: key (type: string)
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: string)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: string)
-                        Statistics: Num rows: 1 Data size: 20 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
         Map 5 
             Map Operator Tree:
                 TableScan
                   alias: t3
-                  Statistics: Num rows: 1 Data size: 35 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 5 Data size: 30 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 35 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 3 Data size: 18 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: key (type: string)
                       sort order: +
                       Map-reduce partition columns: key (type: string)
-                      Statistics: Num rows: 1 Data size: 35 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 3 Data size: 18 Basic stats: COMPLETE Column stats: NONE
                       value expressions: val (type: string)
         Reducer 2 
             Reduce Operator Tree:
@@ -738,12 +738,12 @@ STAGE PLANS:
                   0 _col0 (type: string)
                   1 _col0 (type: string)
                 outputColumnNames: _col0
-                Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: string)
                   sort order: +
                   Map-reduce partition columns: _col0 (type: string)
-                  Statistics: Num rows: 1 Data size: 33 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
         Reducer 3 
             Reduce Operator Tree:
               Join Operator
@@ -753,10 +753,10 @@ STAGE PLANS:
                   0 _col0 (type: string)
                   1 key (type: string)
                 outputColumnNames: _col0, _col1, _col2
-                Statistics: Num rows: 1 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 1 Data size: 36 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 3 Data size: 9 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/spark/pcr.q.out b/ql/src/test/results/clientpositive/spark/pcr.q.out
index cfd2cce..4a5128d 100644
--- a/ql/src/test/results/clientpositive/spark/pcr.q.out
+++ b/ql/src/test/results/clientpositive/spark/pcr.q.out
@@ -4390,8 +4390,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                               name default.pcr_t2
                               numFiles 1
-                              numRows 0
-                              rawDataSize 0
+                              numRows 20
+                              rawDataSize 160
                               serialization.ddl struct pcr_t2 { i32 key, string value}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -4429,8 +4429,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                               name default.pcr_t3
                               numFiles 1
-                              numRows 0
-                              rawDataSize 0
+                              numRows 20
+                              rawDataSize 160
                               serialization.ddl struct pcr_t3 { i32 key, string value}
                               serialization.format 1
                               serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -4509,8 +4509,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                 name default.pcr_t2
                 numFiles 1
-                numRows 0
-                rawDataSize 0
+                numRows 20
+                rawDataSize 160
                 serialization.ddl struct pcr_t2 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -4540,8 +4540,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                 name default.pcr_t3
                 numFiles 1
-                numRows 0
-                rawDataSize 0
+                numRows 20
+                rawDataSize 160
                 serialization.ddl struct pcr_t3 { i32 key, string value}
                 serialization.format 1
                 serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
diff --git a/ql/src/test/results/clientpositive/spark/ppd_join5.q.out b/ql/src/test/results/clientpositive/spark/ppd_join5.q.out
index 0cf9080..6bf5080 100644
--- a/ql/src/test/results/clientpositive/spark/ppd_join5.q.out
+++ b/ql/src/test/results/clientpositive/spark/ppd_join5.q.out
@@ -59,40 +59,40 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: a
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (id1 is not null and id2 is not null) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: id1 (type: string), id2 (type: string)
                       sort order: ++
                       Map-reduce partition columns: id1 (type: string), id2 (type: string)
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
         Map 4 
             Map Operator Tree:
                 TableScan
                   alias: b
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (id is not null and (d <= 1)) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: id (type: string), id (type: string)
                       sort order: ++
                       Map-reduce partition columns: id (type: string), id (type: string)
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       value expressions: d (type: int)
         Map 5 
             Map Operator Tree:
                 TableScan
                   alias: c
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (d <= 1) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       sort order: 
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       value expressions: d (type: int)
         Reducer 2 
             Reduce Operator Tree:
@@ -103,10 +103,10 @@ STAGE PLANS:
                   0 id1 (type: string), id2 (type: string)
                   1 id (type: string), id (type: string)
                 outputColumnNames: _col0, _col1, _col6
-                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: string), _col1 (type: string), _col6 (type: int)
         Reducer 3 
             Reduce Operator Tree:
@@ -117,14 +117,14 @@ STAGE PLANS:
                   0 
                   1 
                 outputColumnNames: _col0, _col1, _col6, _col11
-                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                 Select Operator
                   expressions: _col0 (type: string), _col1 (type: string), _col6 (type: int), _col11 (type: int)
                   outputColumnNames: _col0, _col1, _col2, _col3
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
                         output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -167,40 +167,40 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: a
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (id1 is not null and id2 is not null) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: id1 (type: string), id2 (type: string)
                       sort order: ++
                       Map-reduce partition columns: id1 (type: string), id2 (type: string)
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
         Map 4 
             Map Operator Tree:
                 TableScan
                   alias: b
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (id is not null and (d <= 1)) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: id (type: string), id (type: string)
                       sort order: ++
                       Map-reduce partition columns: id (type: string), id (type: string)
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       value expressions: d (type: int)
         Map 5 
             Map Operator Tree:
                 TableScan
                   alias: c
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: (d <= 1) (type: boolean)
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       sort order: 
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       value expressions: d (type: int)
         Reducer 2 
             Reduce Operator Tree:
@@ -211,10 +211,10 @@ STAGE PLANS:
                   0 id1 (type: string), id2 (type: string)
                   1 id (type: string), id (type: string)
                 outputColumnNames: _col0, _col1, _col6
-                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   sort order: 
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col0 (type: string), _col1 (type: string), _col6 (type: int)
         Reducer 3 
             Reduce Operator Tree:
@@ -225,17 +225,17 @@ STAGE PLANS:
                   0 
                   1 
                 outputColumnNames: _col0, _col1, _col6, _col11
-                Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                 Filter Operator
                   predicate: ((_col6 > 1) or (_col11 > 1)) (type: boolean)
-                  Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: _col0 (type: string), _col1 (type: string), _col6 (type: int), _col11 (type: int)
                     outputColumnNames: _col0, _col1, _col2, _col3
-                    Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                     File Output Operator
                       compressed: false
-                      Statistics: Num rows: 1 Data size: 4 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 1 Data size: 3 Basic stats: COMPLETE Column stats: NONE
                       table:
                           input format: org.apache.hadoop.mapred.TextInputFormat
                           output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_12.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_12.q.out
index bfddda1..fa8ecff 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_12.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_12.q.out
@@ -149,18 +149,18 @@ STAGE PLANS:
                         1 key (type: int)
                       outputColumnNames: _col0, _col7
                       Position of Big Table: 0
-                      Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 825 Data size: 8764 Basic stats: COMPLETE Column stats: NONE
                       Select Operator
                         expressions: _col0 (type: int), _col7 (type: string)
                         outputColumnNames: _col0, _col1
-                        Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 825 Data size: 8764 Basic stats: COMPLETE Column stats: NONE
                         File Output Operator
                           compressed: false
                           GlobalTableId: 1
 #### A masked pattern was here ####
                           NumFilesPerFileSink: 1
                           Static Partition Specification: ds=1/
-                          Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
+                          Statistics: Num rows: 825 Data size: 8764 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
                           table:
                               input format: org.apache.hadoop.mapred.TextInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_13.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_13.q.out
index dcacc27..4639ae3 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_13.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_13.q.out
@@ -326,12 +326,12 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: b
-                  Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                   GatherStats: false
                   Filter Operator
                     isSamplingPred: false
                     predicate: UDFToDouble(value) is not null (type: boolean)
-                    Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                     Spark HashTable Sink Operator
                       keys:
                         0 UDFToDouble(key) (type: double)
@@ -358,8 +358,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                     name default.test_table4
                     numFiles 16
-                    numRows 0
-                    rawDataSize 0
+                    numRows 500
+                    rawDataSize 5312
                     serialization.ddl struct test_table4 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -380,8 +380,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                       name default.test_table4
                       numFiles 16
-                      numRows 0
-                      rawDataSize 0
+                      numRows 500
+                      rawDataSize 5312
                       serialization.ddl struct test_table4 { i32 key, string value}
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -403,12 +403,12 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: a
-                  Statistics: Num rows: 55 Data size: 5812 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
                   GatherStats: false
                   Filter Operator
                     isSamplingPred: false
                     predicate: UDFToDouble(key) is not null (type: boolean)
-                    Statistics: Num rows: 28 Data size: 2958 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 250 Data size: 2656 Basic stats: COMPLETE Column stats: NONE
                     Map Join Operator
                       condition map:
                            Inner Join 0 to 1
@@ -419,15 +419,15 @@ STAGE PLANS:
                       input vertices:
                         1 Map 3
                       Position of Big Table: 0
-                      Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                       Select Operator
                         expressions: _col0 (type: int), _col1 (type: string), _col5 (type: int), _col6 (type: string)
                         outputColumnNames: _col0, _col1, _col2, _col3
-                        Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           key expressions: _col0 (type: int)
                           sort order: +
-                          Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                          Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                           tag: -1
                           value expressions: _col1 (type: string), _col2 (type: int), _col3 (type: string)
                           auto parallelism: false
@@ -452,8 +452,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                     name default.test_table3
                     numFiles 16
-                    numRows 0
-                    rawDataSize 0
+                    numRows 500
+                    rawDataSize 5312
                     serialization.ddl struct test_table3 { i32 key, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -474,8 +474,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                       name default.test_table3
                       numFiles 16
-                      numRows 0
-                      rawDataSize 0
+                      numRows 500
+                      rawDataSize 5312
                       serialization.ddl struct test_table3 { i32 key, string value}
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -492,16 +492,16 @@ STAGE PLANS:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: string), VALUE._col1 (type: int), VALUE._col2 (type: string)
                 outputColumnNames: _col0, _col1, _col2, _col3
-                Statistics: Num rows: 30 Data size: 3253 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                 Limit
                   Number of rows: 10
-                  Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
                   File Output Operator
                     compressed: false
                     GlobalTableId: 0
 #### A masked pattern was here ####
                     NumFilesPerFileSink: 1
-                    Statistics: Num rows: 10 Data size: 1080 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 10 Data size: 100 Basic stats: COMPLETE Column stats: NONE
 #### A masked pattern was here ####
                     table:
                         input format: org.apache.hadoop.mapred.TextInputFormat
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_15.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_15.q.out
index 4e0d114..3a3c472 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_15.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_15.q.out
@@ -784,12 +784,12 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: b
-                  Statistics: Num rows: 71 Data size: 7718 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 500 Data size: 7218 Basic stats: COMPLETE Column stats: NONE
                   GatherStats: false
                   Filter Operator
                     isSamplingPred: false
                     predicate: (key is not null and value is not null) (type: boolean)
-                    Statistics: Num rows: 18 Data size: 1956 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 125 Data size: 1804 Basic stats: COMPLETE Column stats: NONE
                     Spark HashTable Sink Operator
                       keys:
                         0 key (type: int), value (type: string)
@@ -821,8 +821,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                     name default.test_table2
                     numFiles 16
-                    numRows 0
-                    rawDataSize 0
+                    numRows 500
+                    rawDataSize 7218
                     serialization.ddl struct test_table2 { i32 key, i32 key2, string value}
                     serialization.format 1
                     serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
@@ -843,8 +843,8 @@ STAGE PLANS:
 #### A masked pattern was here ####
                       name default.test_table2
                       numFiles 16
-                      numRows 0
-                      rawDataSize 0
+                      numRows 500
+                      rawDataSize 7218
                       serialization.ddl struct test_table2 { i32 key, i32 key2, string value}
                       serialization.format 1
                       serialization.lib org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
diff --git a/ql/src/test/results/clientpositive/spark/smb_mapjoin_16.q.out b/ql/src/test/results/clientpositive/spark/smb_mapjoin_16.q.out
index f3863bc..84acdb2 100644
--- a/ql/src/test/results/clientpositive/spark/smb_mapjoin_16.q.out
+++ b/ql/src/test/results/clientpositive/spark/smb_mapjoin_16.q.out
@@ -63,7 +63,7 @@ STAGE PLANS:
                       keys:
                         0 key (type: int)
                         1 key (type: int)
-                      Statistics: Num rows: 799 Data size: 3198 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 275 Data size: 2921 Basic stats: COMPLETE Column stats: NONE
                       Group By Operator
                         aggregations: count()
                         mode: hash
diff --git a/ql/src/test/results/clientpositive/spark/union34.q.out b/ql/src/test/results/clientpositive/spark/union34.q.out
index 844d8ba..06a7739 100644
--- a/ql/src/test/results/clientpositive/spark/union34.q.out
+++ b/ql/src/test/results/clientpositive/spark/union34.q.out
@@ -88,14 +88,14 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: src10_2
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: key (type: string)
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                       Spark HashTable Sink Operator
                         keys:
                           0 _col0 (type: string)
@@ -113,14 +113,14 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: src10_1
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: key (type: string), value (type: string)
                       outputColumnNames: _col0, _col1
-                      Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                       Map Join Operator
                         condition map:
                              Inner Join 0 to 1
@@ -130,11 +130,11 @@ STAGE PLANS:
                         outputColumnNames: _col0, _col1
                         input vertices:
                           1 Map 3
-                        Statistics: Num rows: 1 Data size: 125 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 5 Data size: 57 Basic stats: COMPLETE Column stats: NONE
                         Reduce Output Operator
                           key expressions: _col0 (type: string)
                           sort order: +
-                          Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                          Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                           value expressions: _col1 (type: string)
             Local Work:
               Map Reduce Local Work
@@ -142,39 +142,39 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: src10_3
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: key (type: string), value (type: string)
                     outputColumnNames: _col0, _col1
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: string)
                       sort order: +
-                      Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                       value expressions: _col1 (type: string)
         Map 5 
             Map Operator Tree:
                 TableScan
                   alias: src10_4
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: key (type: string), value (type: string)
                     outputColumnNames: _col0, _col1
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: string)
                       sort order: +
-                      Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                       value expressions: _col1 (type: string)
         Reducer 2 
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
@@ -268,64 +268,64 @@ STAGE PLANS:
             Map Operator Tree:
                 TableScan
                   alias: src10_1
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: key (type: string), value (type: string)
                       outputColumnNames: _col0, _col1
-                      Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: string)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: string)
-                        Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                         value expressions: _col1 (type: string)
         Map 4 
             Map Operator Tree:
                 TableScan
                   alias: src10_2
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Filter Operator
                     predicate: key is not null (type: boolean)
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                     Select Operator
                       expressions: key (type: string)
                       outputColumnNames: _col0
-                      Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
                       Reduce Output Operator
                         key expressions: _col0 (type: string)
                         sort order: +
                         Map-reduce partition columns: _col0 (type: string)
-                        Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                        Statistics: Num rows: 5 Data size: 52 Basic stats: COMPLETE Column stats: NONE
         Map 5 
             Map Operator Tree:
                 TableScan
                   alias: src10_3
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: key (type: string), value (type: string)
                     outputColumnNames: _col0, _col1
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: string)
                       sort order: +
-                      Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                       value expressions: _col1 (type: string)
         Map 6 
             Map Operator Tree:
                 TableScan
                   alias: src10_4
-                  Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                   Select Operator
                     expressions: key (type: string), value (type: string)
                     outputColumnNames: _col0, _col1
-                    Statistics: Num rows: 1 Data size: 114 Basic stats: COMPLETE Column stats: NONE
+                    Statistics: Num rows: 10 Data size: 104 Basic stats: COMPLETE Column stats: NONE
                     Reduce Output Operator
                       key expressions: _col0 (type: string)
                       sort order: +
-                      Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                      Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                       value expressions: _col1 (type: string)
         Reducer 2 
             Reduce Operator Tree:
@@ -333,21 +333,21 @@ STAGE PLANS:
                 condition map:
                      Inner Join 0 to 1
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 1 Data size: 125 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 5 Data size: 57 Basic stats: COMPLETE Column stats: NONE
                 Reduce Output Operator
                   key expressions: _col0 (type: string)
                   sort order: +
-                  Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                   value expressions: _col1 (type: string)
         Reducer 3 
             Reduce Operator Tree:
               Select Operator
                 expressions: KEY.reducesinkkey0 (type: string), VALUE._col0 (type: string)
                 outputColumnNames: _col0, _col1
-                Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                 File Output Operator
                   compressed: false
-                  Statistics: Num rows: 3 Data size: 353 Basic stats: COMPLETE Column stats: NONE
+                  Statistics: Num rows: 25 Data size: 265 Basic stats: COMPLETE Column stats: NONE
                   table:
                       input format: org.apache.hadoop.mapred.TextInputFormat
                       output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
-- 
1.7.9.5


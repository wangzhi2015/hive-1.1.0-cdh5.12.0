From 665dce516090d0a79aa9678402f2eeb59e3abfa4 Mon Sep 17 00:00:00 2001
From: Aihua Xu <axu@cloudera.com>
Date: Mon, 18 May 2015 12:13:28 -0400
Subject: [PATCH 0144/1164] Revert "HIVE-5193 : Columnar Pushdown for RC/ORC
 File not happening in HCatLoader (Viraj Bhat via
 Sushanth Sowmyan)"

This reverts commit ecd430d68e8bc1bdf1a1365780a819cb5d3f7fbc.

Conflicts:
	hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
---
 .../org/apache/hive/hcatalog/pig/HCatLoader.java   |    8 -----
 .../apache/hive/hcatalog/pig/TestHCatLoader.java   |   38 --------------------
 2 files changed, 46 deletions(-)

diff --git a/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java b/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
index 3795cc1..efeaea8 100644
--- a/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
+++ b/hcatalog/hcatalog-pig-adapter/src/main/java/org/apache/hive/hcatalog/pig/HCatLoader.java
@@ -19,7 +19,6 @@
 package org.apache.hive.hcatalog.pig;
 
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.Enumeration;
 import java.util.HashMap;
 import java.util.List;
@@ -32,7 +31,6 @@
 import org.apache.hadoop.hive.common.classification.InterfaceStability;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.hive.ql.metadata.Table;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.mapreduce.InputFormat;
 import org.apache.hadoop.mapreduce.Job;
 import org.apache.hadoop.security.Credentials;
@@ -163,12 +161,6 @@ public void setLocation(String location, Job job) throws IOException {
     if (requiredFieldsInfo != null) {
       // convert to hcatschema and pass to HCatInputFormat
       try {
-        //push down projections to columnar store works for RCFile and ORCFile
-        ArrayList<Integer> list = new ArrayList<Integer>(requiredFieldsInfo.getFields().size());
-        for (RequiredField rf : requiredFieldsInfo.getFields()) {
-          list.add(rf.getIndex());
-        }
-        ColumnProjectionUtils.appendReadColumns(job.getConfiguration(), list);
         outputSchema = phutil.getHCatSchema(requiredFieldsInfo.getFields(), signature, this.getClass());
         HCatInputFormat.setOutputSchema(job, outputSchema);
       } catch (Exception e) {
diff --git a/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java b/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
index f97e511..4bd42bb 100644
--- a/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
+++ b/hcatalog/hcatalog-pig-adapter/src/test/java/org/apache/hive/hcatalog/pig/TestHCatLoader.java
@@ -19,8 +19,6 @@
 package org.apache.hive.hcatalog.pig;
 
 import java.io.File;
-import java.io.FileWriter;
-import java.io.PrintWriter;
 import java.io.IOException;
 import java.io.RandomAccessFile;
 import java.sql.Date;
@@ -39,7 +37,6 @@
 
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
-import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.cli.CliSessionState;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.CommandNeedRetryException;
@@ -48,7 +45,6 @@
 import org.apache.hadoop.hive.ql.io.StorageFormats;
 import org.apache.hadoop.hive.ql.processors.CommandProcessorResponse;
 import org.apache.hadoop.hive.ql.session.SessionState;
-import org.apache.hadoop.hive.serde2.ColumnProjectionUtils;
 import org.apache.hadoop.mapreduce.Job;
 
 import org.apache.hive.hcatalog.HcatTestUtils;
@@ -485,40 +481,6 @@ public void testProjectionsBasic() throws IOException {
   }
 
   @Test
-  public void testColumnarStorePushdown() throws Exception {
-    assumeTrue(!TestUtil.shouldSkip(storageFormat, DISABLED_STORAGE_FORMATS));
-    String PIGOUTPUT_DIR = TEST_DATA_DIR+ "/colpushdownop";
-    String PIG_FILE = "test.pig";
-    String expectedCols = "0,1";
-    PrintWriter w = new PrintWriter(new FileWriter(PIG_FILE));
-    w.println("A = load '" + COMPLEX_TABLE + "' using org.apache.hive.hcatalog.pig.HCatLoader();");
-    w.println("B = foreach A generate name,studentid;");
-    w.println("C = filter B by name is not null;");
-    w.println("store C into '" + PIGOUTPUT_DIR + "' using PigStorage();");
-    w.close();
-
-    try {
-      String[] args = { "-x", "local", PIG_FILE };
-      PigStats stats = PigRunner.run(args, null);
-      //Pig script was successful
-      assertTrue(stats.isSuccessful());
-      //Single MapReduce job is launched
-      OutputStats outstats = stats.getOutputStats().get(0);
-      assertTrue(outstats!= null);
-      assertEquals(expectedCols,outstats.getConf()
-        .get(ColumnProjectionUtils.READ_COLUMN_IDS_CONF_STR));
-      //delete output file on exit
-      FileSystem fs = FileSystem.get(outstats.getConf());
-      if (fs.exists(new Path(PIGOUTPUT_DIR)))
-      {
-        fs.delete(new Path(PIGOUTPUT_DIR), true);
-      }
-    } finally {
-      new File(PIG_FILE).delete();
-    }
-  }
-
-  @Test
   public void testGetInputBytes() throws Exception {
     assumeTrue(!TestUtil.shouldSkip(storageFormat, DISABLED_STORAGE_FORMATS));
     File file = new File(TEST_WAREHOUSE_DIR + "/" + SPECIFIC_SIZE_TABLE + "/part-m-00000");
-- 
1.7.9.5


From f0a48096f950cb017fecc7f5bc8ce9529b375973 Mon Sep 17 00:00:00 2001
From: Chaoyu Tang <ctang@cloudera.com>
Date: Sat, 25 Feb 2017 22:27:33 -0500
Subject: [PATCH 1002/1164] CDH-50829: Backport HIVE-16019: Query fails when
 group by/order by on same column with uppercase
 name (Chaoyu Tang, reviewed by Ashutosh Chauhan)

Change-Id: Ie0e558fefe217f77f4bc6d9d1528958bb0d99fa6
---
 .../hadoop/hive/ql/parse/CalcitePlanner.java       |    6 +-
 .../hadoop/hive/ql/parse/SemanticAnalyzer.java     |    6 +-
 ql/src/test/queries/clientpositive/order3.q        |   26 +
 ql/src/test/results/clientpositive/order3.q.out    |  553 ++++++++++++++++++++
 4 files changed, 585 insertions(+), 6 deletions(-)
 create mode 100644 ql/src/test/queries/clientpositive/order3.q
 create mode 100644 ql/src/test/results/clientpositive/order3.q.out

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
index 4e23c9e..fe7eb86 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/CalcitePlanner.java
@@ -1620,11 +1620,11 @@ private void addAlternateGByKeyMappings(ASTNode gByExpr, ColumnInfo colInfo,
       if (gByExpr.getType() == HiveParser.DOT
           && gByExpr.getChild(0).getType() == HiveParser.TOK_TABLE_OR_COL) {
         String tab_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr.getChild(0).getChild(0)
-            .getText());
-        String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr.getChild(1).getText());
+            .getText().toLowerCase());
+        String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr.getChild(1).getText().toLowerCase());
         gByRR.put(tab_alias, col_alias, colInfo);
       } else if (gByExpr.getType() == HiveParser.TOK_TABLE_OR_COL) {
-        String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr.getChild(0).getText());
+        String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr.getChild(0).getText().toLowerCase());
         String tab_alias = null;
         /*
          * If the input to the GBy has a tab alias for the column, then add an
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
index a955630..833f61b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/SemanticAnalyzer.java
@@ -12349,13 +12349,13 @@ private void addAlternateGByKeyMappings(ASTNode gByExpr, ColumnInfo colInfo,
     if ( gByExpr.getType() == HiveParser.DOT
           && gByExpr.getChild(0).getType() == HiveParser.TOK_TABLE_OR_COL ) {
       String tab_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr
-                .getChild(0).getChild(0).getText());
+                .getChild(0).getChild(0).getText().toLowerCase());
       String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(
-          gByExpr.getChild(1).getText());
+          gByExpr.getChild(1).getText().toLowerCase());
       gByRR.put(tab_alias, col_alias, colInfo);
     } else if ( gByExpr.getType() == HiveParser.TOK_TABLE_OR_COL ) {
       String col_alias = BaseSemanticAnalyzer.unescapeIdentifier(gByExpr
-              .getChild(0).getText());
+              .getChild(0).getText().toLowerCase());
       String tab_alias = null;
       /*
        * If the input to the GBy has a tab alias for the column, then add an entry
diff --git a/ql/src/test/queries/clientpositive/order3.q b/ql/src/test/queries/clientpositive/order3.q
new file mode 100644
index 0000000..3fdf917
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/order3.q
@@ -0,0 +1,26 @@
+-- Test HIVE-16019
+drop table if exists test;
+create table test(key int, value1 int, value2 string);
+insert into table test values (1, 1, 'val111'), (1, 2, 'val121'), (1, 3, 'val131'), (2, 1, 'val211'), (2, 2, 'val221'), (2, 2, 'val222'), (2, 3, 'val231'), (2, 4, 'val241'),
+(3, 1, 'val311'), (3, 2, 'val321'), (3, 2, 'val322'), (3, 3, 'val331'), (3, 3, 'val332'), (3, 3, 'val333'), (4, 1, 'val411');
+
+set hive.cbo.enable=true;
+EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+
+EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+
+EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3;
+SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3;
+
+set hive.cbo.enable=false;
+EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+
+EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3;
+
+EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3;
+SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3;
+
diff --git a/ql/src/test/results/clientpositive/order3.q.out b/ql/src/test/results/clientpositive/order3.q.out
new file mode 100644
index 0000000..cf58e7a
--- /dev/null
+++ b/ql/src/test/results/clientpositive/order3.q.out
@@ -0,0 +1,553 @@
+PREHOOK: query: -- Test HIVE-16019
+drop table if exists test
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: -- Test HIVE-16019
+drop table if exists test
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create table test(key int, value1 int, value2 string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@test
+POSTHOOK: query: create table test(key int, value1 int, value2 string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@test
+PREHOOK: query: insert into table test values (1, 1, 'val111'), (1, 2, 'val121'), (1, 3, 'val131'), (2, 1, 'val211'), (2, 2, 'val221'), (2, 2, 'val222'), (2, 3, 'val231'), (2, 4, 'val241'),
+(3, 1, 'val311'), (3, 2, 'val321'), (3, 2, 'val322'), (3, 3, 'val331'), (3, 3, 'val332'), (3, 3, 'val333'), (4, 1, 'val411')
+PREHOOK: type: QUERY
+PREHOOK: Output: default@test
+POSTHOOK: query: insert into table test values (1, 1, 'val111'), (1, 2, 'val121'), (1, 3, 'val131'), (2, 1, 'val211'), (2, 2, 'val221'), (2, 2, 'val222'), (2, 3, 'val231'), (2, 4, 'val241'),
+(3, 1, 'val311'), (3, 2, 'val321'), (3, 2, 'val322'), (3, 3, 'val331'), (3, 3, 'val332'), (3, 3, 'val333'), (4, 1, 'val411')
+POSTHOOK: type: QUERY
+POSTHOOK: Output: default@test
+POSTHOOK: Lineage: test.key EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col1, type:string, comment:), ]
+POSTHOOK: Lineage: test.value1 EXPRESSION [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col2, type:string, comment:), ]
+POSTHOOK: Lineage: test.value2 SIMPLE [(values__tmp__table__1)values__tmp__table__1.FieldSchema(name:tmp_values_col3, type:string, comment:), ]
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int)
+              outputColumnNames: _col0
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                keys: _col0 (type: int)
+                mode: hash
+                outputColumnNames: _col0
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Group By Operator
+          keys: KEY._col0 (type: int)
+          mode: mergepartial
+          outputColumnNames: _col0
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int)
+              sort order: +
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int)
+          outputColumnNames: _col0
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1
+2
+3
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value1 (type: int)
+              outputColumnNames: _col0, _col1
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: max(_col1)
+                keys: _col0 (type: int)
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: int)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: max(VALUE._col0)
+          keys: KEY._col0 (type: int)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int)
+              sort order: +
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: int)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: int)
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1	3
+2	4
+3	3
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value1 (type: int)
+              outputColumnNames: _col0, _col2
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: count(_col2)
+                keys: _col0 (type: int), 'AAA' (type: string)
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: string)
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: int), _col1 (type: string)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col2 (type: bigint)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0)
+          keys: KEY._col0 (type: int), KEY._col1 (type: string)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: int), _col2 (type: bigint), _col1 (type: string)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int), 'AAA' (type: string)
+              sort order: ++
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: bigint), _col2 (type: string)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: string)
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1	3	AAA
+2	5	AAA
+3	6	AAA
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int)
+              outputColumnNames: key
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                keys: key (type: int)
+                mode: hash
+                outputColumnNames: _col0
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Group By Operator
+          keys: KEY._col0 (type: int)
+          mode: mergepartial
+          outputColumnNames: _col0
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int)
+              sort order: +
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int)
+          outputColumnNames: _col0
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1
+2
+3
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value1 (type: int)
+              outputColumnNames: key, value1
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: max(value1)
+                keys: key (type: int)
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int)
+                  sort order: +
+                  Map-reduce partition columns: _col0 (type: int)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col1 (type: int)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: max(VALUE._col0)
+          keys: KEY._col0 (type: int)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int)
+              sort order: +
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: int)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: int)
+          outputColumnNames: _col0, _col1
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY, MAX(T1.VALUE1) AS MYVALUE1 FROM TEST T1 GROUP BY T1.KEY ORDER BY T1.KEY LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1	3
+2	4
+3	3
+PREHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-2 depends on stages: Stage-1
+  Stage-0 depends on stages: Stage-2
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: t1
+            Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: key (type: int), value1 (type: int)
+              outputColumnNames: key, value1
+              Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+              Group By Operator
+                aggregations: count(value1)
+                keys: key (type: int), 'AAA' (type: string)
+                mode: hash
+                outputColumnNames: _col0, _col1, _col2
+                Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                Reduce Output Operator
+                  key expressions: _col0 (type: int), _col1 (type: string)
+                  sort order: ++
+                  Map-reduce partition columns: _col0 (type: int), _col1 (type: string)
+                  Statistics: Num rows: 15 Data size: 150 Basic stats: COMPLETE Column stats: NONE
+                  value expressions: _col2 (type: bigint)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: count(VALUE._col0)
+          keys: KEY._col0 (type: int), KEY._col1 (type: string)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Select Operator
+            expressions: _col0 (type: int), _col2 (type: bigint), _col1 (type: string)
+            outputColumnNames: _col0, _col1, _col2
+            Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              table:
+                  input format: org.apache.hadoop.mapred.SequenceFileInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe
+
+  Stage: Stage-2
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            Reduce Output Operator
+              key expressions: _col0 (type: int), 'AAA' (type: string)
+              sort order: ++
+              Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+              value expressions: _col1 (type: bigint), _col2 (type: string)
+      Reduce Operator Tree:
+        Select Operator
+          expressions: KEY.reducesinkkey0 (type: int), VALUE._col0 (type: bigint), VALUE._col1 (type: string)
+          outputColumnNames: _col0, _col1, _col2
+          Statistics: Num rows: 7 Data size: 70 Basic stats: COMPLETE Column stats: NONE
+          Limit
+            Number of rows: 3
+            Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+            File Output Operator
+              compressed: false
+              Statistics: Num rows: 3 Data size: 30 Basic stats: COMPLETE Column stats: NONE
+              table:
+                  input format: org.apache.hadoop.mapred.TextInputFormat
+                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                  serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 3
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+PREHOOK: type: QUERY
+PREHOOK: Input: default@test
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT T1.KEY AS MYKEY, COUNT(T1.VALUE1) AS MYVALUE1, 'AAA' AS C FROM TEST T1 GROUP BY T1.KEY, 'AAA' ORDER BY T1.KEY, 'AAA' LIMIT 3
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@test
+#### A masked pattern was here ####
+1	3	AAA
+2	5	AAA
+3	6	AAA
-- 
1.7.9.5


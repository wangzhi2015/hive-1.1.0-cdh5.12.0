From 7fb3563b59e86b4dc318ec07609341376b484fcb Mon Sep 17 00:00:00 2001
From: Mohit Sabharwal <mohit@cloudera.com>
Date: Fri, 29 Jul 2016 14:04:38 -0700
Subject: [PATCH 0719/1164] CDH-42736 : HIVE-10722 : external table creation
 with msck in Hive can create unusable partition
 (Sergey Shelukhin, reviewed by Sushanth Sowmyan)

    (cherry picked from commit 83cc691c5ac1ef5009cc3270f5fc1618dee14d61)

    Change-Id: I5797576cf68c37b8be72e7f260926493e2bdd695

Change-Id: I9357fdb3510a35ad41bbb91b9ef2c801f89145ed
---
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    7 +-
 .../test/resources/testconfiguration.properties    |    2 +
 .../apache/hadoop/hive/metastore/Warehouse.java    |   10 ++-
 .../org/apache/hadoop/hive/ql/exec/DDLTask.java    |   38 +++++++++-
 .../metadata/formatting/JsonMetaDataFormatter.java |    2 +-
 .../ql/metadata/formatting/MetaDataFormatter.java  |    2 +-
 .../metadata/formatting/TextMetaDataFormatter.java |   76 ++++++++++----------
 .../clientnegative/table_nonprintable_negative.q   |   11 +++
 .../queries/clientpositive/table_nonprintable.q    |   30 ++++++++
 .../table_nonprintable_negative.q.out              |   19 +++++
 .../clientpositive/table_nonprintable.q.out        |   72 +++++++++++++++++++
 11 files changed, 225 insertions(+), 44 deletions(-)
 create mode 100644 ql/src/test/queries/clientnegative/table_nonprintable_negative.q
 create mode 100644 ql/src/test/queries/clientpositive/table_nonprintable.q
 create mode 100644 ql/src/test/results/clientnegative/table_nonprintable_negative.q.out
 create mode 100644 ql/src/test/results/clientpositive/table_nonprintable.q.out

diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 124d73c..32aacce 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -2134,7 +2134,12 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {
       "This is only necessary if the host has mutiple network addresses and if a different network address other than " +
       "hive.server2.thrift.bind.host is to be used."),
     NWAYJOINREORDER("hive.reorder.nway.joins", true,
-      "Runs reordering of tables within single n-way join (i.e.: picks streamtable)");
+      "Runs reordering of tables within single n-way join (i.e.: picks streamtable)"),
+    HIVE_MSCK_PATH_VALIDATION("hive.msck.path.validation", "throw",
+        new StringSet("throw", "skip", "ignore"), "The approach msck should take with HDFS " +
+       "directories that are partition-like but contain unsupported characters. 'throw' (an " +
+       "exception) is the default; 'skip' will skip the invalid directories and still repair the" +
+       " others; 'ignore' will skip the validation (legacy behavior, causes bugs in many cases)");
 
     public final String varname;
     private final String defaultExpr;
diff --git a/itests/src/test/resources/testconfiguration.properties b/itests/src/test/resources/testconfiguration.properties
index a31234b..7c24912 100644
--- a/itests/src/test/resources/testconfiguration.properties
+++ b/itests/src/test/resources/testconfiguration.properties
@@ -47,6 +47,7 @@ minimr.query.files=auto_sortmerge_join_16.q,\
   smb_mapjoin_8.q,\
   stats_counter.q,\
   stats_counter_partitioned.q,\
+  table_nonprintable.q,\
   temp_table_external.q,\
   truncate_column_buckets.q,\
   uber_reduce.q,\
@@ -487,6 +488,7 @@ minimr.query.negative.files=cluster_tasklog_retrieval.q,\
   mapreduce_stack_trace_turnoff.q,\
   mapreduce_stack_trace_turnoff_hadoop20.q,\
   minimr_broken_pipe.q,\
+  table_nonprintable_negative.q,\
   udf_local_resource.q
 
 # tests are sorted use: perl -pe 's@\\\s*\n@ @g' testconfiguration.properties \
diff --git a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
index 9f63168..fcf86c0 100755
--- a/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
+++ b/metastore/src/java/org/apache/hadoop/hive/metastore/Warehouse.java
@@ -392,11 +392,16 @@ public static String makeDynamicPartName(Map<String, String> spec) {
    * @param name Partition name.
    * @param result The result. Must be pre-sized to the expected number of columns.
    */
-  public static void makeValsFromName(
+  public static AbstractList<String> makeValsFromName(
       String name, AbstractList<String> result) throws MetaException {
     assert name != null;
     String[] parts = slash.split(name, 0);
-    if (parts.length != result.size()) {
+    if (result == null) {
+      result = new ArrayList<>(parts.length);
+      for (int i = 0; i < parts.length; ++i) {
+        result.add(null);
+      }
+    } else if (parts.length != result.size()) {
       throw new MetaException(
           "Expected " + result.size() + " components, got " + parts.length + " (" + name + ")");
     }
@@ -407,6 +412,7 @@ public static void makeValsFromName(
       }
       result.set(i, unescapePathName(parts[i].substring(eq + 1)));
     }
+    return result;
   }
 
   public static LinkedHashMap<String, String> makeSpecFromName(String name)
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
index 1e4a594..0133e2c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/DDLTask.java
@@ -33,6 +33,7 @@
 import java.net.URISyntaxException;
 import java.nio.charset.StandardCharsets;
 import java.sql.SQLException;
+import java.util.AbstractList;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -59,6 +60,7 @@
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FsShell;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.StatsSetupConst;
 import org.apache.hadoop.hive.common.type.HiveDecimal;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -1707,6 +1709,40 @@ private int msck(Hive db, MsckDesc msckDesc) {
       checker.checkMetastore(names[0], names[1], msckDesc.getPartSpecs(), result);
       List<CheckResult.PartitionResult> partsNotInMs = result.getPartitionsNotInMs();
       if (msckDesc.isRepairPartitions() && !partsNotInMs.isEmpty()) {
+        AbstractList<String> vals = null;
+        String settingStr = HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION);
+        boolean doValidate = !("ignore".equals(settingStr));
+        boolean doSkip = doValidate && "skip".equals(settingStr);
+        // The default setting is "throw"; assume doValidate && !doSkip means throw.
+        if (doValidate) {
+          // Validate that we can add partition without escaping. Escaping was originally intended
+          // to avoid creating invalid HDFS paths; however, if we escape the HDFS path (that we
+          // deem invalid but HDFS actually supports - it is possible to create HDFS paths with
+          // unprintable characters like ASCII 7), metastore will create another directory instead
+          // of the one we are trying to "repair" here.
+          Iterator<CheckResult.PartitionResult> iter = partsNotInMs.iterator();
+          while (iter.hasNext()) {
+            CheckResult.PartitionResult part = iter.next();
+            try {
+              vals = Warehouse.makeValsFromName(part.getPartitionName(), vals);
+            } catch (MetaException ex) {
+              throw new HiveException(ex);
+            }
+            for (String val : vals) {
+              String escapedPath = FileUtils.escapePathName(val);
+              assert escapedPath != null;
+              if (escapedPath.equals(val)) continue;
+              String errorMsg = "Repair: Cannot add partition " + msckDesc.getTableName()
+                  + ':' + part.getPartitionName() + " due to invalid characters in the name";
+              if (doSkip) {
+                repairOutput.add(errorMsg);
+                iter.remove();
+              } else {
+                throw new HiveException(errorMsg);
+              }
+            }
+          }
+        }
         Table table = db.getTable(msckDesc.getTableName());
         AddPartitionDesc apd = new AddPartitionDesc(
             table.getDbName(), table.getTableName(), false);
@@ -1842,7 +1878,7 @@ private int showPartitions(Hive db, ShowPartitionsDesc showParts) throws HiveExc
       FileSystem fs = resFile.getFileSystem(conf);
       outStream = fs.create(resFile);
 
-      formatter.showTablePartitons(outStream, parts);
+      formatter.showTablePartitions(outStream, parts);
 
       outStream.close();
       outStream = null;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
index 818e7ca..92dc81c 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/JsonMetaDataFormatter.java
@@ -313,7 +313,7 @@ private void putFileSystemsStats(MapBuilder builder, List<Path> locations,
    * Show the table partitions.
    */
   @Override
-  public void showTablePartitons(DataOutputStream out, List<String> parts)
+  public void showTablePartitions(DataOutputStream out, List<String> parts)
       throws HiveException {
     asJson(out, MapBuilder.create().put("partitions",
         makeTablePartions(parts)).build());
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java
index 2504e47..55e1b3b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/MetaDataFormatter.java
@@ -93,7 +93,7 @@ public void showTableStatus(DataOutputStream out,
   /**
    * Show the table partitions.
    */
-  public void showTablePartitons(DataOutputStream out,
+  public void showTablePartitions(DataOutputStream out,
       List<String> parts)
           throws HiveException;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
index 8fabea9..a9e500a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/formatting/TextMetaDataFormatter.java
@@ -105,7 +105,7 @@ public void showTables(DataOutputStream out, Set<String> tables)
     try {
       while (iterTbls.hasNext()) {
         // create a row per table name
-        out.writeBytes(iterTbls.next());
+        out.write(iterTbls.next().getBytes("UTF-8"));
         out.write(terminator);
       }
     } catch (IOException e) {
@@ -148,7 +148,7 @@ public void describeTable(DataOutputStream outStream,  String colPath,
           outStream.write(terminator);
           if (part != null) {
             // show partition information
-            outStream.writeBytes("Detailed Partition Information");
+            outStream.write(("Detailed Partition Information").getBytes("UTF-8"));
             outStream.write(separator);
             outStream.write(part.getTPartition().toString().getBytes("UTF-8"));
             outStream.write(separator);
@@ -156,7 +156,7 @@ public void describeTable(DataOutputStream outStream,  String colPath,
             outStream.write(terminator);
           } else {
             // show table information
-            outStream.writeBytes("Detailed Table Information");
+            outStream.write(("Detailed Table Information").getBytes("UTF-8"));
             outStream.write(separator);
             outStream.write(tbl.getTTable().toString().getBytes("UTF-8"));
             outStream.write(separator);
@@ -213,21 +213,21 @@ public void showTableStatus(DataOutputStream outStream,
               "partition_columns", tbl.getPartCols());
         }
 
-        outStream.writeBytes("tableName:" + tableName);
+        outStream.write(("tableName:" + tableName).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("owner:" + owner);
+        outStream.write(("owner:" + owner).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("location:" + tblLoc);
+        outStream.write(("location:" + tblLoc).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("inputformat:" + inputFormattCls);
+        outStream.write(("inputformat:" + inputFormattCls).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("outputformat:" + outputFormattCls);
+        outStream.write(("outputformat:" + outputFormattCls).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("columns:" + ddlCols);
+        outStream.write(("columns:" + ddlCols).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("partitioned:" + isPartitioned);
+        outStream.write(("partitioned:" + isPartitioned).getBytes("UTF-8"));
         outStream.write(terminator);
-        outStream.writeBytes("partitionColumns:" + partitionCols);
+        outStream.write(("partitionColumns:" + partitionCols).getBytes("UTF-8"));
         outStream.write(terminator);
         // output file system information
         Path tblPath = tbl.getPath();
@@ -338,50 +338,50 @@ private void writeFileSystemStats(DataOutputStream outStream,
     String unknownString = "unknown";
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("totalNumberFiles:");
-    outStream.writeBytes(unknown ? unknownString : "" + numOfFiles);
+    outStream.write("totalNumberFiles:".getBytes("UTF-8"));
+    outStream.write((unknown ? unknownString : "" + numOfFiles).getBytes("UTF-8"));
     outStream.write(terminator);
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("totalFileSize:");
-    outStream.writeBytes(unknown ? unknownString : "" + totalFileSize);
+    outStream.write("totalFileSize:".getBytes("UTF-8"));
+    outStream.write((unknown ? unknownString : "" + totalFileSize).getBytes("UTF-8"));
     outStream.write(terminator);
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("maxFileSize:");
-    outStream.writeBytes(unknown ? unknownString : "" + maxFileSize);
+    outStream.write("maxFileSize:".getBytes("UTF-8"));
+    outStream.write((unknown ? unknownString : "" + maxFileSize).getBytes("UTF-8"));
     outStream.write(terminator);
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("minFileSize:");
+    outStream.write("minFileSize:".getBytes("UTF-8"));
     if (numOfFiles > 0) {
-      outStream.writeBytes(unknown ? unknownString : "" + minFileSize);
+      outStream.write((unknown ? unknownString : "" + minFileSize).getBytes("UTF-8"));
     } else {
-      outStream.writeBytes(unknown ? unknownString : "" + 0);
+      outStream.write((unknown ? unknownString : "" + 0).getBytes("UTF-8"));
     }
     outStream.write(terminator);
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("lastAccessTime:");
+    outStream.write("lastAccessTime:".getBytes("UTF-8"));
     outStream.writeBytes((unknown || lastAccessTime < 0) ? unknownString : ""
         + lastAccessTime);
     outStream.write(terminator);
 
     for (int k = 0; k < indent; k++) {
-      outStream.writeBytes(Utilities.INDENT);
+      outStream.write(Utilities.INDENT.getBytes("UTF-8"));
     }
-    outStream.writeBytes("lastUpdateTime:");
-    outStream.writeBytes(unknown ? unknownString : "" + lastUpdateTime);
+    outStream.write("lastUpdateTime:".getBytes("UTF-8"));
+    outStream.write((unknown ? unknownString : "" + lastUpdateTime).getBytes("UTF-8"));
     outStream.write(terminator);
           }
 
@@ -389,7 +389,7 @@ private void writeFileSystemStats(DataOutputStream outStream,
    * Show the table partitions.
    */
   @Override
-  public void showTablePartitons(DataOutputStream outStream, List<String> parts)
+  public void showTablePartitions(DataOutputStream outStream, List<String> parts)
       throws HiveException
       {
     try {
@@ -399,9 +399,9 @@ public void showTablePartitons(DataOutputStream outStream, List<String> parts)
         SessionState ss = SessionState.get();
         if (ss != null && ss.getConf() != null &&
             !ss.getConf().getBoolVar(HiveConf.ConfVars.HIVE_DECODE_PARTITION_NAME)) {
-          outStream.writeBytes(part);
+          outStream.write(part.getBytes("UTF-8"));
         } else {
-          outStream.writeBytes(FileUtils.unescapePathName(part));
+          outStream.write(FileUtils.unescapePathName(part).getBytes("UTF-8"));
         }
         outStream.write(terminator);
       }
@@ -420,7 +420,7 @@ public void showDatabases(DataOutputStream outStream, List<String> databases)
     try {
       for (String database : databases) {
         // create a row per database name
-        outStream.writeBytes(database);
+        outStream.write(database.getBytes("UTF-8"));
         outStream.write(terminator);
       }
     } catch (IOException e) {
@@ -436,26 +436,26 @@ public void showDatabaseDescription(DataOutputStream outStream, String database,
       String location, String ownerName, String ownerType, Map<String, String> params)
           throws HiveException {
     try {
-      outStream.writeBytes(database);
+      outStream.write(database.getBytes("UTF-8"));
       outStream.write(separator);
       if (comment != null) {
         outStream.write(comment.getBytes("UTF-8"));
       }
       outStream.write(separator);
       if (location != null) {
-        outStream.writeBytes(location);
+        outStream.write(location.getBytes("UTF-8"));
       }
       outStream.write(separator);
       if (ownerName != null) {
-        outStream.writeBytes(ownerName);
+        outStream.write(ownerName.getBytes("UTF-8"));
       }
       outStream.write(separator);
       if (ownerType != null) {
-        outStream.writeBytes(ownerType);
+        outStream.write(ownerType.getBytes("UTF-8"));
       }
       outStream.write(separator);
       if (params != null && !params.isEmpty()) {
-        outStream.writeBytes(params.toString());
+        outStream.write(params.toString().getBytes("UTF-8"));
       }
       outStream.write(terminator);
     } catch (IOException e) {
diff --git a/ql/src/test/queries/clientnegative/table_nonprintable_negative.q b/ql/src/test/queries/clientnegative/table_nonprintable_negative.q
new file mode 100644
index 0000000..d7b4830
--- /dev/null
+++ b/ql/src/test/queries/clientnegative/table_nonprintable_negative.q
@@ -0,0 +1,11 @@
+set hive.msck.path.validation=throw;
+
+dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=Foo;
+dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=Foo;
+dfs -ls hdfs:///tmp/temp_table_external/day=Foo;
+
+create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+location 'hdfs:///tmp/temp_table_external';
+
+msck repair table table_external;
diff --git a/ql/src/test/queries/clientpositive/table_nonprintable.q b/ql/src/test/queries/clientpositive/table_nonprintable.q
new file mode 100644
index 0000000..5ae228e
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/table_nonprintable.q
@@ -0,0 +1,30 @@
+set hive.msck.path.validation=skip;
+
+dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=¢Bar;
+dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=¢Bar;
+dfs -ls hdfs:///tmp/temp_table_external/day=¢Bar;
+
+dfs ${system:test.dfs.mkdir} hdfs:///tmp/temp_table_external/day=Foo;
+dfs -copyFromLocal ../../data/files/in1.txt hdfs:///tmp/temp_table_external/day=Foo;
+dfs -ls hdfs:///tmp/temp_table_external/day=Foo;
+
+dfs -ls hdfs:///tmp/temp_table_external;
+
+create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+location 'hdfs:///tmp/temp_table_external';
+
+msck repair table table_external;
+
+dfs -ls hdfs:///tmp/temp_table_external;
+
+show partitions table_external;
+select * from table_external;
+
+alter table table_external drop partition (day='¢Bar');
+
+show partitions table_external;
+
+drop table table_external;
+
+dfs -rmr hdfs:///tmp/temp_table_external;
diff --git a/ql/src/test/results/clientnegative/table_nonprintable_negative.q.out b/ql/src/test/results/clientnegative/table_nonprintable_negative.q.out
new file mode 100644
index 0000000..15af756
--- /dev/null
+++ b/ql/src/test/results/clientnegative/table_nonprintable_negative.q.out
@@ -0,0 +1,19 @@
+Found 1 items
+#### A masked pattern was here ####
+PREHOOK: query: create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+#### A masked pattern was here ####
+PREHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+PREHOOK: Output: database:default
+PREHOOK: Output: default@table_external
+POSTHOOK: query: create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+#### A masked pattern was here ####
+POSTHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@table_external
+PREHOOK: query: msck repair table table_external
+PREHOOK: type: MSCK
+FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
diff --git a/ql/src/test/results/clientpositive/table_nonprintable.q.out b/ql/src/test/results/clientpositive/table_nonprintable.q.out
new file mode 100644
index 0000000..d7c93f2
--- /dev/null
+++ b/ql/src/test/results/clientpositive/table_nonprintable.q.out
@@ -0,0 +1,72 @@
+Found 1 items
+#### A masked pattern was here ####
+Found 1 items
+#### A masked pattern was here ####
+Found 2 items
+#### A masked pattern was here ####
+PREHOOK: query: create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+#### A masked pattern was here ####
+PREHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+PREHOOK: Output: database:default
+PREHOOK: Output: default@table_external
+POSTHOOK: query: create external table table_external (c1 int, c2 int)
+partitioned by (day string)
+#### A masked pattern was here ####
+POSTHOOK: type: CREATETABLE
+#### A masked pattern was here ####
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@table_external
+PREHOOK: query: msck repair table table_external
+PREHOOK: type: MSCK
+POSTHOOK: query: msck repair table table_external
+POSTHOOK: type: MSCK
+Partitions not in metastore:	table_external:day=¢Bar
+Repair: Cannot add partition table_external:day=Foo due to invalid characters in the name
+Repair: Added partition to metastore table_external:day=¢Bar
+Found 2 items
+#### A masked pattern was here ####
+PREHOOK: query: show partitions table_external
+PREHOOK: type: SHOWPARTITIONS
+PREHOOK: Input: default@table_external
+POSTHOOK: query: show partitions table_external
+POSTHOOK: type: SHOWPARTITIONS
+POSTHOOK: Input: default@table_external
+day=¢Bar
+PREHOOK: query: select * from table_external
+PREHOOK: type: QUERY
+PREHOOK: Input: default@table_external
+PREHOOK: Input: default@table_external@day=¢Bar
+#### A masked pattern was here ####
+POSTHOOK: query: select * from table_external
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@table_external
+POSTHOOK: Input: default@table_external@day=¢Bar
+#### A masked pattern was here ####
+NULL	35	¢Bar
+48	NULL	¢Bar
+100	100	¢Bar
+PREHOOK: query: alter table table_external drop partition (day='¢Bar')
+PREHOOK: type: ALTERTABLE_DROPPARTS
+PREHOOK: Input: default@table_external
+PREHOOK: Output: default@table_external@day=¢Bar
+POSTHOOK: query: alter table table_external drop partition (day='¢Bar')
+POSTHOOK: type: ALTERTABLE_DROPPARTS
+POSTHOOK: Input: default@table_external
+POSTHOOK: Output: default@table_external@day=¢Bar
+PREHOOK: query: show partitions table_external
+PREHOOK: type: SHOWPARTITIONS
+PREHOOK: Input: default@table_external
+POSTHOOK: query: show partitions table_external
+POSTHOOK: type: SHOWPARTITIONS
+POSTHOOK: Input: default@table_external
+PREHOOK: query: drop table table_external
+PREHOOK: type: DROPTABLE
+PREHOOK: Input: default@table_external
+PREHOOK: Output: default@table_external
+POSTHOOK: query: drop table table_external
+POSTHOOK: type: DROPTABLE
+POSTHOOK: Input: default@table_external
+POSTHOOK: Output: default@table_external
+#### A masked pattern was here ####
-- 
1.7.9.5


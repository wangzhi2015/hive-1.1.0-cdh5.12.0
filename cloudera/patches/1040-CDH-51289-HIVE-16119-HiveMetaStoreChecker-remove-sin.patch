From 4bac7ab1af20a2c8aee939f0c1b3b7bbb9b3c1fc Mon Sep 17 00:00:00 2001
From: Zoltan Haindrich <kirk@rxd.hu>
Date: Thu, 9 Mar 2017 08:32:35 +0100
Subject: [PATCH 1040/1164] CDH-51289 : HIVE-16119: HiveMetaStoreChecker:
 remove singleThread logic duplication (Zoltan
 Haindrich reviewed by Vihang Karajgaonkar,
 Ashutosh Chauhan)

Change-Id: I66c01e46073a3c29aa86c403a4265b751143a6bf
---
 .../hive/ql/metadata/HiveMetaStoreChecker.java     |   90 ++++----------------
 .../hive/ql/metadata/TestHiveMetaStoreChecker.java |   40 ++++-----
 2 files changed, 36 insertions(+), 94 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
index 8d41197..30607a4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/metadata/HiveMetaStoreChecker.java
@@ -30,13 +30,14 @@
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.concurrent.ExecutorService;
 import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
 import java.util.concurrent.Executors;
 import java.util.concurrent.Future;
+import java.util.concurrent.ThreadFactory;
 import java.util.concurrent.ThreadPoolExecutor;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-import com.google.common.annotations.VisibleForTesting;
 import com.google.common.collect.Sets;
 import org.apache.hadoop.hive.metastore.api.FieldSchema;
 import org.apache.hadoop.fs.FileStatus;
@@ -54,6 +55,7 @@
 import org.apache.hadoop.hive.ql.parse.PrunedPartitionList;
 import org.apache.thrift.TException;
 
+import com.google.common.util.concurrent.MoreExecutors;
 import com.google.common.util.concurrent.ThreadFactoryBuilder;
 
 /**
@@ -388,35 +390,19 @@ private void checkPartitionDirs(Path basePath, Set<Path> allDirs, int maxDepth)
     // pool here the smaller sized pool of the two becomes a bottleneck
     int poolSize = conf.getInt(ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT.varname, 15);
 
-    // Check if too low config is provided for move files. 2x CPU is reasonable max count.
-    poolSize = poolSize == 0 ? poolSize : Math.max(poolSize,
-        getMinPoolSize());
-
-    // Fixed thread pool on need basis
-    final ThreadPoolExecutor pool = poolSize > 0 ? (ThreadPoolExecutor)
-        Executors.newFixedThreadPool(poolSize,
-            new ThreadFactoryBuilder().setDaemon(true).setNameFormat("MSCK-GetPaths-%d").build()) : null;
-
-    if (pool == null) {
-      LOG.debug("Not-using threaded version of MSCK-GetPaths");
-      Queue<Path> basePaths = new LinkedList<>();
-      basePaths.add(basePath);
-      checkPartitionDirsSingleThreaded(basePaths, allDirs, basePath.getFileSystem(conf), maxDepth,
-          maxDepth);
+    ExecutorService executor;
+    if (poolSize <= 1) {
+      LOG.debug("Using single-threaded version of MSCK-GetPaths");
+      executor = MoreExecutors.sameThreadExecutor();
     } else {
-      LOG.debug("Using multi-threaded version of MSCK-GetPaths with number of threads "
-          + pool.getMaximumPoolSize());
-      checkPartitionDirsInParallel((ThreadPoolExecutor) pool, basePath, allDirs,
-          basePath.getFileSystem(conf), maxDepth);
-    }
-    if (pool != null) {
-      pool.shutdown();
+      LOG.debug("Using multi-threaded version of MSCK-GetPaths with number of threads " + poolSize);
+      ThreadFactory threadFactory =
+          new ThreadFactoryBuilder().setDaemon(true).setNameFormat("MSCK-GetPaths-%d").build();
+      executor = (ThreadPoolExecutor) Executors.newFixedThreadPool(poolSize, threadFactory);
     }
-  }
+    checkPartitionDirs(executor, basePath, allDirs, basePath.getFileSystem(conf), maxDepth);
 
-  @VisibleForTesting
-  int getMinPoolSize() {
-    return Runtime.getRuntime().availableProcessors() * 2;
+    executor.shutdown();
   }
 
   private final class PathDepthInfoCallable implements Callable<Path> {
@@ -492,7 +478,7 @@ private Path processPathDepthInfo(final PathDepthInfo pd)
     }
   }
 
-  private void checkPartitionDirsInParallel(final ThreadPoolExecutor pool,
+  private void checkPartitionDirs(final ExecutorService executor,
       final Path basePath, final Set<Path> result,
       final FileSystem fs, final int maxDepth) throws HiveException {
     try {
@@ -511,7 +497,7 @@ private void checkPartitionDirsInParallel(final ThreadPoolExecutor pool,
         //process each level in parallel
         while(!nextLevel.isEmpty()) {
           futures.add(
-              pool.submit(new PathDepthInfoCallable(nextLevel.poll(), maxDepth, fs, tempQueue)));
+              executor.submit(new PathDepthInfoCallable(nextLevel.poll(), maxDepth, fs, tempQueue)));
         }
         while(!futures.isEmpty()) {
           Path p = futures.poll().get();
@@ -524,52 +510,8 @@ private void checkPartitionDirsInParallel(final ThreadPoolExecutor pool,
       }
     } catch (InterruptedException | ExecutionException e) {
       LOG.error(e.getMessage());
-      pool.shutdownNow();
+      executor.shutdownNow();
       throw new HiveException(e.getCause());
     }
   }
-
-  /*
-   * Original recursive implementation works well for single threaded use-case but has limitations
-   * if we attempt to parallelize this directly
-   */
-  private void checkPartitionDirsSingleThreaded(Queue<Path> basePaths, final Set<Path> allDirs,
-      final FileSystem fs, final int depth, final int maxDepth) throws IOException, HiveException {
-    for (final Path path : basePaths) {
-      FileStatus[] statuses = fs.listStatus(path, FileUtils.HIDDEN_FILES_PATH_FILTER);
-      final Queue<Path> nextLevel = new LinkedList<>();
-      boolean fileFound = false;
-      for (FileStatus status : statuses) {
-        if (status.isDirectory()) {
-          nextLevel.add(status.getPath());
-        } else {
-          fileFound = true;
-        }
-      }
-      if (depth != 0) {
-        // we are in the middle of the search and we find a file
-        if (fileFound) {
-          if ("throw".equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION))) {
-            throw new HiveException(
-                "MSCK finds a file rather than a folder when it searches for " + path.toString());
-          } else {
-            LOG.warn("MSCK finds a file rather than a folder when it searches for "
-                + path.toString());
-          }
-        }
-        if (!nextLevel.isEmpty()) {
-          checkPartitionDirsSingleThreaded(nextLevel, allDirs, fs, depth - 1, maxDepth);
-        } else if (depth != maxDepth) {
-          // since nextLevel is empty, we are missing partition columns.
-          if ("throw".equals(HiveConf.getVar(conf, HiveConf.ConfVars.HIVE_MSCK_PATH_VALIDATION))) {
-            throw new HiveException("MSCK is missing partition columns under " + path.toString());
-          } else {
-            LOG.warn("MSCK is missing partition columns under " + path.toString());
-          }
-        }
-      } else {
-        allDirs.add(path);
-      }
-    }
-  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
index 648332f..d8a03b2 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/metadata/TestHiveMetaStoreChecker.java
@@ -16,7 +16,7 @@
  * limitations under the License.
  */
 package org.apache.hadoop.hive.ql.metadata;
-import java.io.File;
+
 import java.io.IOException;
 import java.util.ArrayList;
 import java.util.Collections;
@@ -24,9 +24,6 @@
 import java.util.List;
 import java.util.Map;
 
-import com.google.common.collect.Lists;
-import junit.framework.TestCase;
-
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -44,6 +41,10 @@
 import org.apache.thrift.TException;
 import org.mockito.Mockito;
 
+import com.google.common.collect.Lists;
+
+import junit.framework.TestCase;
+
 /**
  * TestHiveMetaStoreChecker.
  *
@@ -354,11 +355,7 @@ public void testSingleThreadedDeeplyNestedTables()
       throws HiveException, AlreadyExistsException, IOException {
     // set num of threads to 0 so that single-threaded checkMetastore is called
     hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 0);
-    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs
-    // no other easy way to set it deterministically for this test case
-    checker = Mockito.spy(checker);
-    Mockito.when(checker.getMinPoolSize()).thenReturn(2);
-    int poolSize = checker.getMinPoolSize();
+    int poolSize = 2;
     // create a deeply nested table which has more partition keys than the pool size
     Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);
     // add 10 partitions on the filesystem
@@ -380,11 +377,8 @@ public void testSingleThreadedDeeplyNestedTables()
    */
   public void testDeeplyNestedPartitionedTables()
       throws HiveException, AlreadyExistsException, IOException {
-    // currently HiveMetastoreChecker uses a minimum pool size of 2*numOfProcs
-    // no other easy way to set it deterministically for this test case
-    int poolSize = checker.getMinPoolSize();
-    checker = Mockito.spy(checker);
-    Mockito.when(checker.getMinPoolSize()).thenReturn(2);
+    hive.getConf().setIntVar(HiveConf.ConfVars.METASTORE_FS_HANDLER_THREADS_COUNT, 2);
+    int poolSize = 2;
     // create a deeply nested table which has more partition keys than the pool size
     Table testTable = createPartitionedTestTable(dbName, tableName, poolSize + 2, 0);
     // add 10 partitions on the filesystem
@@ -415,18 +409,22 @@ public void testErrorForMissingPartitionColumn() throws AlreadyExistsException,
     createDirectory(sb.toString());
     //check result now
     CheckResult result = new CheckResult();
+    Exception exception = null;
     try {
       checker.checkMetastore(dbName, tableName, null, result);
     } catch (Exception e) {
-      assertTrue("Expected exception HiveException got " + e.getClass(), e instanceof HiveException);
+      exception = e;
     }
+    assertTrue("Expected HiveException", exception!=null && exception instanceof HiveException);
     createFile(sb.toString(), "dummyFile");
     result = new CheckResult();
+    exception = null;
     try {
       checker.checkMetastore(dbName, tableName, null, result);
     } catch (Exception e) {
-      assertTrue("Expected exception HiveException got " + e.getClass(), e instanceof HiveException);
+      exception = e;
     }
+    assertTrue("Expected HiveException", exception!=null && exception instanceof HiveException);
   }
 
   /*
@@ -447,20 +445,22 @@ public void testErrorForMissingPartitionsSingleThreaded()
     createDirectory(sb.toString());
     // check result now
     CheckResult result = new CheckResult();
+    Exception exception = null;
     try {
       checker.checkMetastore(dbName, tableName, null, result);
     } catch (Exception e) {
-      assertTrue("Expected exception HiveException got " + e.getClass(),
-          e instanceof HiveException);
+      exception = e;
     }
+    assertTrue("Expected HiveException", exception!=null && exception instanceof HiveException);
     createFile(sb.toString(), "dummyFile");
     result = new CheckResult();
+    exception = null;
     try {
       checker.checkMetastore(dbName, tableName, null, result);
     } catch (Exception e) {
-      assertTrue("Expected exception HiveException got " + e.getClass(),
-          e instanceof HiveException);
+      exception = e;
     }
+    assertTrue("Expected HiveException", exception!=null && exception instanceof HiveException);
   }
   /**
    * Creates a test partitioned table with the required level of nested partitions and number of
-- 
1.7.9.5


From 8e54ac0b97729f44f05c457cf01f78cebe41181a Mon Sep 17 00:00:00 2001
From: Brock Noland <brock@apache.org>
Date: Sat, 21 Feb 2015 01:21:31 +0000
Subject: [PATCH 0049/1164] HIVE-9726 - Upgrade to spark 1.3 [Spark Branch]

git-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1661265 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 88996a87f7a784f1385b238081f5ed9ab13dcb93)

Conflicts:
	pom.xml
---
 itests/pom.xml                                     |    4 ++--
 pom.xml                                            |    8 --------
 .../exec/spark/status/impl/JobMetricsListener.java |   12 ++++++++++++
 .../apache/hadoop/hive/shims/Hadoop23Shims.java    |    1 +
 .../org/apache/hive/spark/client/RemoteDriver.java |   12 ++++++++++++
 5 files changed, 27 insertions(+), 10 deletions(-)

diff --git a/itests/pom.xml b/itests/pom.xml
index 2d54757..f6e59b9 100644
--- a/itests/pom.xml
+++ b/itests/pom.xml
@@ -88,10 +88,10 @@
                      curl -Sso $DOWNLOAD_DIR/$tarName $url
                     fi
                     tar -zxf $DOWNLOAD_DIR/$tarName -C $BASE_DIR
-                    mv $BASE_DIR/spark-1.2.0-bin-hadoop2-without-hive $BASE_DIR/$finalName
+                    mv $BASE_DIR/spark-1.3.0-bin-hadoop2-without-hive $BASE_DIR/$finalName
                   }
                   mkdir -p $DOWNLOAD_DIR
-                  download "http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-1.2.0-bin-hadoop2-without-hive.tgz" "spark"
+                  download "http://d3jw87u4immizc.cloudfront.net/spark-tarball/spark-1.3.0-bin-hadoop2-without-hive.tgz" "spark"
                   cp -f $HIVE_ROOT/data/conf/spark/log4j.properties $BASE_DIR/spark/conf/
                 </echo>
               </target>
diff --git a/pom.xml b/pom.xml
index 87bc407..135d431 100644
--- a/pom.xml
+++ b/pom.xml
@@ -235,14 +235,6 @@
          <enabled>false</enabled>
        </snapshots>
     </repository>
-    <repository>
-      <id>apache.snapshots</id>
-      <name>Apache Snapshot Repository</name>
-      <url>http://repository.apache.org/snapshots</url>
-      <releases>
-        <enabled>false</enabled>
-      </releases>
-    </repository>
   </repositories>
 
   <!-- Hadoop dependency management is done at the bottom under profiles -->
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
index 8243d4d..51772cd 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/impl/JobMetricsListener.java
@@ -39,6 +39,8 @@
 import org.apache.spark.scheduler.SparkListenerTaskGettingResult;
 import org.apache.spark.scheduler.SparkListenerTaskStart;
 import org.apache.spark.scheduler.SparkListenerUnpersistRDD;
+import org.apache.spark.scheduler.SparkListenerExecutorRemoved;
+import org.apache.spark.scheduler.SparkListenerExecutorAdded;
 
 import com.google.common.collect.Lists;
 import com.google.common.collect.Maps;
@@ -52,6 +54,16 @@
   private final Map<Integer, Map<String, List<TaskMetrics>>> allJobMetrics = Maps.newHashMap();
 
   @Override
+  public void onExecutorRemoved(SparkListenerExecutorRemoved removed) {
+
+  }
+
+  @Override
+  public void onExecutorAdded(SparkListenerExecutorAdded added) {
+
+  }
+
+  @Override
   public void onStageCompleted(SparkListenerStageCompleted stageCompleted) {
 
   }
diff --git a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
index 446f0dc..8ccf8e3 100644
--- a/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
+++ b/shims/0.23/src/main/java/org/apache/hadoop/hive/shims/Hadoop23Shims.java
@@ -451,6 +451,7 @@ public MiniSparkShim(Configuration conf, int numberOfTaskTrackers,
 
       mr = new MiniSparkOnYARNCluster("sparkOnYarn");
       conf.set("fs.defaultFS", nameNode);
+      conf.set("yarn.resourcemanager.scheduler.class", "org.apache.hadoop.yarn.server.resourcemanager.scheduler.fair.FairScheduler");
       mr.init(conf);
       mr.start();
       this.conf = mr.getConfig();
diff --git a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java b/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
index 8bda02b..c2ac0c2 100644
--- a/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
+++ b/spark-client/src/main/java/org/apache/hive/spark/client/RemoteDriver.java
@@ -57,6 +57,8 @@
 import org.apache.spark.scheduler.SparkListenerTaskGettingResult;
 import org.apache.spark.scheduler.SparkListenerTaskStart;
 import org.apache.spark.scheduler.SparkListenerUnpersistRDD;
+import org.apache.spark.scheduler.SparkListenerExecutorRemoved;
+import org.apache.spark.scheduler.SparkListenerExecutorAdded;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -425,6 +427,16 @@ private void monitorJob(JavaFutureAction<?> job,
     private final Map<Integer, Integer> stageToJobId = Maps.newHashMap();
 
     @Override
+    public void onExecutorRemoved(SparkListenerExecutorRemoved removed) {
+
+    }
+
+    @Override
+    public void onExecutorAdded(SparkListenerExecutorAdded added) {
+
+    }
+
+    @Override
     public void onJobStart(SparkListenerJobStart jobStart) {
       synchronized (stageToJobId) {
         for (int i = 0; i < jobStart.stageIds().length(); i++) {
-- 
1.7.9.5


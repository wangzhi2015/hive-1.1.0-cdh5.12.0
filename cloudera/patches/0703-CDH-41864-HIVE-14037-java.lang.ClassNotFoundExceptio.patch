From 74bc0b20faa80528d21f837a5d296cab98dda8cc Mon Sep 17 00:00:00 2001
From: Aihua Xu <aihuaxu@apache.org>
Date: Fri, 1 Jul 2016 14:44:08 -0400
Subject: [PATCH 0703/1164] CDH-41864: HIVE-14037:
 java.lang.ClassNotFoundException for the jar in
 hive.reloadable.aux.jars.path in mapreduce
 (Reviewed by Ferdinand Xu) CDH-41961: HIVE-14142:
 java.lang.ClassNotFoundException for the jar in
 hive.reloadable.aux.jars.path for Hive on Spark
 (Aihua Xu, reviewed by Ferdinand Xu) CDH-42461:
 HIVE-14198: Refactor aux jar related code to make
 them more consistent (Aihua Xu, reviewed by Mohit
 Sabharwal) CDH-42491: HIVE-14229: the jars in
 hive.aux.jar.paths are not added to session
 classpath (Aihua Xu, reviewed by Mohit Sabharwal)

Change-Id: I9cdc51eb54aeebe31e9b56add8df719bb280b5d8
---
 .../org/apache/hadoop/hive/common/FileUtils.java   |   59 +++++++++++++
 .../java/org/apache/hadoop/hive/conf/HiveConf.java |    9 +-
 .../apache/hive/common/util/HiveStringUtils.java   |   21 ++++-
 .../apache/hadoop/hive/common/TestFileUtils.java   |   66 +++++++++++++++
 .../apache/hadoop/hive/hbase/HBaseTestSetup.java   |    7 +-
 .../org/apache/hadoop/hive/ql/exec/Utilities.java  |   32 +------
 .../apache/hadoop/hive/ql/exec/mr/ExecDriver.java  |   88 ++++++++------------
 .../apache/hadoop/hive/ql/exec/mr/MapRedTask.java  |   26 ++----
 .../hive/ql/exec/spark/LocalHiveSparkClient.java   |    3 +-
 .../hive/ql/exec/spark/RemoteHiveSparkClient.java  |    9 +-
 .../hadoop/hive/ql/exec/spark/SparkUtilities.java  |   14 ----
 .../hadoop/hive/ql/processors/ReloadProcessor.java |    2 +-
 .../hadoop/hive/ql/session/SessionState.java       |   72 ++++++++++------
 .../apache/hadoop/hive/ql/exec/TestUtilities.java  |   27 ------
 .../hadoop/hive/ql/session/TestSessionState.java   |    8 +-
 ql/src/test/queries/clientpositive/reloadJar.q     |   17 ++++
 ql/src/test/results/clientpositive/reloadJar.q.out |   64 ++++++++++++++
 .../hive/service/cli/session/HiveSessionImpl.java  |    3 +-
 18 files changed, 341 insertions(+), 186 deletions(-)
 create mode 100644 common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
 create mode 100644 ql/src/test/queries/clientpositive/reloadJar.q
 create mode 100644 ql/src/test/results/clientpositive/reloadJar.q.out

diff --git a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
index 08f29a9..6fae34e 100644
--- a/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
+++ b/common/src/java/org/apache/hadoop/hive/common/FileUtils.java
@@ -26,7 +26,10 @@
 import java.security.AccessControlException;
 import java.security.PrivilegedExceptionAction;
 import java.util.BitSet;
+import java.util.Collection;
+import java.util.HashSet;
 import java.util.List;
+import java.util.Set;
 
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
@@ -34,6 +37,7 @@
 import org.apache.hadoop.fs.FileStatus;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.fs.GlobFilter;
 import org.apache.hadoop.fs.LocalFileSystem;
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.fs.PathFilter;
@@ -850,4 +854,59 @@ public static boolean deleteTmpFile(File tempFile) {
     }
     return false;
   }
+
+  /**
+   * Get the URI of the path. Assume to be local file system if no scheme.
+   */
+  public static URI getURI(String path) throws URISyntaxException {
+    if (path == null) {
+      return null;
+    }
+
+    URI uri = new URI(path);
+    if (uri.getScheme() == null) {
+      // if no scheme in the path, we assume it's file on local fs.
+      uri = new File(path).toURI();
+    }
+
+    return uri;
+  }
+
+  /**
+   * Given a path string, get all the jars from the folder or the files themselves.
+   *
+   * @param pathString  the path string is the comma-separated path list
+   * @return            the list of the file names in the format of URI formats.
+   */
+  public static Set<String> getJarFilesByPath(String pathString, Configuration conf) {
+    Set<String> result = new HashSet<String>();
+    if (pathString == null || org.apache.commons.lang.StringUtils.isBlank(pathString)) {
+        return result;
+    }
+
+    String[] paths = pathString.split(",");
+    for(String path : paths) {
+      try {
+        Path p = new Path(getURI(path));
+        FileSystem fs = p.getFileSystem(conf);
+        if (!fs.exists(p)) {
+          LOG.error("The jar file path " + path + " doesn't exist");
+          continue;
+        }
+        if (fs.isDirectory(p)) {
+          // add all jar files under the folder
+          FileStatus[] files = fs.listStatus(p, new GlobFilter("*.jar"));
+          for(FileStatus file : files) {
+            result.add(file.getPath().toUri().toString());
+          }
+        } else {
+          result.add(p.toUri().toString());
+        }
+      } catch(URISyntaxException | IOException e) {
+        LOG.error("Invalid file path " + path, e);
+      }
+    }
+    return result;
+  }
+
 }
diff --git a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
index 6d706e6..124d73c 100644
--- a/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
+++ b/common/src/java/org/apache/hadoop/hive/conf/HiveConf.java
@@ -42,6 +42,7 @@
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
+import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.common.classification.InterfaceAudience.LimitedPrivate;
 import org.apache.hadoop.hive.conf.Validator.PatternSet;
 import org.apache.hadoop.hive.conf.Validator.RangeValidator;
@@ -661,7 +662,8 @@ public void setSparkConfigUpdated(boolean isSparkConfigUpdated) {
 
     // reloadable jars
     HIVERELOADABLEJARS("hive.reloadable.aux.jars.path", "",
-        "Jars can be renewed by executing reload command. And these jars can be "
+        "The locations of the plugin jars, which can be a comma-separated folders or jars. Jars can be renewed\n"
+        + "by executing reload command. And these jars can be "
             + "used as the auxiliary classes like creating a UDF or SerDe."),
 
     // hive added files and jars
@@ -2711,7 +2713,7 @@ private void initialize(Class<?> cls) {
     }
 
     if (auxJars == null) {
-      auxJars = this.get(ConfVars.HIVEAUXJARS.varname);
+      auxJars = StringUtils.join(FileUtils.getJarFilesByPath(this.get(ConfVars.HIVEAUXJARS.varname), this), ',');
     }
 
     if (getBoolVar(ConfVars.METASTORE_SCHEMA_VERIFICATION)) {
@@ -2955,7 +2957,8 @@ public String getAuxJars() {
   }
 
   /**
-   * @param auxJars the auxJars to set
+   * Set the auxiliary jars. Used for unit tests only.
+   * @param auxJars the auxJars to set.
    */
   public void setAuxJars(String auxJars) {
     this.auxJars = auxJars;
diff --git a/common/src/java/org/apache/hive/common/util/HiveStringUtils.java b/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
index 991d6ec..8aa91c4 100644
--- a/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
+++ b/common/src/java/org/apache/hive/common/util/HiveStringUtils.java
@@ -42,6 +42,7 @@
 import com.google.common.collect.Interner;
 import com.google.common.collect.Interners;
 
+import org.apache.commons.lang.StringUtils;
 import org.apache.commons.lang3.text.translate.CharSequenceTranslator;
 import org.apache.commons.lang3.text.translate.EntityArrays;
 import org.apache.commons.lang3.text.translate.LookupTranslator;
@@ -901,6 +902,24 @@ public static String join(CharSequence separator, Iterable<?> strings) {
   }
 
   /**
+   * Concatenates strings, using a separator. Empty/blank string or null will be
+   * ignored.
+   *
+   * @param strings Strings to join.
+   * @param separator Separator to join with.
+   */
+  public static String joinIgnoringEmpty(String[] strings, char separator) {
+    ArrayList<String> list = new ArrayList<String>();
+    for(String str : strings) {
+      if (StringUtils.isNotBlank(str)) {
+        list.add(str);
+      }
+    }
+
+    return StringUtils.join(list, separator);
+  }
+
+  /**
    * Convert SOME_STUFF to SomeStuff
    *
    * @param s input string
@@ -911,7 +930,7 @@ public static String camelize(String s) {
     String[] words = split(s.toLowerCase(Locale.US), ESCAPE_CHAR, '_');
 
     for (String word : words) {
-      sb.append(org.apache.commons.lang.StringUtils.capitalize(word));
+      sb.append(StringUtils.capitalize(word));
     }
 
     return sb.toString();
diff --git a/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java b/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
new file mode 100644
index 0000000..e9fcc13
--- /dev/null
+++ b/common/src/test/org/apache/hadoop/hive/common/TestFileUtils.java
@@ -0,0 +1,66 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.hadoop.hive.common;
+
+import java.io.File;
+import java.io.IOException;
+import java.util.Set;
+
+import org.apache.hadoop.hive.conf.HiveConf;
+import org.junit.Assert;
+import org.junit.Test;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import com.google.common.collect.Sets;
+import com.google.common.io.Files;
+
+import junit.framework.TestCase;
+
+public class TestFileUtils extends TestCase {
+  public static final Logger LOG = LoggerFactory.getLogger(TestFileUtils.class);
+
+  @Test
+  public void testGetJarFilesByPath() {
+    HiveConf conf = new HiveConf(this.getClass());
+    File tmpDir = Files.createTempDir();
+    String jarFileName1 = tmpDir.getAbsolutePath() + File.separator + "a.jar";
+    String jarFileName2 = tmpDir.getAbsolutePath() + File.separator + "b.jar";
+    File jarFile1 = new File(jarFileName1);
+    try {
+      org.apache.commons.io.FileUtils.touch(jarFile1);
+      Set<String> jars = FileUtils.getJarFilesByPath(tmpDir.getAbsolutePath(), conf);
+      Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1),jars);
+
+      jars = FileUtils.getJarFilesByPath("/folder/not/exist", conf);
+      Assert.assertTrue(jars.isEmpty());
+
+      File jarFile2 = new File(jarFileName2);
+      org.apache.commons.io.FileUtils.touch(jarFile2);
+      String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2 + ",/file/not/exist";
+      jars = FileUtils.getJarFilesByPath(newPath, conf);
+      Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1, "file://" + jarFileName2), jars);
+    } catch (IOException e) {
+      LOG.error("failed to copy file to reloading folder", e);
+      Assert.fail(e.getMessage());
+    } finally {
+      org.apache.commons.io.FileUtils.deleteQuietly(tmpDir);
+    }
+  }
+}
diff --git a/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java b/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
index e6383dc..42f85c8 100644
--- a/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
+++ b/itests/util/src/main/java/org/apache/hadoop/hive/hbase/HBaseTestSetup.java
@@ -25,6 +25,7 @@
 import junit.extensions.TestSetup;
 import junit.framework.Test;
 
+import org.apache.commons.lang.StringUtils;
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hbase.HBaseConfiguration;
 import org.apache.hadoop.hbase.HColumnDescriptor;
@@ -66,10 +67,10 @@ void preTest(HiveConf conf) throws Exception {
     conf.set("hbase.master", hbaseCluster.getMaster().getServerName().getHostAndPort());
     conf.set("hbase.zookeeper.property.clientPort", Integer.toString(zooKeeperPort));
     String auxJars = conf.getAuxJars();
-    auxJars = ((auxJars == null) ? "" : (auxJars + ",")) + "file:///"
+    auxJars = (StringUtils.isBlank(auxJars) ? "" : (auxJars + ",")) + "file://"
       + new JobConf(conf, HBaseConfiguration.class).getJar();
-    auxJars += ",file:///" + new JobConf(conf, HBaseSerDe.class).getJar();
-    auxJars += ",file:///" + new JobConf(conf, Watcher.class).getJar();
+    auxJars += ",file://" + new JobConf(conf, HBaseSerDe.class).getJar();
+    auxJars += ",file://" + new JobConf(conf, Watcher.class).getJar();
     conf.setAuxJars(auxJars);
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
index 9124b1d..92e1a08 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/Utilities.java
@@ -38,6 +38,7 @@
 import java.io.Serializable;
 import java.io.UnsupportedEncodingException;
 import java.net.URI;
+import java.net.URISyntaxException;
 import java.net.URL;
 import java.net.URLClassLoader;
 import java.security.MessageDigest;
@@ -2180,40 +2181,13 @@ private static URL urlFromPathString(String onestr) {
     return oneurl;
   }
 
-    /**
-     * get the jar files from specified directory or get jar files by several jar names sperated by comma
-     * @param path
-     * @return
-     */
-    public static Set<String> getJarFilesByPath(String path){
-        Set<String> result = new HashSet<String>();
-        if (path == null || path.isEmpty()) {
-            return result;
-        }
-
-        File paths = new File(path);
-        if (paths.exists() && paths.isDirectory()) {
-            // add all jar files under the reloadable auxiliary jar paths
-            Set<File> jarFiles = new HashSet<File>();
-            jarFiles.addAll(org.apache.commons.io.FileUtils.listFiles(
-                    paths, new String[]{"jar"}, true));
-            for (File f : jarFiles) {
-                result.add(f.getAbsolutePath());
-            }
-        } else {
-            String[] files = path.split(",");
-            Collections.addAll(result, files);
-        }
-        return result;
-    }
-
   /**
    * Add new elements to the classpath.
    *
    * @param newPaths
    *          Array of classpath elements
    */
-  public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) throws Exception {
+  public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths) {
     URLClassLoader loader = (URLClassLoader) cloader;
     List<URL> curPath = Arrays.asList(loader.getURLs());
     ArrayList<URL> newPath = new ArrayList<URL>();
@@ -2240,7 +2214,7 @@ public static ClassLoader addToClassPath(ClassLoader cloader, String[] newPaths)
    * @param pathsToRemove
    *          Array of classpath elements
    */
-  public static void removeFromClassPath(String[] pathsToRemove) throws Exception {
+  public static void removeFromClassPath(String[] pathsToRemove) throws IOException {
     Thread curThread = Thread.currentThread();
     URLClassLoader loader = (URLClassLoader) curThread.getContextClassLoader();
     Set<URL> newPath = new HashSet<URL>(Arrays.asList(loader.getURLs()));
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
index f3fcfbd..c787424 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/ExecDriver.java
@@ -89,6 +89,7 @@
 import org.apache.hadoop.mapred.RunningJob;
 import org.apache.hadoop.security.UserGroupInformation;
 import org.apache.log4j.Appender;
+import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.log4j.BasicConfigurator;
 import org.apache.log4j.FileAppender;
 import org.apache.log4j.LogManager;
@@ -137,6 +138,26 @@ private void initializeFiles(String prop, String files) {
   }
 
   /**
+   * Retrieve the resources from the current session and configuration for the given type.
+   * @return Comma-separated list of resources
+   */
+  protected static String getResource(HiveConf conf, SessionState.ResourceType resType) {
+    switch(resType) {
+    case JAR:
+      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
+      String auxJars = conf.getAuxJars();
+      String reloadableAuxJars = SessionState.get() == null ? null : SessionState.get().getReloadableAuxJars();
+      return HiveStringUtils.joinIgnoringEmpty(new String[]{addedJars, auxJars, reloadableAuxJars}, ',');
+    case FILE:
+      return Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
+    case ARCHIVE:
+      return Utilities.getResourceFiles(conf, SessionState.ResourceType.ARCHIVE);
+    }
+
+    return null;
+  }
+
+  /**
    * Initialization when invoked from QL.
    */
   @Override
@@ -145,25 +166,10 @@ public void initialize(HiveConf conf, QueryPlan queryPlan, DriverContext driverC
 
     job = new JobConf(conf, ExecDriver.class);
 
-    // NOTE: initialize is only called if it is in non-local mode.
-    // In case it's in non-local mode, we need to move the SessionState files
-    // and jars to jobConf.
-    // In case it's in local mode, MapRedTask will set the jobConf.
-    //
-    // "tmpfiles" and "tmpjars" are set by the method ExecDriver.execute(),
-    // which will be called by both local and NON-local mode.
-    String addedFiles = Utilities.getResourceFiles(job, SessionState.ResourceType.FILE);
-    if (StringUtils.isNotBlank(addedFiles)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDFILES, addedFiles);
-    }
-    String addedJars = Utilities.getResourceFiles(job, SessionState.ResourceType.JAR);
-    if (StringUtils.isNotBlank(addedJars)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDJARS, addedJars);
-    }
-    String addedArchives = Utilities.getResourceFiles(job, SessionState.ResourceType.ARCHIVE);
-    if (StringUtils.isNotBlank(addedArchives)) {
-      HiveConf.setVar(job, ConfVars.HIVEADDEDARCHIVES, addedArchives);
-    }
+    initializeFiles("tmpjars", getResource(conf, SessionState.ResourceType.JAR));
+    initializeFiles("tmpfiles", getResource(conf, SessionState.ResourceType.FILE));
+    initializeFiles("tmparchives", getResource(conf, SessionState.ResourceType.ARCHIVE));
+
     conf.stripHiddenConfigurations(job);
     this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);
   }
@@ -293,29 +299,10 @@ public int execute(DriverContext driverContext) {
       throw new RuntimeException(e.getMessage(), e);
     }
 
-
     // No-Op - we don't really write anything here ..
     job.setOutputKeyClass(Text.class);
     job.setOutputValueClass(Text.class);
 
-    // Transfer HIVEAUXJARS and HIVEADDEDJARS to "tmpjars" so hadoop understands
-    // it
-    String auxJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEAUXJARS);
-    String addedJars = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDJARS);
-    if (StringUtils.isNotBlank(auxJars) || StringUtils.isNotBlank(addedJars)) {
-      String allJars = StringUtils.isNotBlank(auxJars) ? (StringUtils.isNotBlank(addedJars) ? addedJars
-          + "," + auxJars
-          : auxJars)
-          : addedJars;
-      LOG.info("adding libjars: " + allJars);
-      initializeFiles("tmpjars", allJars);
-    }
-
-    // Transfer HIVEADDEDFILES to "tmpfiles" so hadoop understands it
-    String addedFiles = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDFILES);
-    if (StringUtils.isNotBlank(addedFiles)) {
-      initializeFiles("tmpfiles", addedFiles);
-    }
     int returnVal = 0;
     boolean noName = StringUtils.isEmpty(HiveConf.getVar(job, HiveConf.ConfVars.HADOOPJOBNAME));
 
@@ -323,11 +310,6 @@ public int execute(DriverContext driverContext) {
       // This is for a special case to ensure unit tests pass
       HiveConf.setVar(job, HiveConf.ConfVars.HADOOPJOBNAME, "JOB" + Utilities.randGen.nextInt());
     }
-    String addedArchives = HiveConf.getVar(job, HiveConf.ConfVars.HIVEADDEDARCHIVES);
-    // Transfer HIVEADDEDARCHIVES to "tmparchives" so hadoop understands it
-    if (StringUtils.isNotBlank(addedArchives)) {
-      initializeFiles("tmparchives", addedArchives);
-    }
 
     try{
       MapredLocalWork localwork = mWork.getMapRedLocalWork();
@@ -632,6 +614,7 @@ public static void main(String[] args) throws IOException, HiveException {
     String jobConfFileName = null;
     boolean noLog = false;
     String files = null;
+    String libjars = null;
     boolean localtask = false;
     try {
       for (int i = 0; i < args.length; i++) {
@@ -643,7 +626,9 @@ public static void main(String[] args) throws IOException, HiveException {
           noLog = true;
         } else if (args[i].equals("-files")) {
           files = args[++i];
-        } else if (args[i].equals("-localtask")) {
+        } else if (args[i].equals("-libjars")) {
+          libjars = args[++i];
+        }else if (args[i].equals("-localtask")) {
           localtask = true;
         }
       }
@@ -663,10 +648,15 @@ public static void main(String[] args) throws IOException, HiveException {
       conf.addResource(new Path(jobConfFileName));
     }
 
+    // Initialize the resources from command line
     if (files != null) {
       conf.set("tmpfiles", files);
     }
 
+    if (libjars != null) {
+      conf.set("tmpjars", libjars);
+    }
+
     if(UserGroupInformation.isSecurityEnabled()){
       String hadoopAuthToken = System.getenv(UserGroupInformation.HADOOP_TOKEN_FILE_LOCATION);
       if(hadoopAuthToken != null){
@@ -715,17 +705,11 @@ public static void main(String[] args) throws IOException, HiveException {
 
     // this is workaround for hadoop-17 - libjars are not added to classpath of the
     // child process. so we add it here explicitly
-
-    String auxJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS);
-    String addedJars = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEADDEDJARS);
     try {
       // see also - code in CliDriver.java
       ClassLoader loader = conf.getClassLoader();
-      if (StringUtils.isNotBlank(auxJars)) {
-        loader = Utilities.addToClassPath(loader, StringUtils.split(auxJars, ","));
-      }
-      if (StringUtils.isNotBlank(addedJars)) {
-        loader = Utilities.addToClassPath(loader, StringUtils.split(addedJars, ","));
+      if (StringUtils.isNotBlank(libjars)) {
+        loader = Utilities.addToClassPath(loader, StringUtils.split(libjars, ","));
       }
       conf.setClassLoader(loader);
       // Also set this to the Thread ContextClassLoader, so new threads will
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
index 326ca93..de250d2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/mr/MapRedTask.java
@@ -44,8 +44,11 @@
 import org.apache.hadoop.hive.ql.plan.OperatorDesc;
 import org.apache.hadoop.hive.ql.plan.ReduceWork;
 import org.apache.hadoop.hive.ql.session.SessionState;
+import org.apache.hadoop.hive.ql.session.SessionState.ResourceType;
 import org.apache.hadoop.hive.shims.ShimLoader;
+import org.apache.hive.common.util.HiveStringUtils;
 import org.apache.hive.common.util.StreamPrinter;
+
 /**
  * Extension of ExecDriver:
  * - can optionally spawn a map-reduce task from a separate jvm
@@ -147,24 +150,8 @@ public int execute(DriverContext driverContext) {
       String hadoopExec = conf.getVar(HiveConf.ConfVars.HADOOPBIN);
       String hiveJar = conf.getJar();
 
-      String libJarsOption;
-      String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
-      conf.setVar(ConfVars.HIVEADDEDJARS, addedJars);
-      String auxJars = conf.getAuxJars();
-      // Put auxjars and addedjars together into libjars
-      if (StringUtils.isEmpty(addedJars)) {
-        if (StringUtils.isEmpty(auxJars)) {
-          libJarsOption = " ";
-        } else {
-          libJarsOption = " -libjars " + auxJars + " ";
-        }
-      } else {
-        if (StringUtils.isEmpty(auxJars)) {
-          libJarsOption = " -libjars " + addedJars + " ";
-        } else {
-          libJarsOption = " -libjars " + addedJars + "," + auxJars + " ";
-        }
-      }
+      String libJars = super.getResource(conf, ResourceType.JAR);
+      String libJarsOption = StringUtils.isEmpty(libJars) ? " " : " -libjars " + libJars + " ";
 
       // Generate the hiveConfArgs after potentially adding the jars
       String hiveConfArgs = generateCmdLine(conf, ctx);
@@ -193,7 +180,8 @@ public int execute(DriverContext driverContext) {
           + planPath.toString() + " " + isSilent + " " + hiveConfArgs;
 
       String workDir = (new File(".")).getCanonicalPath();
-      String files = Utilities.getResourceFiles(conf, SessionState.ResourceType.FILE);
+
+      String files = super.getResource(conf, ResourceType.FILE);
       if (!files.isEmpty()) {
         cmdLine = cmdLine + " -files " + files;
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java
index 19d3fee..4831850 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/LocalHiveSparkClient.java
@@ -152,7 +152,8 @@ private synchronized void refreshLocalResources(SparkWork sparkWork, HiveConf co
     addJars((new JobConf(this.getClass())).getJar());
 
     // add aux jars
-    addJars(HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS));
+    addJars(conf.getAuxJars());
+    addJars(SessionState.get() == null ? null : SessionState.get().getReloadableAuxJars());
 
     // add added jars
     String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
index 249cdae..058525b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
@@ -28,7 +28,6 @@
 import java.util.ArrayList;
 import java.util.List;
 import java.util.Map;
-import java.util.Set;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.TimeoutException;
@@ -39,6 +38,7 @@
 import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.fs.FileSystem;
 import org.apache.hadoop.fs.Path;
+import org.apache.hadoop.hive.common.FileUtils;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.conf.HiveConf.ConfVars;
 import org.apache.hadoop.hive.ql.Context;
@@ -211,7 +211,8 @@ private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) throws IO
     addJars((new JobConf(this.getClass())).getJar());
 
     // add aux jars
-    addJars(HiveConf.getVar(conf, HiveConf.ConfVars.HIVEAUXJARS));
+    addJars(conf.getAuxJars());
+    addJars(SessionState.get() == null ? null : SessionState.get().getReloadableAuxJars());
 
     // add added jars
     String addedJars = Utilities.getResourceFiles(conf, SessionState.ResourceType.JAR);
@@ -241,7 +242,7 @@ private void refreshLocalResources(SparkWork sparkWork, HiveConf conf) throws IO
   private void addResources(String addedFiles) throws IOException {
     for (String addedFile : CSV_SPLITTER.split(Strings.nullToEmpty(addedFiles))) {
       try {
-        URI fileUri = SparkUtilities.getURI(addedFile);
+        URI fileUri = FileUtils.getURI(addedFile);
         if (fileUri != null && !localFiles.contains(fileUri)) {
           localFiles.add(fileUri);
           if (SparkUtilities.needUploadToHDFS(fileUri, sparkConf)) {
@@ -258,7 +259,7 @@ private void addResources(String addedFiles) throws IOException {
   private void addJars(String addedJars) throws IOException {
     for (String addedJar : CSV_SPLITTER.split(Strings.nullToEmpty(addedJars))) {
       try {
-        URI jarUri = SparkUtilities.getURI(addedJar);
+        URI jarUri = FileUtils.getURI(addedJar);
         if (jarUri != null && !localJars.contains(jarUri)) {
           localJars.add(jarUri);
           if (SparkUtilities.needUploadToHDFS(jarUri, sparkConf)) {
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
index 2ac8654..77069da 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkUtilities.java
@@ -54,20 +54,6 @@ public static BytesWritable copyBytesWritable(BytesWritable bw) {
     return copy;
   }
 
-  public static URI getURI(String path) throws URISyntaxException {
-    if (path == null) {
-      return null;
-    }
-
-      URI uri = new URI(path);
-      if (uri.getScheme() == null) {
-        // if no file schema in path, we assume it's file on local fs.
-        uri = new File(path).toURI();
-      }
-
-    return uri;
-  }
-
   /**
    * Uploads a local file to HDFS
    *
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java b/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java
index b84c9dd..c3bfae2 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/processors/ReloadProcessor.java
@@ -39,7 +39,7 @@ public void init() {
   public CommandProcessorResponse run(String command) throws CommandNeedRetryException {
     SessionState ss = SessionState.get();
     try {
-      ss.reloadAuxJars();
+      ss.loadReloadableAuxJars();
     } catch (IOException e) {
       LOG.error("fail to reload auxiliary jar files", e);
       return CommandProcessorResponse.create(e);
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
index 79d815b..1bdb6e9 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/session/SessionState.java
@@ -39,6 +39,7 @@
 import java.util.UUID;
 
 import org.apache.commons.lang.StringUtils;
+import org.apache.commons.lang3.ArrayUtils;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.conf.Configuration;
@@ -981,8 +982,28 @@ static void validateFiles(List<String> newFiles) throws IllegalArgumentException
     }
   }
 
-  // reloading the jars under the path specified in hive.reloadable.aux.jars.path property
-  public void reloadAuxJars() throws IOException {
+  /**
+   * Load the jars under the path specified in hive.aux.jars.path property. Add
+   * the jars to the classpath so the local task can refer to them.
+   * @throws IOException
+   */
+  public void loadAuxJars() throws IOException {
+    String[] jarPaths = StringUtils.split(conf.getAuxJars(), ',');
+    if (ArrayUtils.isEmpty(jarPaths)) return;
+
+    URLClassLoader currentCLoader =
+        (URLClassLoader) SessionState.get().getConf().getClassLoader();
+    currentCLoader =
+        (URLClassLoader) Utilities.addToClassPath(currentCLoader, jarPaths);
+    conf.setClassLoader(currentCLoader);
+    Thread.currentThread().setContextClassLoader(currentCLoader);
+  }
+
+  /**
+   * Reload the jars under the path specified in hive.reloadable.aux.jars.path property.
+   * @throws IOException
+   */
+  public void loadReloadableAuxJars() throws IOException {
     final Set<String> reloadedAuxJars = new HashSet<String>();
 
     final String renewableJarPath = conf.getVar(ConfVars.HIVERELOADABLEJARS);
@@ -991,7 +1012,7 @@ public void reloadAuxJars() throws IOException {
       return;
     }
 
-    Set<String> jarPaths = Utilities.getJarFilesByPath(renewableJarPath);
+    Set<String> jarPaths = FileUtils.getJarFilesByPath(renewableJarPath, conf);
 
     // load jars under the hive.reloadable.aux.jars.path
     if(!jarPaths.isEmpty()){
@@ -999,32 +1020,21 @@ public void reloadAuxJars() throws IOException {
     }
 
     // remove the previous renewable jars
-    try {
-      if (preReloadableAuxJars != null && !preReloadableAuxJars.isEmpty()) {
-        Utilities.removeFromClassPath(preReloadableAuxJars.toArray(new String[0]));
-      }
-    } catch (Exception e) {
-      String msg = "Fail to remove the reloaded jars loaded last time: " + e;
-      throw new IOException(msg, e);
+    if (preReloadableAuxJars != null && !preReloadableAuxJars.isEmpty()) {
+      Utilities.removeFromClassPath(preReloadableAuxJars.toArray(new String[0]));
     }
 
-    try {
-      if (reloadedAuxJars != null && !reloadedAuxJars.isEmpty()) {
-        URLClassLoader currentCLoader =
-            (URLClassLoader) SessionState.get().getConf().getClassLoader();
-        currentCLoader =
-            (URLClassLoader) Utilities.addToClassPath(currentCLoader,
-                reloadedAuxJars.toArray(new String[0]));
-        conf.setClassLoader(currentCLoader);
-        Thread.currentThread().setContextClassLoader(currentCLoader);
-      }
-      preReloadableAuxJars.clear();
-      preReloadableAuxJars.addAll(reloadedAuxJars);
-    } catch (Exception e) {
-      String msg =
-          "Fail to add jars from the path specified in hive.reloadable.aux.jars.path property: " + e;
-      throw new IOException(msg, e);
+    if (reloadedAuxJars != null && !reloadedAuxJars.isEmpty()) {
+      URLClassLoader currentCLoader =
+          (URLClassLoader) SessionState.get().getConf().getClassLoader();
+      currentCLoader =
+          (URLClassLoader) Utilities.addToClassPath(currentCLoader,
+              reloadedAuxJars.toArray(new String[0]));
+      conf.setClassLoader(currentCLoader);
+      Thread.currentThread().setContextClassLoader(currentCLoader);
     }
+    preReloadableAuxJars.clear();
+    preReloadableAuxJars.addAll(reloadedAuxJars);
   }
 
   static void registerJars(List<String> newJars) throws IllegalArgumentException {
@@ -1047,7 +1057,7 @@ static boolean unregisterJar(List<String> jarsToUnregister) {
       Utilities.removeFromClassPath(jarsToUnregister.toArray(new String[0]));
       console.printInfo("Deleted " + jarsToUnregister + " from class path");
       return true;
-    } catch (Exception e) {
+    } catch (IOException e) {
       console.printError("Unable to unregister " + jarsToUnregister
           + "\nException: " + e.getMessage(), "\n"
               + org.apache.hadoop.util.StringUtils.stringifyException(e));
@@ -1546,4 +1556,12 @@ public void setupQueryCurrentTimestamp() {
   public Timestamp getQueryCurrentTimestamp() {
     return queryCurrentTimestamp;
   }
+
+  /**
+   * Gets the comma-separated reloadable aux jars
+   * @return the list of reloadable aux jars
+   */
+  public String getReloadableAuxJars() {
+    return StringUtils.join(preReloadableAuxJars, ',');
+  }
 }
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
index c797dec..ffd380d 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/TestUtilities.java
@@ -19,7 +19,6 @@
 package org.apache.hadoop.hive.ql.exec;
 
 import static org.apache.hadoop.hive.ql.exec.Utilities.getFileExtension;
-
 import static org.mockito.Mockito.doReturn;
 import static org.mockito.Mockito.mock;
 import static org.mockito.Mockito.when;
@@ -43,10 +42,8 @@
 import junit.framework.TestCase;
 
 import org.apache.commons.io.FileUtils;
-
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
-
 import org.apache.hadoop.fs.Path;
 import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.Context;
@@ -64,7 +61,6 @@
 import org.apache.hadoop.hive.ql.udf.generic.GenericUDFFromUtcTimestamp;
 import org.apache.hadoop.hive.serde2.typeinfo.TypeInfoFactory;
 import org.apache.hadoop.mapred.JobConf;
-
 import org.junit.Test;
 
 
@@ -138,29 +134,6 @@ public void testgetDbTableName() throws HiveException{
     }
   }
 
-  public void testGetJarFilesByPath() {
-    File f = Files.createTempDir();
-    String jarFileName1 = f.getAbsolutePath() + File.separator + "a.jar";
-    String jarFileName2 = f.getAbsolutePath() + File.separator + "b.jar";
-    File jarFile = new File(jarFileName1);
-    try {
-      FileUtils.touch(jarFile);
-      HashSet<String> jars = (HashSet) Utilities.getJarFilesByPath(f.getAbsolutePath());
-      Assert.assertEquals(Sets.newHashSet(jarFile.getAbsolutePath()),jars);
-
-      File jarFile2 = new File(jarFileName2);
-      FileUtils.touch(jarFile2);
-      String newPath = "file://" + jarFileName1 + "," + "file://" + jarFileName2;
-      jars = (HashSet) Utilities.getJarFilesByPath(newPath);
-
-      Assert.assertEquals(Sets.newHashSet("file://" + jarFileName1, "file://" + jarFileName2), jars);
-    } catch (IOException e) {
-      LOG.error("failed to copy file to reloading folder", e);
-      Assert.fail(e.getMessage());
-    } finally {
-      FileUtils.deleteQuietly(f);
-    }
-  }
 
   public void testMaskIfPassword() {
     Assert.assertNull(Utilities.maskIfPassword("",null));
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java b/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
index 9e16c0c..09d6661 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/session/TestSessionState.java
@@ -202,7 +202,7 @@ public void testReloadAuxJars2() {
     try {
       dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);
       Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);
-      ss.reloadAuxJars();
+      ss.loadReloadableAuxJars();
       Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));
     } catch (Exception e) {
       LOG.error("Reload auxiliary jar test fail with message: ", e);
@@ -234,7 +234,7 @@ public void testReloadExistingAuxJars2() {
       dist = new File(reloadFolder.getAbsolutePath() + File.separator + reloadClazzFileName);
 
       Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzDistFileName)), dist);
-      ss.reloadAuxJars();
+      ss.loadReloadableAuxJars();
 
       Assert.assertEquals("version1", getReloadedClazzVersion(ss.getConf().getClassLoader()));
 
@@ -242,11 +242,11 @@ public void testReloadExistingAuxJars2() {
       FileUtils.deleteQuietly(dist);
       Files.copy(new File(HiveTestUtils.getFileFromClasspath(clazzV2FileName)), dist);
 
-      ss.reloadAuxJars();
+      ss.loadReloadableAuxJars();
       Assert.assertEquals("version2", getReloadedClazzVersion(ss.getConf().getClassLoader()));
 
       FileUtils.deleteQuietly(dist);
-      ss.reloadAuxJars();
+      ss.loadReloadableAuxJars();
     } catch (Exception e) {
       LOG.error("refresh existing jar file case failed with message: ", e);
       Assert.fail(e.getMessage());
diff --git a/ql/src/test/queries/clientpositive/reloadJar.q b/ql/src/test/queries/clientpositive/reloadJar.q
new file mode 100644
index 0000000..6768a4f
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/reloadJar.q
@@ -0,0 +1,17 @@
+dfs -mkdir  ${system:test.tmp.dir}/aux;
+dfs -cp ${system:hive.root}/data/files/identity_udf.jar ${system:test.tmp.dir}/aux/udfexample.jar;
+
+SET hive.reloadable.aux.jars.path=${system:test.tmp.dir}/aux;
+RELOAD;
+CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF';
+
+EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1;
+
+SELECT example_iden(key)
+FROM src LIMIT 1;
+
+DROP TEMPORARY FUNCTION example_iden;
+
+dfs -rm -r ${system:test.tmp.dir}/aux;
diff --git a/ql/src/test/results/clientpositive/reloadJar.q.out b/ql/src/test/results/clientpositive/reloadJar.q.out
new file mode 100644
index 0000000..cff41b3
--- /dev/null
+++ b/ql/src/test/results/clientpositive/reloadJar.q.out
@@ -0,0 +1,64 @@
+PREHOOK: query: CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF'
+PREHOOK: type: CREATEFUNCTION
+PREHOOK: Output: example_iden
+POSTHOOK: query: CREATE TEMPORARY FUNCTION example_iden AS 'IdentityStringUDF'
+POSTHOOK: type: CREATEFUNCTION
+POSTHOOK: Output: example_iden
+PREHOOK: query: EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1
+PREHOOK: type: QUERY
+POSTHOOK: query: EXPLAIN
+SELECT example_iden(key)
+FROM src LIMIT 1
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-1 is a root stage
+  Stage-0 depends on stages: Stage-1
+
+STAGE PLANS:
+  Stage: Stage-1
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: src
+            Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+            Select Operator
+              expressions: example_iden(key) (type: string)
+              outputColumnNames: _col0
+              Statistics: Num rows: 500 Data size: 5312 Basic stats: COMPLETE Column stats: NONE
+              Limit
+                Number of rows: 1
+                Statistics: Num rows: 1 Data size: 10 Basic stats: COMPLETE Column stats: NONE
+                File Output Operator
+                  compressed: false
+                  Statistics: Num rows: 1 Data size: 10 Basic stats: COMPLETE Column stats: NONE
+                  table:
+                      input format: org.apache.hadoop.mapred.TextInputFormat
+                      output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                      serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-0
+    Fetch Operator
+      limit: 1
+      Processor Tree:
+        ListSink
+
+PREHOOK: query: SELECT example_iden(key)
+FROM src LIMIT 1
+PREHOOK: type: QUERY
+PREHOOK: Input: default@src
+#### A masked pattern was here ####
+POSTHOOK: query: SELECT example_iden(key)
+FROM src LIMIT 1
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@src
+#### A masked pattern was here ####
+238
+PREHOOK: query: DROP TEMPORARY FUNCTION example_iden
+PREHOOK: type: DROPFUNCTION
+PREHOOK: Output: example_iden
+POSTHOOK: query: DROP TEMPORARY FUNCTION example_iden
+POSTHOOK: type: DROPFUNCTION
+POSTHOOK: Output: example_iden
+#### A masked pattern was here ####
diff --git a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
index b41889f..d68ed08 100644
--- a/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
+++ b/service/src/java/org/apache/hive/service/cli/session/HiveSessionImpl.java
@@ -153,7 +153,8 @@ public void open(Map<String, String> sessionConfMap) throws HiveSQLException {
     sessionState.setIsHiveServerQuery(true);
     SessionState.start(sessionState);
     try {
-      sessionState.reloadAuxJars();
+      sessionState.loadAuxJars();
+      sessionState.loadReloadableAuxJars();
     } catch (IOException e) {
       String msg = "Failed to load reloadable jar file path: " + e;
       LOG.error(msg, e);
-- 
1.7.9.5


From ee8371a9ffbe7f12cd5d4d1ecbca0cc6d5a94e46 Mon Sep 17 00:00:00 2001
From: Rui Li <rui.li@intel.com>
Date: Fri, 1 Apr 2016 14:36:18 +0800
Subject: [PATCH 0599/1164] CDH-40202 : HIVE-12650: Improve error messages for
 Hive on Spark in case the cluster has no
 resources available (Rui reviewed by Xuefu)

(cherry picked from commit 03b81bc9c40b6de4f238f6b7660488e711b869c4)

Change-Id: I43d02c9e78a7802c1f4649c6a83e578b4dd6482c
---
 .../hive/ql/exec/spark/RemoteHiveSparkClient.java  |   20 +++++++++++++++-----
 .../hadoop/hive/ql/exec/spark/SparkTask.java       |    3 +++
 .../ql/exec/spark/status/LocalSparkJobMonitor.java |    2 +-
 .../exec/spark/status/RemoteSparkJobMonitor.java   |    5 ++++-
 .../apache/hive/spark/client/SparkClientImpl.java  |    9 ++++++++-
 5 files changed, 31 insertions(+), 8 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
index 43bc766..249cdae 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/RemoteHiveSparkClient.java
@@ -31,6 +31,7 @@
 import java.util.Set;
 import java.util.concurrent.Future;
 import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
 
 import org.apache.commons.lang.StringUtils;
 import org.apache.commons.logging.Log;
@@ -110,7 +111,12 @@ private void createRemoteClient() throws Exception {
       int curExecutors = 0;
       long ts = System.currentTimeMillis();
       do {
-        curExecutors = getExecutorCount();
+        try {
+          curExecutors = getExecutorCount(MAX_PREWARM_TIME, TimeUnit.MILLISECONDS);
+        } catch (TimeoutException e) {
+          // let's don't fail on future timeout since we have a timeout for pre-warm
+          LOG.warn("Timed out getting executor count.", e);
+        }
         if (curExecutors >= minExecutors) {
           LOG.info("Finished prewarming Spark executors. The current number of executors is " + curExecutors);
           return;
@@ -118,8 +124,8 @@ private void createRemoteClient() throws Exception {
         Thread.sleep(500); // sleep half a second
       } while (System.currentTimeMillis() - ts < MAX_PREWARM_TIME);
 
-      LOG.info("Timeout (" + MAX_PREWARM_TIME + 
-          "s) occurred while prewarming executors. The current number of executors is " + curExecutors);
+      LOG.info("Timeout (" + MAX_PREWARM_TIME / 1000 + "s) occurred while prewarming executors. " +
+          "The current number of executors is " + curExecutors);
     }
   }
 
@@ -143,6 +149,11 @@ private int getExecutorsToWarm() {
     return minExecutors;
   }
 
+  private int getExecutorCount(long timeout, TimeUnit unit) throws Exception {
+    Future<Integer> handler = remoteClient.getExecutorCount();
+    return handler.get(timeout, unit);
+  }
+
   @Override
   public SparkConf getSparkConf() {
     return sparkConf;
@@ -150,8 +161,7 @@ public SparkConf getSparkConf() {
 
   @Override
   public int getExecutorCount() throws Exception {
-    Future<Integer> handler = remoteClient.getExecutorCount();
-    return handler.get(sparkClientTimtout, TimeUnit.SECONDS).intValue();
+    return getExecutorCount(sparkClientTimtout, TimeUnit.SECONDS);
   }
 
   @Override
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
index 8cbee23..08b811a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/SparkTask.java
@@ -116,6 +116,9 @@ public int execute(DriverContext driverContext) {
         }
         LOG.info("Execution completed successfully");
       } else if (rc == 2) { // Cancel job if the monitor found job submission timeout.
+        // TODO: If the timeout is because of lack of resources in the cluster, we should
+        // ideally also cancel the app request here. But w/o facilities from Spark or YARN,
+        // it's difficult to do it on hive side alone. See HIVE-12650.
         jobRef.cancelJob();
       }
       sparkJobStatus.cleanup();
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java
index d109c6f..5f0352a 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/LocalSparkJobMonitor.java
@@ -59,7 +59,7 @@ public int startMonitor() {
         if (state == null) {
           long timeCount = (System.currentTimeMillis() - startTime)/1000;
           if (timeCount > monitorTimeoutInteval) {
-            LOG.info("Job hasn't been submitted after " + timeCount + "s. Aborting it.");
+            console.printError("Job hasn't been submitted after " + timeCount + "s. Aborting it.");
             console.printError("Status: " + state);
             running = false;
             done = true;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RemoteSparkJobMonitor.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RemoteSparkJobMonitor.java
index 6990e80..11f263b 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RemoteSparkJobMonitor.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/status/RemoteSparkJobMonitor.java
@@ -66,7 +66,10 @@ public int startMonitor() {
         case QUEUED:
           long timeCount = (System.currentTimeMillis() - startTime) / 1000;
           if ((timeCount > monitorTimeoutInteval)) {
-            LOG.info("Job hasn't been submitted after " + timeCount + "s. Aborting it.");
+            console.printError("Job hasn't been submitted after " + timeCount + "s." +
+                " Aborting it.\nPossible reasons include network issues, " +
+                "errors in remote driver or the cluster has no available resources, etc.\n" +
+                "Please check YARN or Spark driver's logs for further information.");
             console.printError("Status: " + state);
             running = false;
             done = true;
diff --git a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java b/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
index 687d9a3..dfe263f 100644
--- a/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
+++ b/spark-client/src/main/java/org/apache/hive/spark/client/SparkClientImpl.java
@@ -48,6 +48,7 @@
 import java.util.Properties;
 import java.util.UUID;
 import java.util.concurrent.Future;
+import java.util.concurrent.TimeoutException;
 import java.util.concurrent.atomic.AtomicInteger;
 
 import org.apache.hadoop.hive.conf.HiveConf;
@@ -102,7 +103,13 @@
       // The RPC server will take care of timeouts here.
       this.driverRpc = rpcServer.registerClient(clientId, secret, protocol).get();
     } catch (Throwable e) {
-      LOG.warn("Error while waiting for client to connect.", e);
+      if (e.getCause() instanceof TimeoutException) {
+        LOG.error("Timed out waiting for client to connect.\nPossible reasons include network " +
+            "issues, errors in remote driver or the cluster has no available resources, etc." +
+            "\nPlease check YARN or Spark driver's logs for further information.", e);
+      } else {
+        LOG.error("Error while waiting for client to connect.", e);
+      }
       driverThread.interrupt();
       try {
         driverThread.join();
-- 
1.7.9.5


From f83774e406973f8d96bf1c5f7dd6fd2a16a75a40 Mon Sep 17 00:00:00 2001
From: Xuefu Zhang <xuefu@apache.org>
Date: Tue, 10 Feb 2015 19:38:40 +0000
Subject: [PATCH 0043/1164] HIVE-9574: Lazy computing in
 HiveBaseFunctionResultList may hurt performance
 [Spark Branch] (Jimmy via Xuefu)

git-svn-id: https://svn.apache.org/repos/asf/hive/branches/spark@1658786 13f79535-47bb-0310-9956-ffa450edef68
(cherry picked from commit 3e7a258f910c78110405c3564d1ab3122281ae27)
---
 .../ql/exec/spark/HiveBaseFunctionResultList.java  |    7 +-
 .../hive/ql/exec/spark/HiveKVResultCache.java      |  302 +++++++++++++-------
 .../hadoop/hive/ql/exec/spark/HiveMapFunction.java |    6 +-
 .../ql/exec/spark/HiveMapFunctionResultList.java   |   17 +-
 .../hive/ql/exec/spark/HiveReduceFunction.java     |    6 +-
 .../exec/spark/HiveReduceFunctionResultList.java   |   16 +-
 .../hadoop/hive/ql/exec/spark/KryoSerializer.java  |    2 +-
 .../hive/ql/exec/spark/TestHiveKVResultCache.java  |   29 +-
 8 files changed, 248 insertions(+), 137 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java
index 78ab680..5b65036 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveBaseFunctionResultList.java
@@ -22,7 +22,6 @@
 import java.util.Iterator;
 import java.util.NoSuchElementException;
 
-import org.apache.hadoop.conf.Configuration;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.apache.hadoop.mapred.OutputCollector;
@@ -49,9 +48,9 @@
   private final HiveKVResultCache lastRecordOutput;
   private boolean iteratorAlreadyCreated = false;
 
-  public HiveBaseFunctionResultList(Configuration conf, Iterator<T> inputIterator) {
+  public HiveBaseFunctionResultList(Iterator<T> inputIterator) {
     this.inputIterator = inputIterator;
-    this.lastRecordOutput = new HiveKVResultCache(conf);
+    this.lastRecordOutput = new HiveKVResultCache();
   }
 
   @Override
@@ -87,8 +86,6 @@ public boolean hasNext(){
         return true;
       }
 
-      lastRecordOutput.clear();
-
       // Process the records in the input iterator until
       //  - new output records are available for serving downstream operator,
       //  - input records are exhausted or
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveKVResultCache.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveKVResultCache.java
index 8ead0cb..9db2e8d 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveKVResultCache.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveKVResultCache.java
@@ -17,141 +17,251 @@
  */
 package org.apache.hadoop.hive.ql.exec.spark;
 
-import java.util.ArrayList;
-import java.util.List;
+import java.io.File;
+import java.io.FileInputStream;
+import java.io.FileOutputStream;
+import java.io.IOException;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
-import org.apache.hadoop.hive.ql.exec.persistence.RowContainer;
+import org.apache.commons.logging.Log;
+import org.apache.commons.logging.LogFactory;
+import org.apache.hadoop.fs.FileUtil;
+import org.apache.hadoop.hive.common.ObjectPair;
 import org.apache.hadoop.hive.ql.io.HiveKey;
-import org.apache.hadoop.hive.ql.metadata.HiveException;
-import org.apache.hadoop.hive.ql.plan.PlanUtils;
-import org.apache.hadoop.hive.ql.plan.TableDesc;
-import org.apache.hadoop.hive.serde.serdeConstants;
-import org.apache.hadoop.hive.serde2.SerDe;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils;
-import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.ObjectInspectorCopyOption;
 import org.apache.hadoop.io.BytesWritable;
-import org.apache.hadoop.mapred.Reporter;
 
 import scala.Tuple2;
 
+import com.esotericsoftware.kryo.io.Input;
+import com.esotericsoftware.kryo.io.Output;
+import com.google.common.annotations.VisibleForTesting;
 import com.google.common.base.Preconditions;
 
 /**
- * Wrapper around {@link org.apache.hadoop.hive.ql.exec.persistence.RowContainer}
- *
- * This class is thread safe.
+ * A cache with fixed buffer. If the buffer is full, new entries will
+ * be written to disk. This class is thread safe since multiple threads
+ * could access it (doesn't have to be concurrently), for example,
+ * the StreamThread in ScriptOperator.
  */
-@SuppressWarnings({"deprecation", "unchecked", "rawtypes"})
-public class HiveKVResultCache {
+@SuppressWarnings("unchecked")
+class HiveKVResultCache {
+  private static final Log LOG = LogFactory.getLog(HiveKVResultCache.class);
+
+  @VisibleForTesting
+  static final int IN_MEMORY_NUM_ROWS = 1024;
+
+  private ObjectPair<HiveKey, BytesWritable>[] writeBuffer;
+  private ObjectPair<HiveKey, BytesWritable>[] readBuffer;
 
-  public static final int IN_MEMORY_CACHE_SIZE = 512;
-  private static final String COL_NAMES = "key,value";
-  private static final String COL_TYPES =
-      serdeConstants.BINARY_TYPE_NAME + ":" + serdeConstants.BINARY_TYPE_NAME;
+  private File parentFile;
+  private File tmpFile;
 
-  // Used to cache rows added while container is iterated.
-  private RowContainer backupContainer;
+  private int readCursor = 0;
+  private int writeCursor = 0;
 
-  private RowContainer container;
-  private Configuration conf;
-  private int cursor = 0;
+  // Indicate if the read buffer has data, for example,
+  // when in reading, data on disk could be pull in
+  private boolean readBufferUsed = false;
+  private int rowsInReadBuffer = 0;
 
-  public HiveKVResultCache(Configuration conf) {
-    container = initRowContainer(conf);
-    this.conf = conf;
+  private Input input;
+  private Output output;
+
+  public HiveKVResultCache() {
+    writeBuffer = new ObjectPair[IN_MEMORY_NUM_ROWS];
+    readBuffer = new ObjectPair[IN_MEMORY_NUM_ROWS];
+    for (int i = 0; i < IN_MEMORY_NUM_ROWS; i++) {
+      writeBuffer[i] = new ObjectPair<HiveKey, BytesWritable>();
+      readBuffer[i] = new ObjectPair<HiveKey, BytesWritable>();
+    }
   }
 
-  private static RowContainer initRowContainer(Configuration conf) {
-    RowContainer container;
-    try {
-      container = new RowContainer(IN_MEMORY_CACHE_SIZE, conf, Reporter.NULL);
+  private void switchBufferAndResetCursor() {
+    ObjectPair<HiveKey, BytesWritable>[] tmp = readBuffer;
+    rowsInReadBuffer = writeCursor;
+    readBuffer = writeBuffer;
+    readBufferUsed = true;
+    readCursor = 0;
+    writeBuffer = tmp;
+    writeCursor = 0;
+  }
 
-      String fileFormat = HiveConf.getVar(conf, HiveConf.ConfVars.HIVEQUERYRESULTFILEFORMAT);
-      TableDesc tableDesc =
-          PlanUtils.getDefaultQueryOutputTableDesc(COL_NAMES, COL_TYPES, fileFormat);
+  private void setupOutput() throws IOException {
+    if (parentFile == null) {
+      while (true) {
+        parentFile = File.createTempFile("hive-resultcache", "");
+        if (parentFile.delete() && parentFile.mkdir()) {
+          parentFile.deleteOnExit();
+          break;
+        }
+        if (LOG.isDebugEnabled()) {
+          LOG.debug("Retry creating tmp result-cache directory...");
+        }
+      }
+    }
 
-      SerDe serDe = (SerDe) tableDesc.getDeserializer();
-      ObjectInspector oi = ObjectInspectorUtils.getStandardObjectInspector(
-          serDe.getObjectInspector(), ObjectInspectorCopyOption.WRITABLE);
+    if (tmpFile == null || input != null) {
+      tmpFile = File.createTempFile("ResultCache", ".tmp", parentFile);
+      LOG.info("ResultCache created temp file " + tmpFile.getAbsolutePath());
+      tmpFile.deleteOnExit();
+    }
 
-      container.setSerDe(serDe, oi);
-      container.setTableDesc(tableDesc);
-    } catch (Exception ex) {
-      throw new RuntimeException("Failed to create RowContainer", ex);
+    FileOutputStream fos = null;
+    try {
+      fos = new FileOutputStream(tmpFile);
+      output = new Output(fos);
+    } finally {
+      if (output == null && fos != null) {
+        fos.close();
+      }
     }
-    return container;
   }
 
-  public void add(HiveKey key, BytesWritable value) {
-    byte[] hiveKeyBytes = KryoSerializer.serialize(key);
-    BytesWritable wrappedHiveKey = new BytesWritable(hiveKeyBytes);
-    List<BytesWritable> row = new ArrayList<BytesWritable>(2);
-    row.add(wrappedHiveKey);
-    row.add(value);
+  private BytesWritable readValue(Input input) {
+    return new BytesWritable(input.readBytes(input.readInt()));
+  }
 
-    synchronized (this) {
-      try {
-        if (cursor == 0) {
-          container.addRow(row);
-        } else {
-          if (backupContainer == null) {
-            backupContainer = initRowContainer(conf);
+  private void writeValue(Output output, BytesWritable bytesWritable) {
+    int size = bytesWritable.getLength();
+    output.writeInt(size);
+    output.writeBytes(bytesWritable.getBytes(), 0, size);
+  }
+
+  private HiveKey readHiveKey(Input input) {
+    HiveKey hiveKey = new HiveKey(
+      input.readBytes(input.readInt()), input.readInt());
+    hiveKey.setDistKeyLength(input.readInt());
+    return hiveKey;
+  }
+
+  private void writeHiveKey(Output output, HiveKey hiveKey) {
+    int size = hiveKey.getLength();
+    output.writeInt(size);
+    output.writeBytes(hiveKey.getBytes(), 0, size);
+    output.writeInt(hiveKey.hashCode());
+    output.writeInt(hiveKey.getDistKeyLength());
+  }
+
+  public synchronized void add(HiveKey key, BytesWritable value) {
+    if (writeCursor >= IN_MEMORY_NUM_ROWS) { // Write buffer is full
+      if (!readBufferUsed) { // Read buffer isn't used, switch buffer
+        switchBufferAndResetCursor();
+      } else {
+        // Need to spill from write buffer to disk
+        try {
+          if (output == null) {
+            setupOutput();
+          }
+          for (int i = 0; i < IN_MEMORY_NUM_ROWS; i++) {
+            ObjectPair<HiveKey, BytesWritable> pair = writeBuffer[i];
+            writeHiveKey(output, pair.getFirst());
+            writeValue(output, pair.getSecond());
+            pair.setFirst(null);
+            pair.setSecond(null);
           }
-          backupContainer.addRow(row);
+          writeCursor = 0;
+        } catch (Exception e) {
+          clear(); // Clean up the cache
+          throw new RuntimeException("Failed to spill rows to disk", e);
         }
-      } catch (HiveException ex) {
-        throw new RuntimeException("Failed to add KV pair to RowContainer", ex);
       }
     }
+    ObjectPair<HiveKey, BytesWritable> pair = writeBuffer[writeCursor++];
+    pair.setFirst(key);
+    pair.setSecond(value);
   }
 
   public synchronized void clear() {
-    if (cursor == 0) {
-      return;
-    }
-    try {
-      container.clearRows();
-    } catch (HiveException ex) {
-      throw new RuntimeException("Failed to clear rows in RowContainer", ex);
+    writeCursor = readCursor = rowsInReadBuffer = 0;
+    readBufferUsed = false;
+
+    if (parentFile != null) {
+      if (input != null) {
+        try {
+          input.close();
+        } catch (Throwable ignored) {
+        }
+        input = null;
+      }
+      if (output != null) {
+        try {
+          output.close();
+        } catch (Throwable ignored) {
+        }
+        output = null;
+      }
+      try {
+        FileUtil.fullyDelete(parentFile);
+      } catch (Throwable ignored) {
+      }
+      parentFile = null;
+      tmpFile = null;
     }
-    cursor = 0;
   }
 
   public synchronized boolean hasNext() {
-    if (container.rowCount() > 0 && cursor < container.rowCount()) {
-      return true;
-    }
-    if (backupContainer == null
-        || backupContainer.rowCount() == 0) {
-      return false;
-    }
-    clear();
-    // Switch containers
-    RowContainer tmp = container;
-    container = backupContainer;
-    backupContainer = tmp;
-    return true;
+    return readBufferUsed || writeCursor > 0;
   }
 
-  public Tuple2<HiveKey, BytesWritable> next() {
-    try {
-      List<BytesWritable> row;
-      synchronized (this) {
-        Preconditions.checkState(hasNext());
-        if (cursor == 0) {
-          row = container.first();
+  public synchronized Tuple2<HiveKey, BytesWritable> next() {
+    Preconditions.checkState(hasNext());
+    if (!readBufferUsed) {
+      try {
+        if (input == null && output != null) {
+          // Close output stream if open
+          output.close();
+          output = null;
+
+          FileInputStream fis = null;
+          try {
+            fis = new FileInputStream(tmpFile);
+            input = new Input(fis);
+          } finally {
+            if (input == null && fis != null) {
+              fis.close();
+            }
+          }
+        }
+        if (input != null) {
+          // Load next batch from disk
+          for (int i = 0; i < IN_MEMORY_NUM_ROWS; i++) {
+            ObjectPair<HiveKey, BytesWritable> pair = readBuffer[i];
+            pair.setFirst(readHiveKey(input));
+            pair.setSecond(readValue(input));
+          }
+          if (input.eof()) {
+            input.close();
+            input = null;
+          }
+          rowsInReadBuffer = IN_MEMORY_NUM_ROWS;
+          readBufferUsed = true;
+          readCursor = 0;
+        } else if (writeCursor == 1) {
+          ObjectPair<HiveKey, BytesWritable> pair = writeBuffer[0];
+          Tuple2<HiveKey, BytesWritable> row = new Tuple2<HiveKey, BytesWritable>(
+            pair.getFirst(), pair.getSecond());
+          pair.setFirst(null);
+          pair.setSecond(null);
+          writeCursor = 0;
+          return row;
         } else {
-          row = container.next();
+          // No record on disk, more data in write buffer
+          switchBufferAndResetCursor();
         }
-        cursor++;
+      } catch (Exception e) {
+        clear(); // Clean up the cache
+        throw new RuntimeException("Failed to load rows from disk", e);
       }
-      HiveKey key = KryoSerializer.deserialize(row.get(0).getBytes(), HiveKey.class);
-      return new Tuple2<HiveKey, BytesWritable>(key, row.get(1));
-    } catch (HiveException ex) {
-      throw new RuntimeException("Failed to get row from RowContainer", ex);
     }
+    ObjectPair<HiveKey, BytesWritable> pair = readBuffer[readCursor];
+    Tuple2<HiveKey, BytesWritable> row = new Tuple2<HiveKey, BytesWritable>(
+      pair.getFirst(), pair.getSecond());
+    pair.setFirst(null);
+    pair.setSecond(null);
+    if (++readCursor >= rowsInReadBuffer) {
+      readBufferUsed = false;
+      rowsInReadBuffer = 0;
+      readCursor = 0;
+    }
+    return row;
   }
 }
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
index 7a09b4d..53c5c0e 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunction.java
@@ -18,6 +18,8 @@
 
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import java.util.Iterator;
+
 import org.apache.hadoop.hive.ql.exec.Utilities;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.hive.ql.io.merge.MergeFileMapper;
@@ -25,8 +27,6 @@
 
 import scala.Tuple2;
 
-import java.util.Iterator;
-
 public class HiveMapFunction extends HivePairFlatMapFunction<
   Iterator<Tuple2<BytesWritable, BytesWritable>>, HiveKey, BytesWritable> {
 
@@ -51,7 +51,7 @@ public HiveMapFunction(byte[] jobConfBuffer, SparkReporter sparkReporter) {
       mapRecordHandler = new SparkMapRecordHandler();
     }
 
-    HiveMapFunctionResultList result = new HiveMapFunctionResultList(jobConf, it, mapRecordHandler);
+    HiveMapFunctionResultList result = new HiveMapFunctionResultList(it, mapRecordHandler);
     mapRecordHandler.init(jobConf, result, sparkReporter);
 
     return result;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunctionResultList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunctionResultList.java
index e92e299..4767cd5 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunctionResultList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveMapFunctionResultList.java
@@ -17,27 +17,28 @@
  */
 package org.apache.hadoop.hive.ql.exec.spark;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.io.BytesWritable;
-import scala.Tuple2;
-
 import java.io.IOException;
 import java.util.Iterator;
 
+import org.apache.hadoop.io.BytesWritable;
+
+import scala.Tuple2;
+
 public class HiveMapFunctionResultList extends
     HiveBaseFunctionResultList<Tuple2<BytesWritable, BytesWritable>> {
+  private static final long serialVersionUID = 1L;
   private final SparkRecordHandler recordHandler;
 
   /**
    * Instantiate result set Iterable for Map function output.
    *
-   * @param conf Hive configuration.
    * @param inputIterator Input record iterator.
    * @param handler Initialized {@link SparkMapRecordHandler} instance.
    */
-  public HiveMapFunctionResultList(Configuration conf,
-      Iterator<Tuple2<BytesWritable, BytesWritable>> inputIterator, SparkRecordHandler handler) {
-    super(conf, inputIterator);
+  public HiveMapFunctionResultList(
+      Iterator<Tuple2<BytesWritable, BytesWritable>> inputIterator,
+      SparkRecordHandler handler) {
+    super(inputIterator);
     recordHandler = handler;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
index 070ea4d..f6595f1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunction.java
@@ -18,13 +18,13 @@
 
 package org.apache.hadoop.hive.ql.exec.spark;
 
+import java.util.Iterator;
+
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 
 import scala.Tuple2;
 
-import java.util.Iterator;
-
 public class HiveReduceFunction extends HivePairFlatMapFunction<
   Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>>, HiveKey, BytesWritable> {
 
@@ -42,7 +42,7 @@ public HiveReduceFunction(byte[] buffer, SparkReporter sparkReporter) {
 
     SparkReduceRecordHandler reducerRecordhandler = new SparkReduceRecordHandler();
     HiveReduceFunctionResultList result =
-        new HiveReduceFunctionResultList(jobConf, it, reducerRecordhandler);
+        new HiveReduceFunctionResultList(it, reducerRecordhandler);
     reducerRecordhandler.init(jobConf, result, sparkReporter);
 
     return result;
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
index d4ff37c..d57cac4 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HiveReduceFunctionResultList.java
@@ -17,29 +17,29 @@
  */
 package org.apache.hadoop.hive.ql.exec.spark;
 
-import org.apache.hadoop.conf.Configuration;
+import java.io.IOException;
+import java.util.Iterator;
+
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
-import scala.Tuple2;
 
-import java.io.IOException;
-import java.util.Iterator;
+import scala.Tuple2;
 
 public class HiveReduceFunctionResultList extends
     HiveBaseFunctionResultList<Tuple2<HiveKey, Iterable<BytesWritable>>> {
+  private static final long serialVersionUID = 1L;
   private final SparkReduceRecordHandler reduceRecordHandler;
 
   /**
    * Instantiate result set Iterable for Reduce function output.
    *
-   * @param conf Hive configuration.
    * @param inputIterator Input record iterator.
    * @param reducer Initialized {@link org.apache.hadoop.hive.ql.exec.mr.ExecReducer} instance.
    */
-  public HiveReduceFunctionResultList(Configuration conf,
+  public HiveReduceFunctionResultList(
       Iterator<Tuple2<HiveKey, Iterable<BytesWritable>>> inputIterator,
-    SparkReduceRecordHandler reducer) {
-    super(conf, inputIterator);
+      SparkReduceRecordHandler reducer) {
+    super(inputIterator);
     this.reduceRecordHandler = reducer;
   }
 
diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
index 286816b..76a4fd1 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/KryoSerializer.java
@@ -19,11 +19,11 @@
 package org.apache.hadoop.hive.ql.exec.spark;
 
 import java.io.ByteArrayInputStream;
-import java.io.ByteArrayOutputStream;
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.IOException;
 
+import org.apache.commons.io.output.ByteArrayOutputStream;
 import org.apache.commons.logging.Log;
 import org.apache.commons.logging.LogFactory;
 import org.apache.hadoop.hive.ql.exec.Utilities;
diff --git a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java b/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java
index 0df4598..ee9f9b7 100644
--- a/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java
+++ b/ql/src/test/org/apache/hadoop/hive/ql/exec/spark/TestHiveKVResultCache.java
@@ -27,8 +27,6 @@
 import java.util.List;
 import java.util.concurrent.LinkedBlockingQueue;
 
-import org.apache.hadoop.conf.Configuration;
-import org.apache.hadoop.hive.conf.HiveConf;
 import org.apache.hadoop.hive.ql.io.HiveKey;
 import org.apache.hadoop.io.BytesWritable;
 import org.junit.Test;
@@ -42,8 +40,7 @@
   @Test
   public void testSimple() throws Exception {
     // Create KV result cache object, add one (k,v) pair and retrieve them.
-    HiveConf conf = new HiveConf();
-    HiveKVResultCache cache = new HiveKVResultCache(conf);
+    HiveKVResultCache cache = new HiveKVResultCache();
 
     HiveKey key = new HiveKey("key".getBytes(), "key".hashCode());
     BytesWritable value = new BytesWritable("value".getBytes());
@@ -60,10 +57,9 @@ public void testSimple() throws Exception {
 
   @Test
   public void testSpilling() throws Exception {
-    HiveConf conf = new HiveConf();
-    HiveKVResultCache cache = new HiveKVResultCache(conf);
+    HiveKVResultCache cache = new HiveKVResultCache();
 
-    final int recordCount = HiveKVResultCache.IN_MEMORY_CACHE_SIZE * 3;
+    final int recordCount = HiveKVResultCache.IN_MEMORY_NUM_ROWS * 3;
 
     // Test using the same cache where first n rows are inserted then cache is cleared.
     // Next reuse the same cache and insert another m rows and verify the cache stores correctly.
@@ -104,10 +100,18 @@ private void testSpillingHelper(HiveKVResultCache cache, int numRecords) {
   @Test
   public void testResultList() throws Exception {
     scanAndVerify(10000, 0, 0, "a", "b");
+    scanAndVerify(10000, 511, 0, "a", "b");
+    scanAndVerify(10000, 511 * 2, 0, "a", "b");
+    scanAndVerify(10000, 511, 10, "a", "b");
+    scanAndVerify(10000, 511 * 2, 10, "a", "b");
     scanAndVerify(10000, 512, 0, "a", "b");
     scanAndVerify(10000, 512 * 2, 0, "a", "b");
-    scanAndVerify(10000, 512, 10, "a", "b");
-    scanAndVerify(10000, 512 * 2, 10, "a", "b");
+    scanAndVerify(10000, 512, 3, "a", "b");
+    scanAndVerify(10000, 512 * 6, 10, "a", "b");
+    scanAndVerify(10000, 512 * 7, 5, "a", "b");
+    scanAndVerify(10000, 512 * 9, 19, "a", "b");
+    scanAndVerify(10000, 1, 0, "a", "b");
+    scanAndVerify(10000, 1, 1, "a", "b");
   }
 
   private static void scanAndVerify(
@@ -176,8 +180,8 @@ private static String bytesWritableToString(BytesWritable bw) {
     // A queue to notify separateRowGenerator to generate the next batch of rows.
     private LinkedBlockingQueue<Boolean> queue;
 
-    MyHiveFunctionResultList(Configuration conf, Iterator inputIterator) {
-      super(conf, inputIterator);
+    MyHiveFunctionResultList(Iterator inputIterator) {
+      super(inputIterator);
     }
 
     void init(long rows, int threshold, int separate, String p1, String p2) {
@@ -258,8 +262,7 @@ protected void closeRecordProcessor() {
   private static long scanResultList(long rows, int threshold, int separate,
       List<Tuple2<HiveKey, BytesWritable>> output, String prefix1, String prefix2) {
     final long iteratorCount = threshold == 0 ? 1 : rows * (100 - separate) / 100 / threshold;
-    MyHiveFunctionResultList resultList = new MyHiveFunctionResultList(
-        new HiveConf(), new Iterator() {
+    MyHiveFunctionResultList resultList = new MyHiveFunctionResultList(new Iterator() {
       // Input record iterator, not used
       private int i = 0;
       @Override
-- 
1.7.9.5


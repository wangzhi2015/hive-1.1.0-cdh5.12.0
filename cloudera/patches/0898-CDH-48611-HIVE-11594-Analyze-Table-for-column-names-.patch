From 4573e6f3a640c5f1fa06d04d228c2e34d7b93f3d Mon Sep 17 00:00:00 2001
From: Gopal V <gopalv@apache.org>
Date: Thu, 20 Aug 2015 14:03:09 -0700
Subject: [PATCH 0898/1164] CDH-48611 HIVE-11594: Analyze Table for column
 names with embedded spaces (Gopal V, reviewed by
 Ashutosh Chauhan)

Change-Id: If76045a51bd3e39cb02a2ce23ba3c3c581f28bcc
---
 .../hive/ql/parse/ColumnStatsSemanticAnalyzer.java |    6 +-
 .../queries/clientpositive/columnstats_quoting.q   |    8 ++
 .../clientpositive/columnstats_quoting.q.out       |  114 ++++++++++++++++++++
 3 files changed, 125 insertions(+), 3 deletions(-)
 create mode 100644 ql/src/test/queries/clientpositive/columnstats_quoting.q
 create mode 100644 ql/src/test/results/clientpositive/columnstats_quoting.q.out

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java b/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
index 3a3f67a..8c29b79 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/parse/ColumnStatsSemanticAnalyzer.java
@@ -189,7 +189,7 @@ private StringBuilder genPartitionClause(Map<String,String> partSpec) throws Sem
         } else {
           whereClause.append(" and ");
         }
-        whereClause.append(partKey).append(" = ").append(genPartValueString(partKey, value));
+        whereClause.append("`").append(partKey).append("` = ").append(genPartValueString(partKey, value));
       }
     }
 
@@ -322,9 +322,9 @@ private String genRewrittenQuery(List<String> colNames, int numBitVectors, Map<S
       if (i > 0) {
         rewrittenQueryBuilder.append(" , ");
       }
-      rewrittenQueryBuilder.append("compute_stats(");
+      rewrittenQueryBuilder.append("compute_stats(`");
       rewrittenQueryBuilder.append(colNames.get(i));
-      rewrittenQueryBuilder.append(" , ");
+      rewrittenQueryBuilder.append("` , ");
       rewrittenQueryBuilder.append(numBitVectors);
       rewrittenQueryBuilder.append(" )");
     }
diff --git a/ql/src/test/queries/clientpositive/columnstats_quoting.q b/ql/src/test/queries/clientpositive/columnstats_quoting.q
new file mode 100644
index 0000000..1bf4f91
--- /dev/null
+++ b/ql/src/test/queries/clientpositive/columnstats_quoting.q
@@ -0,0 +1,8 @@
+DROP TABLE IF EXISTS user_web_events;
+create temporary table user_web_events(`user id` bigint, `user name` string);
+
+explain analyze table user_web_events compute statistics for columns;
+analyze table user_web_events compute statistics for columns;
+
+explain analyze table user_web_events compute statistics for columns `user id`;
+analyze table user_web_events compute statistics for columns `user id`;
diff --git a/ql/src/test/results/clientpositive/columnstats_quoting.q.out b/ql/src/test/results/clientpositive/columnstats_quoting.q.out
new file mode 100644
index 0000000..8614fcc
--- /dev/null
+++ b/ql/src/test/results/clientpositive/columnstats_quoting.q.out
@@ -0,0 +1,114 @@
+PREHOOK: query: DROP TABLE IF EXISTS user_web_events
+PREHOOK: type: DROPTABLE
+POSTHOOK: query: DROP TABLE IF EXISTS user_web_events
+POSTHOOK: type: DROPTABLE
+PREHOOK: query: create temporary table user_web_events(`user id` bigint, `user name` string)
+PREHOOK: type: CREATETABLE
+PREHOOK: Output: database:default
+PREHOOK: Output: default@user_web_events
+POSTHOOK: query: create temporary table user_web_events(`user id` bigint, `user name` string)
+POSTHOOK: type: CREATETABLE
+POSTHOOK: Output: database:default
+POSTHOOK: Output: default@user_web_events
+PREHOOK: query: explain analyze table user_web_events compute statistics for columns
+PREHOOK: type: QUERY
+POSTHOOK: query: explain analyze table user_web_events compute statistics for columns
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+  Stage-1 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: user_web_events
+            Select Operator
+              expressions: user id (type: bigint), user name (type: string)
+              outputColumnNames: user id, user name
+              Group By Operator
+                aggregations: compute_stats(user id, 16), compute_stats(user name, 16)
+                mode: hash
+                outputColumnNames: _col0, _col1
+                Reduce Output Operator
+                  sort order: 
+                  value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>), _col1 (type: struct<columntype:string,maxlength:bigint,sumlength:bigint,count:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: compute_stats(VALUE._col0), compute_stats(VALUE._col1)
+          mode: mergepartial
+          outputColumnNames: _col0, _col1
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-1
+    Column Stats Work
+      Column Stats Desc:
+          Columns: user id, user name
+          Column Types: bigint, string
+          Table: default.user_web_events
+
+PREHOOK: query: analyze table user_web_events compute statistics for columns
+PREHOOK: type: QUERY
+PREHOOK: Input: default@user_web_events
+#### A masked pattern was here ####
+POSTHOOK: query: analyze table user_web_events compute statistics for columns
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@user_web_events
+#### A masked pattern was here ####
+PREHOOK: query: explain analyze table user_web_events compute statistics for columns `user id`
+PREHOOK: type: QUERY
+POSTHOOK: query: explain analyze table user_web_events compute statistics for columns `user id`
+POSTHOOK: type: QUERY
+STAGE DEPENDENCIES:
+  Stage-0 is a root stage
+  Stage-1 is a root stage
+
+STAGE PLANS:
+  Stage: Stage-0
+    Map Reduce
+      Map Operator Tree:
+          TableScan
+            alias: user_web_events
+            Select Operator
+              expressions: user id (type: bigint)
+              outputColumnNames: user id
+              Group By Operator
+                aggregations: compute_stats(user id, 16)
+                mode: hash
+                outputColumnNames: _col0
+                Reduce Output Operator
+                  sort order: 
+                  value expressions: _col0 (type: struct<columntype:string,min:bigint,max:bigint,countnulls:bigint,bitvector:string,numbitvectors:int>)
+      Reduce Operator Tree:
+        Group By Operator
+          aggregations: compute_stats(VALUE._col0)
+          mode: mergepartial
+          outputColumnNames: _col0
+          File Output Operator
+            compressed: false
+            table:
+                input format: org.apache.hadoop.mapred.TextInputFormat
+                output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
+                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe
+
+  Stage: Stage-1
+    Column Stats Work
+      Column Stats Desc:
+          Columns: user id
+          Column Types: bigint
+          Table: default.user_web_events
+
+PREHOOK: query: analyze table user_web_events compute statistics for columns `user id`
+PREHOOK: type: QUERY
+PREHOOK: Input: default@user_web_events
+#### A masked pattern was here ####
+POSTHOOK: query: analyze table user_web_events compute statistics for columns `user id`
+POSTHOOK: type: QUERY
+POSTHOOK: Input: default@user_web_events
+#### A masked pattern was here ####
-- 
1.7.9.5


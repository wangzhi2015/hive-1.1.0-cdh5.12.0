From bb21a25612d3d28449c999126a1b892bc892ee88 Mon Sep 17 00:00:00 2001
From: Aihua Xu <axu@cloudera.com>
Date: Mon, 7 Nov 2016 11:25:17 +0800
Subject: [PATCH 0810/1164] CDH-46986 HIVE-15054: Hive insertion query
 execution fails on Hive on Spark (Aihua Xu via
 Rui Li)

(cherry picked from commit 0951c9c6443fda41e9e4ab5f8302a043f564a5d8)

Change-Id: I52c70702c42db53a97113bc2d81078a3733e9277
---
 .../ql/exec/spark/HivePairFlatMapFunction.java     |   10 +++++++---
 1 file changed, 7 insertions(+), 3 deletions(-)

diff --git a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HivePairFlatMapFunction.java b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HivePairFlatMapFunction.java
index 7df626b..4f1b7d7 100644
--- a/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HivePairFlatMapFunction.java
+++ b/ql/src/java/org/apache/hadoop/hive/ql/exec/spark/HivePairFlatMapFunction.java
@@ -70,10 +70,14 @@ private void setupMRLegacyConfigs() {
       taskAttemptIdBuilder.append("r_");
     }
 
-    // Spark task attempt id is increased by Spark context instead of task, which may introduce
-    // unstable qtest output, since non Hive features depends on this, we always set it to 0 here.
+    // Hive requires this TaskAttemptId to be unique. MR's TaskAttemptId is composed
+    // of "attempt_timestamp_jobNum_m/r_taskNum_attemptNum". The counterpart for
+    // Spark should be "attempt_timestamp_stageNum_m/r_partitionId_attemptNum".
+    // When there're multiple attempts for a task, Hive will rely on the partitionId
+    // to figure out if the data are duplicate or not when collecting the final outputs
+    // (see org.apache.hadoop.hive.ql.exec.Utils.removeTempOrDuplicateFiles)
     taskAttemptIdBuilder.append(taskIdFormat.format(TaskContext.get().partitionId()))
-      .append("_0");
+      .append("_").append(TaskContext.get().attemptNumber());
 
     String taskAttemptIdStr = taskAttemptIdBuilder.toString();
     jobConf.set("mapred.task.id", taskAttemptIdStr);
-- 
1.7.9.5

